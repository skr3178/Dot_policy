<!DOCTYPE html>
<!-- saved from url=(0101)https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="origin-trial" content="A7vZI3v+Gz7JfuRolKNM4Aff6zaGuT7X0mf3wtoZTnKv6497cVMnhy03KDqX7kBz/q/iidW7srW31oQbBt4VhgoAAACUeyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGUuY29tOjQ0MyIsImZlYXR1cmUiOiJEaXNhYmxlVGhpcmRQYXJ0eVN0b3JhZ2VQYXJ0aXRpb25pbmczIiwiZXhwaXJ5IjoxNzU3OTgwODAwLCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><title>Build Your Own Llama 3 Architecture from Scratch Using PyTorch | by Milan Tamang | Towards AI</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" name="apple-itunes-app" content="app-id=828256236, app-argument=/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c, affiliate-data=pt=698524&amp;ct=smart_app_banner&amp;mt=8"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-09-08T15:34:02.488Z"><meta data-rh="true" name="title" content="Build Your Own Llama 3 Architecture from Scratch Using PyTorch | by Milan Tamang | Towards AI"><meta data-rh="true" property="og:title" content="Build Your Own Llama 3 Architecture from Scratch Using PyTorch"><meta data-rh="true" property="al:android:url" content="medium://p/2ce1ecaa901c"><meta data-rh="true" property="al:ios:url" content="medium://p/2ce1ecaa901c"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="A step-by-step guide to building the complete architecture of the Llama 3 model from scratch and performing training and inferencing on a custom dataset. As shown in the Llama 3 architecture diagram…"><meta data-rh="true" property="og:description" content="A step-by-step guide to building the complete architecture of the Llama 3 model from scratch and performing training and inferencing on a…"><meta data-rh="true" property="og:url" content="https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c"><meta data-rh="true" property="al:web:url" content="https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c"><meta data-rh="true" property="og:image" content="https://miro.medium.com/v2/resize:fit:934/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png"><meta data-rh="true" property="article:author" content="https://medium.com/@tamangmilan"><meta data-rh="true" name="author" content="Milan Tamang"><meta data-rh="true" name="robots" content="index,noarchive,follow,max-image-preview:large"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" property="twitter:title" content="Build Your Own Llama 3 Architecture from Scratch Using PyTorch"><meta data-rh="true" name="twitter:site" content="@towards_AI"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/2ce1ecaa901c"><meta data-rh="true" property="twitter:description" content="A step-by-step guide to building the complete architecture of the Llama 3 model from scratch and performing training and inferencing on a…"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/v2/resize:fit:934/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" name="twitter:label1" content="Reading time"><meta data-rh="true" name="twitter:data1" content="26 min read"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://pub.towardsai.net/osd.xml"><link data-rh="true" rel="manifest" href="https://pub.towardsai.net/manifest.json"><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/unbound.css"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/unbound.css"><link data-rh="true" rel="author" href="https://medium.com/@tamangmilan"><link data-rh="true" rel="canonical" href="https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/2ce1ecaa901c"><script type="text/javascript" async="" charset="utf-8" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/recaptcha__en.js" crossorigin="anonymous" integrity="sha384-TG5g5Dk43Fs+5i/7xL9a4bBQ4Pg/EAutBT8nLaRiYl1GoFPQDxyCkJYzTQEw2Njo"></script><script async="" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/branch-latest.min.js"></script><script async="" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/shim.js"></script><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"SocialMediaPosting","image":["https://miro.medium.com/"],"url":"https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c","dateCreated":"2024-09-01T09:13:59Z","datePublished":"2024-09-01T09:13:59Z","dateModified":"2024-09-08T15:34:02Z","headline":"Build Your Own Llama 3 Architecture from Scratch Using PyTorch","name":"Build Your Own Llama 3 Architecture from Scratch Using PyTorch","description":"“” is published by Milan Tamang in Towards AI.","identifier":"2ce1ecaa901c","author":{"@type":"Person","name":"Milan Tamang","url":"https://medium.com/@tamangmilan"},"creator":["Milan Tamang"],"publisher":{"@type":"Organization","name":"Towards AI","url":"https://pub.towardsai.net","logo":{"@type":"ImageObject","width":777,"height":777,"url":"https://miro.medium.com/v2/resize:fit:777/1*JyIThO-cLjlChQLb6kSlVQ.png"}},"mainEntityOfPage":"https://pub.towardsai.net/build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c","isAccessibleForFree":true}</script><style type="text/css" data-fela-rehydration="570" data-fela-type="STATIC">html{box-sizing:border-box;-webkit-text-size-adjust:100%}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}.grecaptcha-badge{visibility:hidden}
/*XCode style (c) Angel Garcia <angelgarcia.mail@gmail.com>*/.hljs {background: #fff;color: black;
}/* Gray DOCTYPE selectors like WebKit */
.xml .hljs-meta {color: #c0c0c0;
}.hljs-comment,
.hljs-quote {color: #007400;
}.hljs-tag,
.hljs-attribute,
.hljs-keyword,
.hljs-selector-tag,
.hljs-literal,
.hljs-name {color: #aa0d91;
}.hljs-variable,
.hljs-template-variable {color: #3F6E74;
}.hljs-code,
.hljs-string,
.hljs-meta .hljs-string {color: #c41a16;
}.hljs-regexp,
.hljs-link {color: #0E0EFF;
}.hljs-title,
.hljs-symbol,
.hljs-bullet,
.hljs-number {color: #1c00cf;
}.hljs-section,
.hljs-meta {color: #643820;
}.hljs-title.class_,
.hljs-class .hljs-title,
.hljs-type,
.hljs-built_in,
.hljs-params {color: #5c2699;
}.hljs-attr {color: #836C28;
}.hljs-subst {color: #000;
}.hljs-formula {background-color: #eee;font-style: italic;
}.hljs-addition {background-color: #baeeba;
}.hljs-deletion {background-color: #ffc8bd;
}.hljs-selector-id,
.hljs-selector-class {color: #9b703f;
}.hljs-doctag,
.hljs-strong {font-weight: bold;
}.hljs-emphasis {font-style: italic;
}
</style><style type="text/css" data-fela-rehydration="570" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k1{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k2{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.d{display:none}.m{display:block}.n{position:sticky}.o{top:0}.p{z-index:500}.q{padding:0 24px}.r{align-items:center}.s{border-bottom:solid 1px #F2F2F2}.z{height:41px}.ab{line-height:20px}.ac{display:flex}.ae{height:57px}.af{flex:1 0 auto}.ag{color:inherit}.ah{fill:inherit}.ai{font-size:inherit}.aj{border:none}.ak{font-family:inherit}.al{letter-spacing:inherit}.am{font-weight:inherit}.an{padding:0}.ao{margin:0}.ap{cursor:pointer}.aq:disabled{cursor:not-allowed}.ar:disabled{color:#6B6B6B}.as:disabled{fill:#6B6B6B}.av{width:auto}.aw path{fill:#242424}.ax{height:25px}.ay{margin-left:24px}.az{border-radius:20px}.ba{width:240px}.bb{background:#F9F9F9}.bc path{fill:#6B6B6B}.be{outline:none}.bf{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{width:100%}.bi{padding:10px 20px 10px 0}.bj{background-color:transparent}.bk{color:#242424}.bl::placeholder{color:#6B6B6B}.bm{display:inline-block}.bn{margin-left:12px}.bo{margin-right:12px}.bp{border-radius:4px}.bq{height:24px}.bw{background-color:#F9F9F9}.bx{border-radius:50%}.by{height:32px}.bz{width:32px}.ca{flex:1 1 auto}.cb{justify-content:center}.ch{max-width:680px}.ci{min-width:0}.cj{animation:k1 1.2s ease-in-out infinite}.ck{height:100vh}.cl{margin-bottom:16px}.cm{margin-top:48px}.cn{align-items:flex-start}.co{flex-direction:column}.cp{justify-content:space-between}.cq{margin-bottom:24px}.cw{width:80%}.cx{background-color:#F2F2F2}.dd{height:44px}.de{width:44px}.df{margin:auto 0}.dg{margin-bottom:4px}.dh{height:16px}.di{width:120px}.dj{width:80px}.dp{margin-bottom:8px}.dq{width:96%}.dr{width:98%}.ds{width:81%}.dw{margin-left:8px}.dx{color:#6B6B6B}.dy{font-size:13px}.dz{height:100%}.ec{margin-right:32px}.ed{border:inherit}.ee{position:relative}.ef{fill:#6B6B6B}.ei{background:transparent}.ej svg{margin-left:4px}.ek svg{fill:#6B6B6B}.em{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.eo{position:absolute}.eq{box-sizing:border-box}.er{max-width:100%}.es{overflow:hidden}.et{text-overflow:ellipsis}.eu{white-space:nowrap}.ev{border-bottom:1px solid #F2F2F2}.ew{height:3px}.ex{background-color:#00ADFF}.ey{max-width:1192px}.fe{font-weight:500}.fu{margin:0 8px}.fv{display:inline}.fw{font-size:16px}.fx{line-height:24px}.fy{color:#1A8917}.fz{fill:#1A8917}.ga:disabled{opacity:0.3}.gd{pointer-events:none}.ge{will-change:opacity, transform}.gf{width:calc(100% - 0px)}.gi{opacity:0}.gj{transform:translateY(89px)}.gk{width:148px}.gl{border-radius:2px}.gm{height:38px}.gn{width:38px}.gp{margin-top:16px}.gq{text-decoration:underline}.gr{word-wrap:break-word}.gs{word-break:break-word}.gy{margin:0 24px}.hc{background:rgba(255, 255, 255, 1)}.hd{border:1px solid #F2F2F2}.he{box-shadow:0 1px 4px #F2F2F2}.hf{max-height:100vh}.hg{overflow-y:auto}.hh{left:0}.hi{top:calc(100vh + 100px)}.hj{bottom:calc(100vh + 100px)}.hk{width:10px}.hl:after{display:block}.hm:after{content:""}.hn:after{clear:both}.ho{line-height:1.23}.hp{letter-spacing:0}.hq{font-style:normal}.hr{font-weight:700}.jb{gap:12px}.jc{align-items:baseline}.jd{width:36px}.je{height:36px}.jf{border:2px solid rgba(255, 255, 255, 1)}.jg{z-index:0}.jh{box-shadow:none}.ji{border:1px solid rgba(0, 0, 0, 0.05)}.jj{margin-bottom:2px}.jk{flex-wrap:nowrap}.jm{width:12px}.jn{flex-wrap:wrap}.jo{padding-left:8px}.jp{padding-right:8px}.kq> *{flex-shrink:0}.kr{overflow-x:scroll}.ks::-webkit-scrollbar{display:none}.kt{scrollbar-width:none}.ku{-ms-overflow-style:none}.kv{width:74px}.kw{flex-direction:row}.kx{z-index:2}.ky{margin-right:4px}.lb{-webkit-user-select:none}.lc{border:0}.ld{cursor:progress}.le{fill:rgba(117, 117, 117, 1)}.lh{opacity:0.25}.li{outline:0}.lj{user-select:none}.lk> svg{pointer-events:none}.lt{margin-left:4px}.lu{margin-top:0px}.lv{opacity:1}.lw{padding:4px 0}.lz{width:16px}.ma{padding:8px 2px}.md svg path{fill:#6B6B6B}.me{display:inline-flex}.mk svg{color:#6B6B6B}.nb{line-height:1.58}.nc{letter-spacing:-0.004em}.nd{font-family:source-serif-pro, Georgia, Cambria, "Times New Roman", Times, serif}.nv{margin-bottom:-0.46em}.nw{margin-left:auto}.nx{margin-right:auto}.ny{max-width:934px}.oe{clear:both}.og{cursor:zoom-in}.oh{z-index:auto}.oj{width:1px}.ok{height:1px}.ol{margin:-1px}.om{clip:rect(0, 0, 0, 0)}.on{border-width:0}.oo{height:auto}.op{margin-top:10px}.oq{text-align:center}.or{max-width:728px}.ou{font-weight:600}.ov{line-height:1.18}.ow{letter-spacing:-0.022em}.pc{margin-bottom:-0.31em}.pi{list-style-type:decimal}.pj{margin-left:30px}.pk{padding-left:0px}.pq{list-style-type:disc}.pr{max-width:1672px}.ps{overflow-x:auto}.pt{font-family:source-code-pro, Menlo, Monaco, "Courier New", Courier, monospace}.pu{padding:32px}.pv{border:1px solid #E5E5E5}.pw{line-height:1.4}.px{margin-top:-0.2em}.py{margin-bottom:-0.2em}.pz{white-space:pre}.qa{min-width:fit-content}.qb{max-width:2540px}.qc{max-width:1612px}.qd{max-width:1726px}.qe{max-width:1788px}.qf{max-width:1728px}.qg{max-width:972px}.qh{max-width:1280px}.qi{max-width:1614px}.qj{max-width:1350px}.qk{font-style:italic}.ql{margin-bottom:26px}.qm{margin-top:6px}.qn{margin-top:8px}.qo{margin-right:8px}.qp{padding:8px 16px}.qq{border-radius:100px}.qr{transition:background 300ms ease}.qt{border-top:none}.qu{margin-bottom:50px}.qv{height:52px}.qw{max-height:52px}.qx{box-sizing:content-box}.qy{position:static}.qz{z-index:1}.rb{max-width:155px}.rh{margin-right:20px}.ri{flex:0 0 auto}.rj{margin-bottom:64px}.rk{margin-bottom:48px}.rx{height:48px}.ry{width:48px}.rz{height:64px}.sa{width:64px}.sb{align-self:flex-end}.sh{padding-right:4px}.so{white-space:pre-wrap}.su{height:0px}.ti{gap:18px}.tj{fill:rgba(61, 61, 61, 1)}.tl{padding-bottom:20px}.tr{margin-top:32px}.ts{fill:#242424}.tt{background:0}.tu{border-color:#242424}.ty:disabled{cursor:inherit !important}.tz:disabled:hover{color:#242424}.ua:disabled:hover{fill:#242424}.ub:disabled:hover{border-color:#242424}.uc{border-radius:99em}.ui{border-width:1px}.uj{border-style:solid}.uk{text-decoration:none}.uq{border-bottom:solid 1px #E5E5E5}.ur{margin-top:72px}.us{padding:24px 0}.ut{margin-bottom:0px}.uu{margin-right:16px}.uv path{stroke:#242424}.uw{border-radius:19px}.ux{padding:9px 16px 9px 16px}.uy{word-break:keep-all}.at:hover:not(:disabled){color:rgba(25, 25, 25, 1)}.au:hover:not(:disabled){fill:rgba(25, 25, 25, 1)}.eg:hover{color:#242424}.eh:hover{fill:#242424}.el:hover svg{fill:#242424}.ep:hover{background-color:rgba(0, 0, 0, 0.1)}.gb:hover:not(:disabled){color:#156D12}.gc:hover:not(:disabled){fill:#156D12}.go:hover{background-color:none}.jl:hover{text-decoration:underline}.lg:hover{fill:rgba(117, 117, 117, 1)}.lx:hover{fill:#000000}.ly:hover p{color:#000000}.mb:hover:not(:disabled) svg path{fill:#000000}.ml:hover svg{color:#000000}.qs:hover{background-color:#F2F2F2}.tk:hover{fill:rgba(25, 25, 25, 1)}.tv:hover{color:#000000}.tw:hover{border-color:#242424}.tx:hover{cursor:pointer}.bd:focus-within path{fill:#242424}.lf:focus{fill:rgba(117, 117, 117, 1)}.mc:focus svg path{fill:#000000}.mm:focus svg{color:#000000}.oi:focus{transform:scale(1.01)}.ll:active{border-style:none}</style><link type="text/css" rel="stylesheet" id="dark-mode-custom-link"><link type="text/css" rel="stylesheet" id="dark-mode-general-link"><style lang="en" type="text/css" id="dark-mode-custom-style"></style><style lang="en" type="text/css" id="dark-mode-native-style"></style><style lang="en" type="text/css" id="dark-mode-native-sheet"></style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (min-width: 1080px)">.e{display:none}.bv{width:64px}.cg{margin:0 64px}.cv{height:48px}.dc{margin-bottom:52px}.do{margin-bottom:48px}.eb{display:flex}.fd{max-width:250px}.fr{font-size:20px}.fs{line-height:24px}.ft{letter-spacing:0}.gx{margin-bottom:50px}.hb{max-width:680px}.im{font-size:42px}.in{margin-top:1.19em}.io{margin-bottom:32px}.ip{line-height:52px}.iq{letter-spacing:-0.011em}.iz{align-items:center}.ja{flex-direction:row}.kc{border-top:solid 1px #F2F2F2}.kd{border-bottom:solid 1px #F2F2F2}.ke{margin:32px 0 0}.kf{padding:3px 8px}.ko> *{margin-right:24px}.kp> :last-child{margin-right:0}.ls{margin-top:0px}.mj{margin:0}.ns{margin-top:2.14em}.nt{line-height:32px}.nu{letter-spacing:-0.003em}.od{margin-top:56px}.pb{margin-top:1.72em}.ph{margin-top:0.94em}.pp{margin-top:1.14em}.rg{display:inline-block}.rn{margin-bottom:0}.ro{margin-right:20px}.sc{max-width:500px}.st{margin-bottom:88px}.tf{font-size:24px}.tg{line-height:30px}.th{letter-spacing:-0.016em}.tq{margin:40px 0 16px}.uh{width:min-width}.up{padding-top:72px}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.f{display:none}.lr{margin-top:0px}.os{margin-left:auto}.ot{text-align:center}.rf{display:inline-block}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (max-width: 903.98px)">.g{display:none}.lq{margin-top:0px}.re{display:inline-block}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (max-width: 727.98px)">.h{display:none}.lo{margin-top:0px}.lp{margin-right:0px}.rd{display:inline-block}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (max-width: 551.98px)">.i{display:none}.t{display:flex}.u{justify-content:space-between}.br{width:24px}.cc{margin:0 24px}.cr{height:40px}.cy{margin-bottom:44px}.dk{margin-bottom:32px}.dt{justify-content:center}.ez{max-width:150px}.ff{font-size:16px}.fg{line-height:20px}.fh{letter-spacing:0}.gt{margin-bottom:2px}.hs{font-size:32px}.ht{margin-top:1.01em}.hu{margin-bottom:24px}.hv{line-height:38px}.hw{letter-spacing:-0.014em}.ir{align-items:flex-start}.is{flex-direction:column-reverse}.jq{margin:24px -24px 0}.jr{padding:0}.kg> *{margin-right:8px}.kh> :last-child{margin-right:24px}.kz{margin-left:0px}.lm{margin-top:0px}.ln{margin-right:0px}.mf{margin:0}.mn{border:1px solid #F2F2F2}.mo{border-radius:99em}.mp{padding:0px 16px 0px 12px}.mq{height:38px}.mr{align-items:center}.mt svg{margin-right:8px}.ne{font-size:18px}.nf{margin-top:1.56em}.ng{line-height:28px}.nh{letter-spacing:-0.003em}.nz{margin-top:40px}.ox{margin-top:1.23em}.pd{margin-top:0.67em}.pl{margin-top:1.34em}.rc{display:inline-block}.rm{flex-direction:column}.rv{margin-bottom:20px}.rw{margin-right:0}.sg{max-width:100%}.si{font-size:24px}.sj{line-height:30px}.sk{letter-spacing:-0.016em}.sp{margin-bottom:64px}.sv{font-size:20px}.sw{line-height:24px}.tm{margin:32px 0 16px}.ud{width:100%}.ul{padding-top:48px}.ms:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.j{display:none}.bu{width:64px}.cf{margin:0 64px}.cu{height:48px}.db{margin-bottom:52px}.dn{margin-bottom:48px}.ea{display:flex}.fc{max-width:250px}.fo{font-size:20px}.fp{line-height:24px}.fq{letter-spacing:0}.gw{margin-bottom:50px}.ha{max-width:680px}.ih{font-size:42px}.ii{margin-top:1.19em}.ij{margin-bottom:32px}.ik{line-height:52px}.il{letter-spacing:-0.011em}.ix{align-items:center}.iy{flex-direction:row}.jy{border-top:solid 1px #F2F2F2}.jz{border-bottom:solid 1px #F2F2F2}.ka{margin:32px 0 0}.kb{padding:3px 8px}.km> *{margin-right:24px}.kn> :last-child{margin-right:0}.mi{margin:0}.np{margin-top:2.14em}.nq{line-height:32px}.nr{letter-spacing:-0.003em}.oc{margin-top:56px}.pa{margin-top:1.72em}.pg{margin-top:0.94em}.po{margin-top:1.14em}.rp{margin-bottom:0}.rq{margin-right:20px}.sd{max-width:500px}.ss{margin-bottom:88px}.tc{font-size:24px}.td{line-height:30px}.te{letter-spacing:-0.016em}.tp{margin:40px 0 16px}.ug{width:min-width}.uo{padding-top:72px}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.k{display:none}.x{display:flex}.y{justify-content:space-between}.bt{width:64px}.ce{margin:0 48px}.ct{height:48px}.da{margin-bottom:52px}.dm{margin-bottom:48px}.dv{justify-content:center}.fb{max-width:250px}.fl{font-size:20px}.fm{line-height:24px}.fn{letter-spacing:0}.gv{margin-bottom:50px}.gz{max-width:680px}.ic{font-size:42px}.id{margin-top:1.19em}.ie{margin-bottom:32px}.if{line-height:52px}.ig{letter-spacing:-0.011em}.iv{align-items:center}.iw{flex-direction:row}.ju{border-top:solid 1px #F2F2F2}.jv{border-bottom:solid 1px #F2F2F2}.jw{margin:32px 0 0}.jx{padding:3px 8px}.kk> *{margin-right:24px}.kl> :last-child{margin-right:0}.mh{margin:0}.nm{margin-top:2.14em}.nn{line-height:32px}.no{letter-spacing:-0.003em}.ob{margin-top:56px}.oz{margin-top:1.72em}.pf{margin-top:0.94em}.pn{margin-top:1.14em}.rr{margin-bottom:0}.rs{margin-right:20px}.se{max-width:500px}.sr{margin-bottom:88px}.sz{font-size:24px}.ta{line-height:30px}.tb{letter-spacing:-0.016em}.to{margin:40px 0 16px}.uf{width:min-width}.un{padding-top:72px}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.l{display:none}.v{display:flex}.w{justify-content:space-between}.bs{width:24px}.cd{margin:0 24px}.cs{height:40px}.cz{margin-bottom:44px}.dl{margin-bottom:32px}.du{justify-content:center}.fa{max-width:150px}.fi{font-size:16px}.fj{line-height:20px}.fk{letter-spacing:0}.gu{margin-bottom:2px}.hx{font-size:32px}.hy{margin-top:1.01em}.hz{margin-bottom:24px}.ia{line-height:38px}.ib{letter-spacing:-0.014em}.it{align-items:flex-start}.iu{flex-direction:column-reverse}.js{margin:24px 0 0}.jt{padding:0}.ki> *{margin-right:8px}.kj> :last-child{margin-right:8px}.la{margin-left:0px}.mg{margin:0}.mu{border:1px solid #F2F2F2}.mv{border-radius:99em}.mw{padding:0px 16px 0px 12px}.mx{height:38px}.my{align-items:center}.na svg{margin-right:8px}.ni{font-size:18px}.nj{margin-top:1.56em}.nk{line-height:28px}.nl{letter-spacing:-0.003em}.oa{margin-top:40px}.oy{margin-top:1.23em}.pe{margin-top:0.67em}.pm{margin-top:1.34em}.rl{flex-direction:column}.rt{margin-bottom:20px}.ru{margin-right:0}.sf{max-width:100%}.sl{font-size:24px}.sm{line-height:30px}.sn{letter-spacing:-0.016em}.sq{margin-bottom:64px}.sx{font-size:20px}.sy{line-height:24px}.tn{margin:32px 0 16px}.ue{width:100%}.um{padding-top:48px}.mz:hover{border-color:#E5E5E5}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="print">.ra{display:none}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.gg{transition:opacity 200ms}.of{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style><style type="text/css" data-fela-rehydration="570" data-fela-type="RULE" media="all and (max-width: 1232px)">.gh{display:none}</style><style data-fela-type="RULE" type="text/css" media="(orientation: landscape) and (max-width: 903.98px)"></style><link rel="icon" href="https://miro.medium.com/v2/resize:fill:160:160/1*yPSiS-zAdm-XKV-t6eEuOg.png" data-rh="true"><link rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/v2/resize:fill:190:190/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156" data-rh="true"><link rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/v2/resize:fill:150:150/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156" data-rh="true"><link rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/v2/resize:fill:95:95/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156" data-rh="true"><link rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/v2/resize:fill:75:75/10fd5c419ac61637245384e7099e131627900034828f4f386bdaa47a74eae156" data-rh="true"><link rel="mask-icon" href="https://miro.medium.com/v2/resize:fill:625:625/7*GAOKVe--MXbEJmV9230oOQ.png" color="#171717" data-rh="true"><script async="true" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/js" data-rh="true"></script><script data-rh="true">window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-7JY7T788PK');</script><script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/enterprise.js" data-rh="true"></script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener banner closeBanner closeJourney data deepview deepviewCta first init link logout removeListener setBranchViewData setIdentity track trackCommerceEvent logEvent disableTracking getBrowserFingerprintId crossPlatformIds lastAttributedTouchData setAPIResponseCallback qrCode setRequestMetaData setAPIUrl getAPIUrl setDMAParamsForEEA".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><a href="https://pub.towardsai.net/sitemap/sitemap.xml" class="d">Sitemap</a><div class="e f g h i j k l"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="m c"><div class="m n o p c" style="transform: translateY(-57px);"><div class="an r s t dt v du x dv j e z ab"><a class="dx ah dy bf al b an ao ap aq ar as at au t v x j e r dz ab" href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2ce1ecaa901c&amp;%7Efeature=LiOpenInAppButton&amp;%7Echannel=ShowPostUnderCollection&amp;%7Estage=mobileNavBar&amp;source=post_page---top_nav_layout_nav-----------------------------------------" rel="noopener follow">Open in app<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" fill="none" viewBox="0 0 10 10" class="dw"><path fill="currentColor" d="M.985 8.485a.375.375 0 1 0 .53.53zM8.75 1.25h.375A.375.375 0 0 0 8.75.875zM8.375 6.5a.375.375 0 1 0 .75 0zM3.5.875a.375.375 0 1 0 0 .75zm-1.985 8.14 7.5-7.5-.53-.53-7.5 7.5zm6.86-7.765V6.5h.75V1.25zM3.5 1.625h5.25v-.75H3.5z"></path></svg></a></div><div class="q r s ac ae"><div class="ac r af"><a class="ag ah ai aj ak al am an ao ap aq ar as at au ac" aria-label="Homepage" data-testid="headerMediumLogo" href="https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><svg xmlns="http://www.w3.org/2000/svg" width="719" height="160" fill="none" aria-labelledby="wordmark-medium-desc" viewBox="0 0 719 160" class="av aw ax"><desc id="wordmark-medium-desc">Medium Logo</desc><path fill="#242424" d="m174.104 9.734.215-.047V8.02H130.39L89.6 103.89 48.81 8.021H1.472v1.666l.212.047c8.018 1.81 12.09 4.509 12.09 14.242V137.93c0 9.734-4.087 12.433-12.106 14.243l-.212.047v1.671h32.118v-1.665l-.213-.048c-8.018-1.809-12.089-4.509-12.089-14.242V30.586l52.399 123.305h2.972l53.925-126.743V140.75c-.687 7.688-4.721 10.062-11.982 11.701l-.215.05v1.652h55.948v-1.652l-.215-.05c-7.269-1.639-11.4-4.013-12.087-11.701l-.037-116.774h.037c0-9.733 4.071-12.432 12.087-14.242m25.555 75.488c.915-20.474 8.268-35.252 20.606-35.507 3.806.063 6.998 1.312 9.479 3.714 5.272 5.118 7.751 15.812 7.368 31.793zm-.553 5.77h65.573v-.275c-.186-15.656-4.721-27.834-13.466-36.196-7.559-7.227-18.751-11.203-30.507-11.203h-.263c-6.101 0-13.584 1.48-18.909 4.16-6.061 2.807-11.407 7.003-15.855 12.511-7.161 8.874-11.499 20.866-12.554 34.343q-.05.606-.092 1.212a50 50 0 0 0-.065 1.151 85.807 85.807 0 0 0-.094 5.689c.71 30.524 17.198 54.917 46.483 54.917 25.705 0 40.675-18.791 44.407-44.013l-1.886-.664c-6.557 13.556-18.334 21.771-31.738 20.769-18.297-1.369-32.314-19.922-31.042-42.395m139.722 41.359c-2.151 5.101-6.639 7.908-12.653 7.908s-11.513-4.129-15.418-11.63c-4.197-8.053-6.405-19.436-6.405-32.92 0-28.067 8.729-46.22 22.24-46.22 5.657 0 10.111 2.807 12.236 7.704zm43.499 20.008c-8.019-1.897-12.089-4.722-12.089-14.951V1.309l-48.716 14.353v1.757l.299-.024c6.72-.543 11.278.386 13.925 2.83 2.072 1.915 3.082 4.853 3.082 8.987v18.66c-4.803-3.067-10.516-4.56-17.448-4.56-14.059 0-26.909 5.92-36.176 16.672-9.66 11.205-14.767 26.518-14.767 44.278-.003 31.72 15.612 53.039 38.851 53.039 13.595 0 24.533-7.449 29.54-20.013v16.865h43.711v-1.746zM424.1 19.819c0-9.904-7.468-17.374-17.375-17.374-9.859 0-17.573 7.632-17.573 17.374s7.721 17.374 17.573 17.374c9.907 0 17.375-7.47 17.375-17.374m11.499 132.546c-8.019-1.897-12.089-4.722-12.089-14.951h-.035V43.635l-43.714 12.551v1.705l.263.024c9.458.842 12.047 4.1 12.047 15.152v81.086h43.751v-1.746zm112.013 0c-8.018-1.897-12.089-4.722-12.089-14.951V43.635l-41.621 12.137v1.71l.246.026c7.733.813 9.967 4.257 9.967 15.36v59.279c-2.578 5.102-7.415 8.131-13.274 8.336-9.503 0-14.736-6.419-14.736-18.073V43.638l-43.714 12.55v1.703l.262.024c9.459.84 12.05 4.097 12.05 15.152v50.17a56.3 56.3 0 0 0 .91 10.444l.787 3.423c3.701 13.262 13.398 20.197 28.59 20.197 12.868 0 24.147-7.966 29.115-20.43v17.311h43.714v-1.747zm169.818 1.788v-1.749l-.213-.05c-8.7-2.006-12.089-5.789-12.089-13.49v-63.79c0-19.89-11.171-31.761-29.883-31.761-13.64 0-25.141 7.882-29.569 20.16-3.517-13.01-13.639-20.16-28.606-20.16-13.146 0-23.449 6.938-27.869 18.657V43.643L545.487 55.68v1.715l.263.024c9.345.829 12.047 4.181 12.047 14.95v81.784h40.787v-1.746l-.215-.053c-6.941-1.631-9.181-4.606-9.181-12.239V66.998c1.836-4.289 5.537-9.37 12.853-9.37 9.086 0 13.692 6.296 13.692 18.697v77.828h40.797v-1.746l-.215-.053c-6.94-1.631-9.18-4.606-9.18-12.239V75.066a42 42 0 0 0-.578-7.26c1.947-4.661 5.86-10.177 13.475-10.177 9.214 0 13.691 6.114 13.691 18.696v77.828z"></path></svg></a><div class="ay i"><div class="ac aj az ba bb r bc bd"><div class="bm" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults" aria-haspopup="listbox" role="listbox"></div><div class="bn bo ac"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" tabindex="0" class="aj be bf bg ab bh bi bj bk bl" placeholder="Search" value=""></div></div></div><div class="i l x ea eb"><div class="ec ac"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" data-testid="headerWriteButton" href="https://medium.com/new-story?source=post_page---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><div class="bf b bg ab dx ee ef ac r eg eh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Write"><path fill="currentColor" d="M14 4a.5.5 0 0 0 0-1zm7 6a.5.5 0 0 0-1 0zm-7-7H4v1h10zM3 4v16h1V4zm1 17h16v-1H4zm17-1V10h-1v10zm-1 1a1 1 0 0 0 1-1h-1zM3 20a1 1 0 0 0 1 1v-1zM4 3a1 1 0 0 0-1 1h1z"></path><path stroke="currentColor" d="m17.5 4.5-8.458 8.458a.25.25 0 0 0-.06.098l-.824 2.47a.25.25 0 0 0 .316.316l2.47-.823a.25.25 0 0 0 .098-.06L19.5 6.5m-2-2 2.323-2.323a.25.25 0 0 1 .354 0l1.646 1.646a.25.25 0 0 1 0 .354L19.5 6.5m-2-2 2 2"></path></svg><div class="dw m">Write</div></div></a></div></div><div class="l k j e"><div class="ec ac"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" data-testid="headerSearchButton" href="https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><div class="bf b bg ab dx ee ef ac r eg eh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Search"><path fill="currentColor" fill-rule="evenodd" d="M4.092 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0m6.95-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .79-.79l-3.73-3.73A8.05 8.05 0 0 0 11.042 3z" clip-rule="evenodd"></path></svg></div></a></div></div><div class=""><div class="ec ac"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" data-testid="headerNotificationButton" href="https://medium.com/me/notifications?source=post_page---top_nav_layout_nav-----------------------------------------" rel="noopener follow"><div class="bf b bg ab dx ee ef ac r eg eh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" aria-label="Notifications"><path stroke="currentColor" stroke-linecap="round" d="M15 18.5a3 3 0 1 1-6 0"></path><path stroke="currentColor" stroke-linejoin="round" d="M5.5 10.532V9a6.5 6.5 0 0 1 13 0v1.532c0 1.42.564 2.782 1.568 3.786l.032.032c.256.256.4.604.4.966v2.934a.25.25 0 0 1-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.934c0-.363.144-.71.4-.966l.032-.032A5.35 5.35 0 0 0 5.5 10.532Z"></path></svg></div></a></div></div><div class="m" aria-hidden="false"><button class="aj ei an ac r ap ee ej ek el" aria-label="user options menu" data-testid="headerUserIcon"><div class="m ee"><div class="m ee"><img alt="skr3178" class="m eq bx by bz cx" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_dmbNkD5D-u45r44go_cf0g.png" width="32" height="32" loading="lazy"><div class="em bx m by bz eo o aj ep"></div></div></div></button></div></div></div><div class="ac"><div class="ri" style="width: 0px;"></div><div class="ca bh" style="width: calc(100% + 0px);"><div class="m"><div class="bw uz"><div class="jr ir u jt it w va iv dv vb ix vc vd iz ve ac ee"><div class="vf ir vg it iv ix iz ac ee"><div class="vh vi vj vk vl vm vn ac"><svg xmlns="http://www.w3.org/2000/svg" width="64" height="64" fill="none" viewBox="0 0 64 64" role="presentation" aria-hidden="true" focusable="false" class="vo vp"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div><span class="bf b bg ab bk"><span class="qo fv">Get unlimited access to the best of Medium for less than $1/week.</span><div class="bm"><a class="ag ah ai ed ak al am an ao ap aq ar as gq bm" href="https://medium.com/plans?source=upgrade_membership---post_top_nav_upsell-----------------------------------------" rel="noopener follow"><div class="k j e"><span class="fe vq hm vr vs vt vu">Become a member</span></div><div class="i l"><span class="fe">Become a member</span></div></a></div></span></div><div class="vv vw vx vy vz m wa"><div class="i l"><div class="m ee o wb"><button class="ag ah ai ed ak al am an ao ap aq ar as at au ac" data-testid="close-button" aria-label="close"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="ef dx eh eg"><path stroke="currentColor" stroke-linecap="round" d="m5 5 7 7m7 7-7-7m0 0 7-7m-7 7-7 7"></path></svg></button></div></div><div class="k j e"><div class="m ee o wb"><button class="ag ah ai ed ak al am an ao ap aq ar as at au ac ee vq hm wc wd we wf" data-testid="close-button" aria-label="close"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="ef dx eh eg"><path stroke="currentColor" stroke-linecap="round" d="m5 5 7 7m7 7-7-7m0 0 7-7m-7 7-7 7"></path></svg></button></div></div></div></div></div><div><div class="ev bh m"><div class="ew bh ex"></div><div class="ac cb"><div class="cc cd ce cf cg ey ci bh"><div class="ae ac r"><div class="ez fa fb fc fd m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://pub.towardsai.net/?source=post_page---publication_nav-98111c9905da-2ce1ecaa901c---------------------------------------" rel="noopener follow"><h2 class="bf fe ff fg fh fi fj fk fl fm fn fo fp fq fr fs ft bk"><div class="er es et eu">Towards AI</div></h2></a></div><div class="t v k j e"><span class="fu fv" aria-hidden="true"><span class="bf b fw fx bk">·</span></span><p class="bf b fw fx bk"><button class="ag ah ai ed ak al am an ao ap aq ar as gq">Follow publication</button></p></div></div></div></div></div></div><div class="ra" role="dialog" aria-modal="true" tabindex="-1"><div class="wg wh bh dz wi wj wk ap gi gd wl" aria-hidden="true" role="presentation"></div><div class="wm wi wn wo wp wg dz eq wq wr ws lv wt wu wv ww wx wy wz xa xb xc ac co xd" aria-hidden="true"><div class="xe hg"></div></div></div><div class="gd eo ge gf o gg gh gi afo"><div class="ac cb"><div class="cc cd ce cf cg ey ci bh"><div class="gk m"><div class="gd"><div class="ac cn co"><a href="https://pub.towardsai.net/?source=post_page---post_publication_sidebar-98111c9905da-2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="ee"><img alt="Towards AI" class="cx gl m gn gm" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JyIThO-cLjlChQLb6kSlVQ.png" width="38" height="38" loading="lazy"><div class="gl m gm gn eo o em go"></div></div></a><div class="gp m"></div><p class="bf b bg ab dx">The leading AI community and content platform focused on making AI accessible to all. Check out our new course platform: <a class="ag ah ai ed ak al am an ao ap aq ar as gq gr gs" href="https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev" rel="noopener  ugc nofollow">https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev</a></p><div class="gp m"></div><p class="bf b bg ab bk"><button class="ag ah ai ed ak al am an ao ap aq ar as gq">Follow publication</button></p></div></div></div></div></div></div><div class="gt gu gv gw gx m"><div class="ac cb"><div class="ci bh gy gz ha hb"></div></div><article><div class="m"><div class="m"><span class="m"></span><section><div><div class="eo hh yl hj hk gd"></div><div class="speechify-ignore nw nx or ee i l k j ys"><aside class="wp eo o" style="width: 379.6px;"><div class="yv er eo yw eu bh ac r yx"><p class="bf b dy ab dx"><span>Top highlight</span></p></div></aside></div><div class="gs gr hl hm hn"><div class="ac cb"><div class="ci bh gy gz ha hb"><div><h1 id="a79a" class="pw-post-title ho hp hq bf hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io ip iq bk" data-testid="storyTitle" data-selectable-paragraph=""><strong class="am">Build Your Own Llama 3 Architecture from Scratch Using PyTorch</strong></h1><div><div class="speechify-ignore ac cp"><div class="speechify-ignore bh m"><div class="ac ir is it iu iv iw ix iy iz ja jb"><div class="ac r jb"><div class="ac jc"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="1" aria-labelledby="1"><div tabindex="-1" class="be"><a href="https://medium.com/@tamangmilan?source=post_page---byline--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="m jd je bx jf jg"><div class="m ee"><img alt="Milan Tamang" class="m eq bx by bz cx" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_mfDWMsklE8l0-1rrnhbJ4g.jpg" width="32" height="32" loading="lazy" data-testid="authorPhoto"><div class="jh bx m by bz eo o ji ep"></div></div></div></a></div></div></div></div><span class="bf b bg ab bk"><div class="jj ac r"><div class="ac r jk"><div class="ac r"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="2" aria-labelledby="2"><div tabindex="-1" class="be"><span class="bf b bg ab bk"><a class="ag ah ai ed ak al am an ao ap aq ar as jl" data-testid="authorName" href="https://medium.com/@tamangmilan?source=post_page---byline--2ce1ecaa901c---------------------------------------" rel="noopener follow">Milan Tamang</a></span></div></div></div></div><div class="jm bm"></div><div class="bm" aria-hidden="false"><button class="tt uv ap ac cb r ym yn yo" style="border: 1px solid rgb(36, 36, 36);"><span class="bf b bg ab bk bh"><span class="bm uy">Follow</span></span></button></div></div></div></span></div><div class="ac r jn"><span class="bf b bg ab dx"><div class="ac af"><span data-testid="storyReadTime">26 min read</span><div class="jo jp m" aria-hidden="true"><span class="m" aria-hidden="true"><span class="bf b bg ab dx">·</span></span></div><span data-testid="storyPublishDate">Sep 1, 2024</span></div></span></div></div><div class="ac cp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="i l x ea eb r"><div class="kv m"><div class="ac r kw kx"><div class="pw-multi-vote-icon ee ky kz la lb"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="79" aria-labelledby="79" role="tooltip"><div tabindex="-1" class="be"><button class="lc ap le yp yq li an lj lk ll lb" data-testid="headerClapButton"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="pw-multi-vote-count m lm ln lo lp lq lr ls"><div><div class="bm" aria-hidden="false" aria-describedby="80" aria-labelledby="80" role="tooltip"><div tabindex="-1" class="be"><p class="bf b dy ab dx"><button class="ag ah ai ed ak al am an ao ap aq ar as at au yr tv">1K<span class="m i h g rf rg"></span></button></p></div></div></div></div></div></div><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="3" aria-labelledby="3"><div tabindex="-1" class="be"><button class="ap lc lv lw ac r ef lx ly" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="lu"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b dy ab dx"><span class="pw-responses-count lt lu">4</span></p></button></div></div></div></div><div class="ac r kg kh ki kj kk kl km kn ko kp kq kr ks kt ku"><div class="lz l k j e"></div><div class="i l"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="4" aria-labelledby="4"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="ag ef ai ed ak al am ma ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="eq me cn"><div class="m af"><div class="ac cb"><div class="mf mg mh mi mj er ci bh"><div class="ac"><div><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/plans?dimension=post_audio_button&amp;postId=2ce1ecaa901c&amp;source=upgrade_membership---post_audio_button-----------------------------------------" rel="noopener follow"><div><div class="bm" aria-hidden="false" aria-describedby="17" aria-labelledby="17" role="tooltip"><div tabindex="-1" class="be"><button aria-label="Listen" data-testid="audioPlayButton" class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm mn mo mp mq t mr ms mt mu mv mw mx v my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"></path></svg><div class="k j e"><p class="bf b bg ab dx">Listen</p></div></button></div></div></div></a></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="6" aria-labelledby="6"><div tabindex="-1" class="be"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm mn mo mp mq t mr ms mt mu mv mw mx v my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg><div class="k j e"><p class="bf b bg ab dx">Share</p></div></button></div></div></div></div><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="85" aria-labelledby="85" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm mn mo mp mq t mr ms mt mu mv mw mx v my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg><div class="k j e"><p class="bf b bg ab dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><p id="b491" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">A step-by-step guide to building the complete architecture of the Llama 3 model from scratch and performing training and inferencing on a custom dataset.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx ny"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_xNP7aBpcmcMk4tXJ-Z8Mw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="1179" loading="eager" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1__xNP7aBpcmcMk4tXJ-Z8Mw.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: Llama 3 architecture shows training and inferencing flow. I imagined this diagram as the official Llama 3 paper doesn’t have one. By the end of this article, I believe you should be able to draw a better architecture than this one.</strong></figcaption></figure><h2 id="93d5" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph=""><strong class="am">What will you achieve by the end of this article?</strong></h2><ol class=""><li id="1ab0" class="nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv pi pj pk bk" data-selectable-paragraph="">You’ll get an in-depth intuition of how each component of the Llama 3 model works under the hood.</li><li id="4e0a" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">You’ll write codes to build each component of Llama 3 and then assemble them all together to build a fully functional Llama 3 model.</li><li id="61e5" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">You’ll also write codes to train your model with new custom datasets.</li><li id="aa26" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">You’ll also write code to perform inferencing so that your Llama 3 model can generate new texts based on input prompts.</li></ol><h2 id="c0f7" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph=""><strong class="am">Prerequisites</strong></h2><ul class=""><li id="4561" class="nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv pq pj pk bk" data-selectable-paragraph="">A basic knowledge of Python and Pytorch is required.</li><li id="973c" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">A basic understanding of transformer concepts such as Self- attention and also knowledge of deep neural networks would certainly help though not compulsory.</li></ul><p id="4abe" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Now that we know what we want to achieve, let’s start building everything step by step.</strong></p><h2 id="2996" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph=""><strong class="am">Step 1: The Input Block</strong></h2><p id="c900" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">As shown in the Llama 3 architecture diagram above, the input block has 3 components:- <strong class="nd hr">Texts/ Prompts, Tokenizer and Embeddings.</strong></p><p id="5e56" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">How do the components inside the Input Block work?</strong> There is a popular saying “A picture is worth a thousand words”, let’s check the flow diagram below to understand the workflow inside the Input block.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx pr"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iKiOUpXAWUJ_vlVkWuC_w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*7iKiOUpXAWUJ_vlVkWuC_w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*7iKiOUpXAWUJ_vlVkWuC_w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*7iKiOUpXAWUJ_vlVkWuC_w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*7iKiOUpXAWUJ_vlVkWuC_w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*7iKiOUpXAWUJ_vlVkWuC_w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*7iKiOUpXAWUJ_vlVkWuC_w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*7iKiOUpXAWUJ_vlVkWuC_w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="555" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_7iKiOUpXAWUJ_vlVkWuC_w.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: Input Block flow diagram displaying prompts, tokenizer, and embedding flow.</strong></figcaption></figure><ul class=""><li id="ce54" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph="">First of all, a single or batch of texts/prompts will be passed into the model. For example: “Hello World” in the above flow diagram.</li><li id="96b1" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">The input to the model should always be in number format as it is unable to process text. Tokenizer helps to convert these texts/prompts into token-ids (which is an index number representation of tokens in vocabulary). We’ll use the popular Tiny Shakespeare dataset to build the vocabulary and also train our model.</li><li id="5f32" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">The tokenizer used in the Llama 3 model is <mark class="yt yu ap">TikToken</mark>, a type of subword tokenizer. However, we’ll be using a character-level tokenizer for our model building. The main reason is that we should know how to build a vocabulary and tokenizer including encode and decode functions all by ourselves. This way we’ll be able to learn how everything works under the hood and we’ll have full control over the code.</li><li id="a262" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">Finally, each token-id will be transformed into an embedding vector of dimensions 128(in original Llama 3 8B, it is 4096). The embeddings will then be passed into the next block called the Decoder Block.</li></ul><p id="f564" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Let’s code the Input block:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="0c7f" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment"># Import necessary libraries</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span>, <span class="hljs-type">Tuple</span>, <span class="hljs-type">List</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt</span></pre><pre class="gp ps pt pu bp pv bb bk"><span id="97e8" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">### Step 1: Input Block ###</span><br><br><span class="hljs-comment"># Using Tiny Shakespeare dataset for character-level tokenizer. Some part of the following character-level tokenizer is referenced from Andrej karpathy's GitHub (https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py) which I found is explained very well.</span><br><span class="hljs-comment"># Load tiny_shakespeare data file (https://github.com/tamangmilan/llama3/blob/main/tiny_shakespeare.txt)</span><br><br>device: <span class="hljs-built_in">str</span> = <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>   <span class="hljs-comment"># Assign device to cuda or cpu based on availability</span><br><br><span class="hljs-comment"># Load tiny_shakespeare data file.</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'tiny_shakespeare.txt'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:<br>  data = f.read()<br><br><span class="hljs-comment"># Prepare vocabulary by taking all the unique characters from the tiny_shakespeare data</span><br>vocab = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(data)))<br><br><span class="hljs-comment"># Training Llama 3 model requires addtional tokens such as &lt;|begin_of_text|&gt;, &lt;|end_of_text|&gt; and &lt;|pad_id|&gt;, we'll add them into vocabulary</span><br>vocab.extend([<span class="hljs-string">'&lt;|begin_of_text|&gt;'</span>,<span class="hljs-string">'&lt;|end_of_text|&gt;'</span>,<span class="hljs-string">'&lt;|pad_id|&gt;'</span>])<br>vocab_size = <span class="hljs-built_in">len</span>(vocab)<br><br><span class="hljs-comment"># Create a mapping between characters with corresponding integer indexes in vocabulary.</span><br><span class="hljs-comment"># This is important to build tokenizers encode and decode functions.</span><br>itos = {i:ch <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab)}<br>stoi = {ch:i <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocab)}<br><br><span class="hljs-comment"># Tokenizers encode function: take a string, output a list of integers</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">encode</span>(<span class="hljs-params">s</span>):<br>  <span class="hljs-keyword">return</span> [stoi[ch] <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> s]<br><br><span class="hljs-comment"># Tokenizers decode function: take a list of integers, output a string</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">decode</span>(<span class="hljs-params">l</span>):<br>  <span class="hljs-keyword">return</span> <span class="hljs-string">''</span>.join(itos[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> l)<br><br><span class="hljs-comment"># Define tensor token variable to be used later during model training</span><br>token_bos = torch.tensor([stoi[<span class="hljs-string">'&lt;|begin_of_text|&gt;'</span>]], dtype=torch.<span class="hljs-built_in">int</span>, device=device)<br>token_eos = torch.tensor([stoi[<span class="hljs-string">'&lt;|end_of_text|&gt;'</span>]], dtype=torch.<span class="hljs-built_in">int</span>, device=device)<br>token_pad = torch.tensor([stoi[<span class="hljs-string">'&lt;|pad_id|&gt;'</span>]], dtype=torch.<span class="hljs-built_in">int</span>, device=device)<br><br>prompts = <span class="hljs-string">"Hello World"</span><br>encoded_tokens = encode(prompts)<br>decoded_text = decode(encoded_tokens)<br><br><span class="hljs-comment">### Test: Input Block Code ###</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>print(f"Lenth of shakespeare in character: {len(data)}")<br>print(f"The vocabulary looks like this: {''.join(vocab)}\n")<br>print(f"Vocab size: {vocab_size}")<br>print(f"encoded_tokens: {encoded_tokens}")<br>print(f"decoded_text: {decoded_text}")<br>"""</span><br><span class="hljs-comment">### Test Results: ###</span><br><span class="hljs-string">"""<br>Lenth of shakespeare in character: 1115394<br>The vocabulary looks like this: <br> !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&lt;|begin_of_text|&gt;&lt;|end_of_text|&gt;&lt;|pad_id|&gt;<br><br>Vocab size: 68<br>encoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]<br>decoded_text: Hello World<br>"""</span></span></pre><h2 id="a101" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">Step 2: The Decoder Block</h2><p id="c689" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">If you look at the architecture diagram above, the decoder block consists of the following sub-components.</p><ul class=""><li id="b6ec" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph="">RMS Norm</li><li id="bb9b" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">Rotary Positional Encoding</li><li id="52ec" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">KV Cache</li><li id="3c19" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">Group Query Attention</li><li id="3e2d" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">FeedForward Network</li><li id="2c6b" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">Decoder Block</li></ul><p id="be63" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">Let’s deep dive into each of these sub-components one by one.</p><h2 id="9ddd" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">2a. RMS Norm (Root Mean Square Normalization):</h2><p id="68c5" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Why do you need RMSNorm? </strong>In the architecture diagram above, you must have noticed that the output of the input block i.e. <strong class="nd hr">embedding vector</strong> passes through the <strong class="nd hr">RMSNorm block</strong>. This is because the embedding vector has many dimensions (4096 dim in Llama3-8b) and there is always a chance of having values in different ranges. This can cause model gradients to explode or vanish hence resulting in slow convergence or even divergence. RMSNorm brings these values into a certain range which helps to stabilize and accelerate the training process. This makes gradients have more consistent magnitudes and that results in making models converge more quickly.</p><p id="e222" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">How does RMSNorm work?</strong> Let’s look at the following diagram first.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qb"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*l3namyydml7LFi67PqUu1w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*l3namyydml7LFi67PqUu1w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*l3namyydml7LFi67PqUu1w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*l3namyydml7LFi67PqUu1w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*l3namyydml7LFi67PqUu1w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*l3namyydml7LFi67PqUu1w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3namyydml7LFi67PqUu1w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*l3namyydml7LFi67PqUu1w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*l3namyydml7LFi67PqUu1w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*l3namyydml7LFi67PqUu1w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*l3namyydml7LFi67PqUu1w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*l3namyydml7LFi67PqUu1w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*l3namyydml7LFi67PqUu1w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*l3namyydml7LFi67PqUu1w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="247" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_l3namyydml7LFi67PqUu1w.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: RMSNorm implementation on the input embedding of shape [3,3]</strong></figcaption></figure><ul class=""><li id="58db" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph="">Just like layer normalization, RMSNorm is applied along the embedding features or dimension. The diagram above has embeddings of shape [3,3] meaning each token has 3 dimensions.</li></ul><p id="03b4" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Example: Let’s apply RMSNorm to the embedding of the first token X1:</strong></p><ul class=""><li id="3662" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph="">The value of token X1 at each dimension i.e. x11, x12, and x13 will be individually divided by the <strong class="nd hr">Root Mean Square</strong> of all these values. The formula is shown in the diagram above.</li><li id="7108" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">E (Epsilon) which is a small constant is added to the Root Mean Square to avoid division by Zero for numerical stability.</li><li id="5c42" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">Finally, a scaling parameter <strong class="nd hr">Gamma (Y)</strong> is multiplied by it. Each feature has one unique Gamma parameter (just like Y1 for dim d1, Y2 for dim d2 and Y3 for dim d3 in the diagram above) is a learning parameter that is scaled up or down to bring further stability to the normalization. The gamma parameter is initialized with value 1 (as shown in the calculation above).</li><li id="f01c" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">As you noticed in the example above, the embedding values are large and spread in a wide range. After applying RMSNorm, the values are much smaller and in a small range. The calculation has been done with actual RMSNorm function.</li></ul><p id="7f8f" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Why choose RMSNorm over layer normalization? </strong>As you noticed above in the example, we didn’t calculate any mean or variance which is done in the case of layer normalization. Thus, we can say that RMSNorm reduces the computational overhead by avoiding the calculation of mean and variance. Also, according to the paper by the Author, RMSNorm gives performance advantages while not compromising on accuracy.</p><p id="fa69" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Let’s code the RMSNorm:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="236f" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment"># Step2: The Decoder Block</span><br><span class="hljs-comment"># Note: Since the Llama 3 model is developed by Meta, so to be in sync with their codebase and for future compatibility,</span><br><span class="hljs-comment"># I will use most of the code from Meta GitHub with some necessary changes required to achieve our goal.</span><br><br><span class="hljs-comment"># Define parameters dataclass: we'll use these parameters during model building, training and inference.</span><br><span class="hljs-comment"># Note: Since we want to see the results of training and inferencing faster rather than focusing on high accuracy, we're taking lower values for most of the parameters which are set higher in the Llama 3 model.</span><br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title.class">ModelArgs</span>:<br>    dim: <span class="hljs-built_in">int</span> = <span class="hljs-number">512</span>              <span class="hljs-comment"># embedding dimension</span><br>    n_layers: <span class="hljs-built_in">int</span> = <span class="hljs-number">8</span>           <span class="hljs-comment"># number of model decoder blocks</span><br>    n_heads: <span class="hljs-built_in">int</span> = <span class="hljs-number">8</span>            <span class="hljs-comment"># number of heads for queries embedding</span><br>    n_kv_heads: <span class="hljs-built_in">int</span> = <span class="hljs-number">4</span>         <span class="hljs-comment"># number of heads for keys and values embedding</span><br>    vocab_size: <span class="hljs-built_in">int</span> = <span class="hljs-built_in">len</span>(vocab) <span class="hljs-comment"># Length of vocabulary</span><br>    multiple_of: <span class="hljs-built_in">int</span> = <span class="hljs-number">256</span>        <span class="hljs-comment"># Require to calculate dim of feedfoward network</span><br>    ffn_dim_multiplier: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>  <span class="hljs-comment"># Require to calculate dim of feedfoward network</span><br>    norm_eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-5</span>                       <span class="hljs-comment"># Default Epsilon value set for the RMSNorm calculation</span><br>    rope_theta: <span class="hljs-built_in">float</span> = <span class="hljs-number">10000.0</span>   <span class="hljs-comment"># Default theta value for the RePE calculation</span><br><br>    max_batch_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">10</span>     <span class="hljs-comment"># Max batch size</span><br>    max_seq_len: <span class="hljs-built_in">int</span> = <span class="hljs-number">256</span>         <span class="hljs-comment"># Max sequence length</span><br><br>    epochs: <span class="hljs-built_in">int</span> = <span class="hljs-number">2500</span>             <span class="hljs-comment"># Total number of training iteration</span><br>    log_interval: <span class="hljs-built_in">int</span> = <span class="hljs-number">10</span>        <span class="hljs-comment"># Number of interval to print the logs and loss values   </span><br>    device: <span class="hljs-built_in">str</span> = <span class="hljs-string">'cuda'</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">'cpu'</span>   <span class="hljs-comment"># Assign device to cuda or cpu based on availability </span></span></pre><pre class="gp ps pt pu bp pv bb bk"><span id="ba87" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step2a: The RMSNorm</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title.class">RMSNorm</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">__init__</span>(<span class="hljs-params">self, dim: <span class="hljs-built_in">int</span>, eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-6</span></span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    device = ModelArgs.device<br>    self.eps = eps<br>    <span class="hljs-comment"># Scaling parameter gamma, initialized with one and the no of parameters is equal to the size of dim</span><br>    self.weight = nn.Parameter(torch.ones(dim).to(device))<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">_norm</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-keyword">return</span> x * torch.rsqrt(x.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + self.eps).to(device)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment">#Shape: x[bs,seq,dim]</span><br>    output = self._norm(x.<span class="hljs-built_in">float</span>()).type_as(x)<br><br>    <span class="hljs-comment">#Shape: x[bs,seq,dim] -&gt; x_norm[bs,seq,dim]</span><br>    <span class="hljs-keyword">return</span> output * self.weight<br><br><span class="hljs-comment">### Test: RMSNorm Code ###</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)<br>rms_norm = RMSNorm(dim=ModelArgs.dim)<br>x_norm = rms_norm(x)<br><br>print(f"Shape of x: {x.shape}")<br>print(f"Shape of x_norm: {x_norm.shape}")<br>"""</span><br><span class="hljs-comment">### Test Results: ###</span><br><span class="hljs-string">"""<br>Shape of x: torch.Size([10, 256, 512])<br>Shape of x_norm: torch.Size([10, 256, 512])<br>"""</span></span></pre><h2 id="a3e5" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">2b. Rotary Positional Encoding (RoPE):</h2><p id="6e81" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Why we do need Rotary Positional Encoding (RoPE)? </strong>Before we get into the why part, let’s review what we’ve done so far. First, we’ve converted input texts into embeddings. Next, we’ve applied RMSNorm to the embeddings. At this point, you must have noticed something is off. Let’s say the input text is “I love apple” or “apple love I”, the model will still treat both sentences as the same and learn it as the same. Because there is no order defined in the embeddings for the model to learn. Hence, the order is very important for any language model. In Llama 3 model architecture, RePE is used to define the position of each token in the sentences that maintain not only the order but also maintains the relative position of tokens in the sentences.</p><p id="d03d" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">So, what is Rotary Positional Encoding and how does it work?</strong> As mentioned in the why section above, RoPE is a type of position encoding that encodes the embeddings which maintains the order of tokens in the sentences by adding <strong class="nd hr">absolute positional information</strong> as well as incorporates the <strong class="nd hr">relative position information</strong> among the tokens. It performs the encoding action by rotating a given embedding by a special matrix called the rotation matrix. <strong class="nd hr">This simple yet very powerful mathematical derivation using rotation matrix is the heart of RoPE.</strong></p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qc"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fpgtE7L7Br3Azn1KNcTOHw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*fpgtE7L7Br3Azn1KNcTOHw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*fpgtE7L7Br3Azn1KNcTOHw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*fpgtE7L7Br3Azn1KNcTOHw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*fpgtE7L7Br3Azn1KNcTOHw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*fpgtE7L7Br3Azn1KNcTOHw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*fpgtE7L7Br3Azn1KNcTOHw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*fpgtE7L7Br3Azn1KNcTOHw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="501" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_fpgtE7L7Br3Azn1KNcTOHw.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: Rotation matrix applied to 2-d vector</strong></figcaption></figure><p id="427a" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">The rotation matrix in the diagram above rotates a vector of 2-dimension. However, the number of dimensions in the Llama 3 model is 4096 which is a lot more. Let’s take a look at how to apply rotation on higher-dimension embeddings.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qd"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s1VfCNxSdWCpvkAR7ELLpA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*s1VfCNxSdWCpvkAR7ELLpA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*s1VfCNxSdWCpvkAR7ELLpA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*s1VfCNxSdWCpvkAR7ELLpA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*s1VfCNxSdWCpvkAR7ELLpA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*s1VfCNxSdWCpvkAR7ELLpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*s1VfCNxSdWCpvkAR7ELLpA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*s1VfCNxSdWCpvkAR7ELLpA.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="571" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_s1VfCNxSdWCpvkAR7ELLpA.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: Example of RoPE implementation to Embedding</strong></figcaption></figure><p id="0bff" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">We now know that the rotation of embeddings involves the multiplication of each embedding position (m) value and theta (θ) for each pair of embedding dimensions. This is how RoPE can capture absolute position as well as relative position information by the implementation of the rotation matrix.</p><p id="d518" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Note</strong>: the rotation matrix needs to be converted to polar form and the embedding vector needs to converted to complex before performing rotation. After rotation is completed, the rotated embeddings need to be converted back to real for attention operation. Also, RoPE is applied to Query and Key embedding only. It doesn’t apply to Value embedding.</p><p id="9a43" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Let’s dive into RoPE coding:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="6781" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step2b: The RoPE</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">precompute_freqs_cis</span>(<span class="hljs-params">dim:<span class="hljs-built_in">int</span>, seq_len: <span class="hljs-built_in">int</span>, theta: <span class="hljs-built_in">float</span>=<span class="hljs-number">10000.0</span></span>):<br>  <span class="hljs-comment"># Computing Theta value for each dim pair which is dim/2</span><br>  device = ModelArgs.device<br>  freqs = <span class="hljs-number">1.0</span> / (theta ** (torch.arange(<span class="hljs-number">0</span>, dim, <span class="hljs-number">2</span>,device=device)[:(dim//<span class="hljs-number">2</span>)].<span class="hljs-built_in">float</span>()/dim))<br><br>  <span class="hljs-comment"># Computing range of positions(m) in the sequence</span><br>  t = torch.arange(seq_len, dtype=torch.float32, device=device)<br><br>  <span class="hljs-comment"># freqs gives all the Theta value range for all the position of tokens in the sequence</span><br>  freqs = torch.outer(t, freqs).to(device)<br><br>  <span class="hljs-comment"># This is the rotation matrix which needs to be converted to Polar form in order to perform rotation to the embedding</span><br>  freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)<br>  <span class="hljs-keyword">return</span> freqs_cis<br><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">reshape_for_broadcast</span>(<span class="hljs-params">freqs_cis, x</span>):<br>  ndim = x.ndim<br>  <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span>&lt;=<span class="hljs-number">1</span>&lt;ndim<br>  <span class="hljs-keyword">assert</span> freqs_cis.shape == (x.shape[<span class="hljs-number">1</span>],x.shape[-<span class="hljs-number">1</span>]), <span class="hljs-string">"the last two dimension of freqs_cis, x must match"</span><br>  shape = [d <span class="hljs-keyword">if</span> i==<span class="hljs-number">1</span> <span class="hljs-keyword">or</span> i==ndim-<span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i,d <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(x.shape)]<br>  <span class="hljs-keyword">return</span> freqs_cis.view(*shape)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">apply_rotary_emb</span>(<span class="hljs-params">xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor</span>)-&gt;<span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor]:<br>  device = ModelArgs.device<br>  <span class="hljs-comment"># Applying rotary positional encoding to both query and key embedding together</span><br>  <span class="hljs-comment"># First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.</span><br>  <span class="hljs-comment"># Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number</span><br>  xq_ = torch.view_as_complex(xq.<span class="hljs-built_in">float</span>().reshape(*xq.shape[:-<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)).to(device)    <span class="hljs-comment">#xq_:[bsz, seq_len, n_heads, head_dim/2]</span><br>  xk_ = torch.view_as_complex(xk.<span class="hljs-built_in">float</span>().reshape(*xk.shape[:-<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)).to(device)    <span class="hljs-comment">#xk_:[bsz, seq_len, n_heads, head_dim/2]</span><br><br>  <span class="hljs-comment"># The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding</span><br>  <span class="hljs-comment"># Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis:[seq_len,head_dim] -&gt; freqs_cis:[1,seq_len,1,head_dim]</span><br>  freqs_cis = reshape_for_broadcast(freqs_cis, xq_)<br><br>  <span class="hljs-comment">#Finally, perform rotation operation by multiplying with freqs_cis.</span><br>  <span class="hljs-comment">#After the rotation is completed, convert both xq_out and xk_out back to real number and return</span><br>  xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="hljs-number">3</span>).to(device) <span class="hljs-comment">#xq_out:[bsz, seq_len, n_heads, head_dim]</span><br>  xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="hljs-number">3</span>).to(device) <span class="hljs-comment">#xk_out:[bsz, seq_len, n_heads, head_dim]</span><br>  <span class="hljs-keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)<br><br><span class="hljs-comment">### Test: RoPE Code ###</span><br><span class="hljs-comment"># Note: x_norm is calculated during RMSNorm and is being used for testing here.</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>head_dim = ModelArgs.dim//ModelArgs.n_heads<br>wq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=device)<br>wk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=device)<br>xq = wq(x_norm)<br>xk = wk(x_norm)<br>print(f"xq.shape: {xq.shape}")<br>print(f"xk.shape: {xk.shape}")<br><br>xq = xq.view(xq.shape[0],xq.shape[1],ModelArgs.n_heads, head_dim)<br>xk = xk.view(xk.shape[0],xk.shape[1],ModelArgs.n_kv_heads, head_dim)<br>print(f"xq.re-shape: {xq.shape}")<br>print(f"xk.re-shape: {xk.shape}")<br><br>freqs_cis = precompute_freqs_cis(dim=head_dim, seq_len=ModelArgs.max_seq_len)<br>print(f"freqs_cis.shape: {freqs_cis.shape}")<br><br>xq_rotate, xk_rotate = apply_rotary_emb(xq, xk, freqs_cis)<br>print(f"xq_rotate.shape: {xq_rotate.shape}")<br>print(f"xk_rotate.shape: {xk_rotate.shape}")<br>"""</span><br><span class="hljs-comment">### Test Results: ###</span><br><span class="hljs-string">"""<br>xq.shape: torch.Size([10, 256, 512])<br>xk.shape: torch.Size([10, 256, 256])<br>xq.re-shape: torch.Size([10, 256, 8, 64])<br>xk.re-shape: torch.Size([10, 256, 4, 64])<br>freqs_cis.shape: torch.Size([256, 32])<br>xq_rotate.shape: torch.Size([10, 256, 8, 64])<br>xk_rotate.shape: torch.Size([10, 256, 4, 64])<br>"""</span></span></pre><h2 id="b216" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">2c. KV Cache (Only required at Inferencing):</h2><p id="c64f" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">What is KV-Cache? </strong>In Llama 3 architecture, at the time of inferencing, the concept of KV-Cache is introduced to store previously generated tokens in the form of Key and Value cache. These caches will be used to calculate self-attention to generate the next token. Only key and value tokens are cached whereas query tokens are not cached, hence the term KV Cache.</p><p id="e659" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Why do we need KV Cache? </strong>Let’s look at the diagram below to clarify our curiosity.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qe"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AC-ns-4_Qdwiol3ZIBolHQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*AC-ns-4_Qdwiol3ZIBolHQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*AC-ns-4_Qdwiol3ZIBolHQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*AC-ns-4_Qdwiol3ZIBolHQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*AC-ns-4_Qdwiol3ZIBolHQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*AC-ns-4_Qdwiol3ZIBolHQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*AC-ns-4_Qdwiol3ZIBolHQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*AC-ns-4_Qdwiol3ZIBolHQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="396" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_AC-ns-4_Qdwiol3ZIBolHQ.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: KV Cache implementation</strong></figcaption></figure><ul class=""><li id="dd62" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph=""><strong class="nd hr">In the A block of the diagram</strong>, when the output3 token is being generated, the previous output tokens (output1, output2) are still being calculated which is not necessary at all. This has caused an additional matrix multiplication during attention calculation hence computation resources are increased a lot.</li><li id="57bd" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph=""><strong class="nd hr">In block B of the diagram</strong>, the output tokens replace the input token in Query embedding. <strong class="nd hr">KV Cache</strong> stores the previously generated tokens. During attention score calculation, we will just have to use 1 token from the query and use previous tokens from the Key and Value cache. It reduces the matrix multiplication from 3x3 to 1x3 from block A to block B, which is almost 66% reduction. In the real world, with huge sequence lengths and batch size, this will help to reduce significant computation power. Finally, there will always be only one latest output token generated. <strong class="nd hr">This is the main reason KV-Cache has been introduced.</strong></li></ul><h2 id="8307" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">2d. Group Query Attention:</h2><p id="b051" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">Group query attention is the same as Muilt-Head attention which was used in previous models such as Llama 1 with the only difference being in the use of separate heads for queries and separate heads for keys/values. Usually, the number of heads assigned to queries is n-times to that of keys, and values heads. Let’s take a look at the diagram to build our understanding further.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qf"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NGDw7teWrXccU5Xf6JIOMw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*NGDw7teWrXccU5Xf6JIOMw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NGDw7teWrXccU5Xf6JIOMw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NGDw7teWrXccU5Xf6JIOMw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NGDw7teWrXccU5Xf6JIOMw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NGDw7teWrXccU5Xf6JIOMw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NGDw7teWrXccU5Xf6JIOMw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*NGDw7teWrXccU5Xf6JIOMw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="439" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_NGDw7teWrXccU5Xf6JIOMw.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: Group query attention and Multi-Head Attention</strong></figcaption></figure><p id="49b2" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">In the given diagram, the multi-head attention has an equal number of heads across all queries, keys and values which is n_heads = 8.</p><p id="f601" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">The Group query attention block has 8 heads for queries (n_heads) and 4- heads (n_kv_heads) for keys and values, which is 2 times less than query heads.</p><p id="d744" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Since MultiHead Attention is already so good, why do we need Group query attention? </strong>To answer this, we need to go back to KV Cache for a while. The KV cache helps reduce computation resources greatly. However, as KV Cache stores more and more previous tokens, the memory resources will increase significantly. This is not a good thing for the model performance point of view as well as the financial point of view. <strong class="nd hr">Hence, Group query attention is introduced.</strong> Reducing the number of heads for K and V decreases the number of parameters to be stored, and hence, less memory is being used. Various test results have proven that the model accuracy remains in the same ranges with this approach.</p><p id="5eae" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Let’s implement this in code:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="7c1a" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## The Attention Block [Step2c: The KV Cache; Step2d: Group Query Attention]</span><br><span class="hljs-comment">## As mentioned before, the naming convention follows original the meta's LLama3 GitHub</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title.class">Attention</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">__init__</span>(<span class="hljs-params">self, args: ModelArgs</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.args = args<br>    <span class="hljs-comment"># Embedding dimension</span><br>    self.dim = args.dim<br>    <span class="hljs-comment"># Number of heads assigned to Query</span><br>    self.n_heads = args.n_heads<br>    <span class="hljs-comment"># Number of heads assigned to Key and values. If "None", the number will be same as Query.</span><br>    self.n_kv_heads = args.n_heads <span class="hljs-keyword">if</span> args.n_kv_heads <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> args.n_kv_heads<br>    <span class="hljs-comment"># Dimension of each head relative to model dimension</span><br>    self.head_dim = args.dim // args.n_heads<br>    <span class="hljs-comment"># Number of repetition in order to make time Key, Value heads to match Query heads number</span><br>    self.n_rep = args.n_heads // args.n_kv_heads<br><br>    <span class="hljs-comment"># Weight initialize for Keys, Querys, Values and Oupt. Notice that the out_feature value of weight for q and kv are based on it's heads</span><br>    self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=<span class="hljs-literal">False</span>, device=device)<br>    self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=<span class="hljs-literal">False</span>, device=device)<br>    self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=<span class="hljs-literal">False</span>, device=device)<br>    self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=<span class="hljs-literal">False</span>, device=device)<br><br>    <span class="hljs-comment"># Initialize caches to store Key, Values at start. (KV Cache Implementation)</span><br>    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)<br>    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">forward</span>(<span class="hljs-params">self, x: torch.Tensor, start_pos, inference</span>):<br>    <span class="hljs-comment"># Shape of the input embedding: [bsz,seq_len,dim]</span><br>    bsz, seq_len, _ = x.shape<br>    <span class="hljs-comment"># Mask will be used during 'Training' and is not required for 'inference' due to the use of KV cache.</span><br>    mask = <span class="hljs-literal">None</span><br><br>    xq = self.wq(x)  <span class="hljs-comment">#x[bsz,seq_len,dim]*wq[dim,n_heads * head_dim] -&gt; q[bsz,seq_len,n_heads * head_dim]</span><br>    xk = self.wk(x)  <span class="hljs-comment">#x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -&gt; k[bsz,seq_len,n_kv_heads * head_dim]</span><br>    xv = self.wv(x)  <span class="hljs-comment">#x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -&gt; v[bsz,seq_len,n_kv_heads * head_dim]</span><br><br>    <span class="hljs-comment"># Reshaping Querys, Keys and Values by their number of heads. (Group Query Attention Implementation)</span><br>    xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)      <span class="hljs-comment">#xq[bsz,seq_len,n_heads, head_dim]</span><br>    xk = xk.view(bsz, seq_len, self.n_kv_heads, self.head_dim)   <span class="hljs-comment">#xk[bsz,seq_len,n_kv_heads, head_dim]</span><br>    xv = xv.view(bsz, seq_len, self.n_kv_heads, self.head_dim)   <span class="hljs-comment">#xv[bsz,seq_len,n_kv_heads, head_dim]</span><br><br>    <span class="hljs-comment"># Model - Inference Mode: kv-cache is enabled at inference mode only.</span><br>    <span class="hljs-keyword">if</span> inference:<br>      <span class="hljs-comment"># Compute rotation matrix for each position in the sequence</span><br>      freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len * <span class="hljs-number">2</span>)<br>      <span class="hljs-comment"># During inferencing, we should only take the rotation matrix range from the current position of the tokens.</span><br>      freqs_cis = freqs_cis[start_pos : start_pos + seq_len]<br>      <span class="hljs-comment"># Apply RoPE to Queries and Keys embeddings</span><br>      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)<br><br>      self.cache_k = self.cache_k.to(xq)<br>      self.cache_v = self.cache_v.to(xq)<br>      <span class="hljs-comment"># Store Keys and Values token embedding into their respective cache [KV Cache Implementation]</span><br>      self.cache_k[:bsz, start_pos:start_pos + seq_len] = xk<br>      self.cache_v[:bsz, start_pos:start_pos + seq_len] = xv<br><br>      <span class="hljs-comment"># Assign all the previous tokens embeddings upto current tokens position to Keys and Values variable for Attention Calculation</span><br>      keys = self.cache_k[:bsz, :start_pos + seq_len]<br>      values = self.cache_v[:bsz, :start_pos + seq_len]<br><br>      <span class="hljs-comment"># At this point, they Keys and Values shape aren't same with Queries Embedding which has to be in order to computer attention score</span><br>      <span class="hljs-comment"># Use repeat_kv function to make Keys,Values shape same as queries shape</span><br>      keys = repeat_kv(keys, self.n_rep)      <span class="hljs-comment">#keys[bsz,seq_len,n_heads,head_dim]</span><br>      values = repeat_kv(values, self.n_rep)  <span class="hljs-comment">#values[bsz,seq_len,n_heads,head_dim]</span><br><br>    <span class="hljs-comment"># Mode - Training mode: KV-Cache not implemented</span><br>    <span class="hljs-keyword">else</span>:<br>      <span class="hljs-comment"># Compute rotation matrix and apply RoPE to queries and keys for for training.</span><br>      freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len)<br><br>      <span class="hljs-comment">#xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]</span><br>      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)<br><br>      <span class="hljs-comment"># Use repeat_kv function to make Keys,Values shape same as the queries shape</span><br>      <span class="hljs-comment">#keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]</span><br>      keys = repeat_kv(xk, self.n_rep)<br>      values = repeat_kv(xv, self.n_rep)<br><br>      <span class="hljs-comment"># For training mode, we'll compute mask and apply to the attention score later</span><br>      mask = torch.full((seq_len, seq_len),<span class="hljs-built_in">float</span>(<span class="hljs-string">"-inf"</span>),device=self.args.device)<br>      mask = torch.triu(mask, diagonal=<span class="hljs-number">1</span>).to(self.args.device)<br><br>    <span class="hljs-comment"># To compute attention, we'll need to perform a transpose operation to reshape all queries, keys and values bring heads at dim 1 and seq at dim 2</span><br>    xq = xq.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)                  <span class="hljs-comment">#xq[bsz,n_heads,seq_len,head_dim]</span><br>    keys = keys.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)              <span class="hljs-comment">#keys[bsz,n_heads,seq_len,head_dim]</span><br>    values = values.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)          <span class="hljs-comment">#values[bsz,n_heads,seq_len,head_dim]</span><br><br>    <span class="hljs-comment"># Computing attention score</span><br>    scores = torch.matmul(xq, keys.transpose(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)).to(self.args.device)/math.sqrt(self.head_dim)<br>    <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>      scores = scores + mask<br><br>    <span class="hljs-comment"># Apply softmax to the attention score</span><br>    scores = F.softmax(scores.<span class="hljs-built_in">float</span>(), dim=-<span class="hljs-number">1</span>).type_as(xq)<br>    <span class="hljs-comment"># Matrix multiplication of attention score with the values</span><br>    output = torch.matmul(scores, values).to(self.args.device)<br><br>    <span class="hljs-comment"># We get the contextual embedding for each head</span><br>    <span class="hljs-comment"># All heads need to be reshaped back and combined to give a single single contextual attention output</span><br>    <span class="hljs-comment"># Shape change: output[bsz,n_heads,seq_len,head_dim] -&gt; output[bsz,seq_len, n_heads,head_dim] -&gt; output[bsz,seq_len, n_heads * head_dim]</span><br>    output = output.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>).contiguous().view(bsz, seq_len, -<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># shape: output [bsz,seq_len,dim]</span><br>    <span class="hljs-keyword">return</span> self.wo(output)<br><br><span class="hljs-comment"># If the number of keys/values heads is less than query heads, this function expands the key/values embeddings with the required number of repetition</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">repeat_kv</span>(<span class="hljs-params">x:torch.Tensor, n_rep: <span class="hljs-built_in">int</span></span>)-&gt; torch.Tensor:<br>  bsz, seq_len, n_kv_heads, head_dim = x.shape<br>  <span class="hljs-keyword">if</span> n_rep == <span class="hljs-number">1</span>:<br>    <span class="hljs-keyword">return</span> x<br>  <span class="hljs-keyword">return</span> (<br>      x[:,:,:,<span class="hljs-literal">None</span>,:]<br>      .expand(bsz,seq_len,n_kv_heads,n_rep, head_dim)<br>      .reshape(bsz,seq_len,n_kv_heads * n_rep, head_dim)<br>  )<br><br><br><span class="hljs-comment">### Test: Repeat_kv function ###</span><br><span class="hljs-comment"># note: xk, x_norm is already calculated during RoPE, RMSNorm testing and is being used for testing here.</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>n_rep = ModelArgs.n_heads // ModelArgs.n_kv_heads<br>keys = repeat_kv(xk, n_rep)<br>print(f"xk.shape: {xk.shape}")<br>print(f"keys.shape: {keys.shape}")<br><br>## Test: Attention function<br># You need take out the triple quotes below to perform testing<br><br>attention = Attention(ModelArgs)<br>x_out = attention(x_norm,start_pos=0, inference=False)<br>print(f"x_out.shape: {x_out.shape}")<br>"""</span><br><span class="hljs-comment">### Test Results: ###</span><br><span class="hljs-string">"""<br>xk.shape: torch.Size([10, 256, 4, 64])<br>keys.shape: torch.Size([10, 256, 8, 64])<br>x_out.shape: torch.Size([10, 256, 512])<br>"""</span></span></pre><h2 id="43bf" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">2e. FeedForward Network (SwiGLU Activation):</h2><p id="f5fe" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">What does FeedForward Network do in the decoder block?</strong> As shown in the architecture diagram above, the attention output is first normalized during RMSNorm and then fed into the FeedForward network. Inside the feedforward network, the attention output embeddings will be expanded to the higher dimension throughout its hidden layers and learn more complex features of the tokens.</p><p id="d210" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Why use SwiGLU instead of ReLU? </strong>Let’s take a look at the diagram to get the answer.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qg"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liwKWW5zzQCmDgJ2yOGnFw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*liwKWW5zzQCmDgJ2yOGnFw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*liwKWW5zzQCmDgJ2yOGnFw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*liwKWW5zzQCmDgJ2yOGnFw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*liwKWW5zzQCmDgJ2yOGnFw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*liwKWW5zzQCmDgJ2yOGnFw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*liwKWW5zzQCmDgJ2yOGnFw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*liwKWW5zzQCmDgJ2yOGnFw.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="677" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_liwKWW5zzQCmDgJ2yOGnFw.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: FeedFoward Network with SwiGLU function</strong></figcaption></figure><p id="c174" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">As shown in the diagram above, the SwiGLU function behaves almost like ReLU in the positive axis. However, in the negative axis, SwiGLU outputs some negative values, which might be useful in learning smaller rather than flat 0 in the case of ReLU. Overall, as per the author, the performance with SwiGLU has been better than that with ReLU; hence, it was chosen.</p><p id="a2ec" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Let’s dive into FeedForward code:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="b308" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step2e: The Feedfoward Network (SwiGLU activation)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title.class">FeedForward</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">__init__</span>(<span class="hljs-params">self, dim:<span class="hljs-built_in">int</span>, hidden_dim:<span class="hljs-built_in">int</span>, multiple_of:<span class="hljs-built_in">int</span>, ffn_dim_multiplier: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>]</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    <span class="hljs-comment"># Models embedding dimension</span><br>    self.dim = dim<br><br>    <span class="hljs-comment"># We must use the hidden dimensions calculation shared by Meta which is the ideal one for this model</span><br>    <span class="hljs-comment"># Hidden dimension are calculated such that it is a multiple of 256.</span><br>    hidden_dim = <span class="hljs-built_in">int</span>(<span class="hljs-number">2</span> * hidden_dim/<span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">if</span> ffn_dim_multiplier <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>      hidden_dim = <span class="hljs-built_in">int</span>(ffn_dim_multiplier * hidden_dim)<br>    hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="hljs-number">1</span>) // multiple_of)<br><br>    <span class="hljs-comment"># define hiddne layers weights</span><br>    self.w1 = nn.Linear(self.dim, hidden_dim, bias=<span class="hljs-literal">False</span>, device=device)<br>    self.w2 = nn.Linear(hidden_dim, self.dim, bias=<span class="hljs-literal">False</span>, device=device)<br>    self.w3 = nn.Linear(self.dim, hidden_dim, bias=<span class="hljs-literal">False</span>, device=device)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">forward</span>(<span class="hljs-params">self, x</span>):<br>    <span class="hljs-comment"># Shape: [bsz,seq_len,dim]</span><br>    <span class="hljs-keyword">return</span> self.w2(F.silu(self.w1(x)) * self.w3(x))<br><br><br><br><span class="hljs-comment">### Test: FeedForward module ###</span><br><span class="hljs-comment"># note: x_out is already computed at Attention testing and is being used for testing here.</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>feed_forward = FeedForward(ModelArgs.dim, 4 * ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)<br>x_out = rms_norm(x_out)<br>x_out = feed_forward(x_out)<br>print(f"feed forward output: x_out.shape: {x_out.shape}")<br>"""</span><br><br><span class="hljs-comment">### Test Results: ###</span><br><span class="hljs-string">"""<br>feed forward output: x_out.shape: torch.Size([10, 256, 512])<br>"""</span></span></pre><h2 id="5036" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">2f. Decoder Block:</h2><p id="fbc1" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">As shown in the architecture diagram above (the very first diagram). The decoder block consists of multiple sub-components, which we’ve learned and coded in earlier sections (2a — 2f). Below is a pointwise operation that is being carried out inside the decoder block.</p><ol class=""><li id="fb66" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pi pj pk bk" data-selectable-paragraph="">The embedding from the input block is fed into the Attention-RMSNorm block. This will be further fed into the Group Query Attention block.</li><li id="2d4c" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">The same embedding from the input block will then be added to the attention output.</li><li id="ebeb" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">After that, the attention output is fed into FeedFoward-RMSNorm and further fed into the FeedFoward network block.</li><li id="9ad1" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">The output of the FeedFoward network is then added again with the attention output.</li><li id="c8ca" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pi pj pk bk" data-selectable-paragraph="">The resulting output is called <strong class="nd hr">Decoder Output. </strong>This decoder output is then fed into another decoder block as input. This same operation will be repeated for the next 31 decoder blocks. The final decoder output of the 32nd decoder block is then passed to the Output block.</li></ol><p id="28e3" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Let’s see this action in the code below:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="3a19" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step2f: The Decoder Block. The class name is assigned as TransformerBlock to match the name of Meta llama 3 code base.</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title.class">TransformerBlock</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">__init__</span>(<span class="hljs-params">self, args: ModelArgs</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    self.args = args<br>    <span class="hljs-comment"># Initilizate RMSNorm for attention</span><br>    self.attention_norm = RMSNorm(dim=args.dim, eps = args.norm_eps)<br>    <span class="hljs-comment"># Initilizate Attention class</span><br>    self.attention = Attention(args)<br>    <span class="hljs-comment"># Initilizate RMSNorm for feedfoward class</span><br>    self.ff_norm = RMSNorm(dim=args.dim, eps = args.norm_eps)<br>    <span class="hljs-comment"># Initilizate feedfoward class</span><br>    self.feedforward = FeedForward(args.dim, <span class="hljs-number">4</span> * args.dim, args.multiple_of, args.ffn_dim_multiplier)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">forward</span>(<span class="hljs-params">self, x, start_pos, inference</span>):<br>    <span class="hljs-comment"># start_pos = token position for inference mode, inference = True for inference and False for training mode</span><br>    <span class="hljs-comment"># i) pass input embedding to attention_norm and then pass to attention block.</span><br>    <span class="hljs-comment"># ii) the output of attention is then added to embedding(before norm)</span><br>    h = x + self.attention(self.attention_norm(x), start_pos, inference)<br><br>    <span class="hljs-comment"># i) pass attention output to ff_norm and then pass to the feedforward network.</span><br>    <span class="hljs-comment"># ii) the output of feedforward network is then added to the attention output(before ff_norm)</span><br>    out = h + self.feedforward(self.ff_norm(h))<br>    <span class="hljs-comment"># Shape: [bsz,seq_len,dim]</span><br>    <span class="hljs-keyword">return</span> out<br><br><br><span class="hljs-comment">### Test: TransformerBlock ###</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)<br>transformer_block = TransformerBlock(ModelArgs)<br>transformer_block_out = transformer_block(x,start_pos=0, inference=False)<br>print(f"transformer_block_out.shape: {transformer_block_out.shape}")<br>"""</span><br><br><span class="hljs-comment">### Test Results: ###</span><br><span class="hljs-string">"""<br>transformer_block_out.shape: torch.Size([10, 64, 128])<br>"""</span></span></pre><h2 id="5d1f" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">Step 3: The Output Block</h2><p id="bc28" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">The decoder output of the final decoder block will feed into the output block. It is first fed into the RMSNorm. Then, it will feed into the Linear Layer which generates logits. Next, one of the following two operations happens.</p><ul class=""><li id="cf23" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph="">If the mode is <strong class="nd hr">inference</strong>, top_p probability is calculated and the next token is generated. The next tokens generated will stop if the max generation length is reached or the end of sentence token is generated as the next token.</li><li id="6b7a" class="nb nc hq nd b ne pl ng nh ni pm nk nl fl pn nn no fo po nq nr fr pp nt nu nv pq pj pk bk" data-selectable-paragraph="">If the mode is <strong class="nd hr">Training, </strong>loss is computed with the target labels and training is repeated till the max epochs length is reached.</li></ul><p id="4d36" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">Let’s take a look at the output block flow diagram for more clarity.</p><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qh"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSiiz1znMsW0EMUDq2Cx7w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*TSiiz1znMsW0EMUDq2Cx7w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*TSiiz1znMsW0EMUDq2Cx7w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*TSiiz1znMsW0EMUDq2Cx7w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*TSiiz1znMsW0EMUDq2Cx7w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*TSiiz1znMsW0EMUDq2Cx7w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*TSiiz1znMsW0EMUDq2Cx7w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*TSiiz1znMsW0EMUDq2Cx7w.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="809" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_TSiiz1znMsW0EMUDq2Cx7w.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph=""><strong class="bf ou">[Image by writer]: LLama 3 output flow diagram for training and inference mode</strong></figcaption></figure><p id="7c43" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Finally, let’s combine all components of 3 blocks (input block, decoder block and output blocks. This gives our final Llama 3 model.</strong></p><p id="187a" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">let’s code the final Llama 3 model:</strong></p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="2569" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step3: The Output Block</span><br><span class="hljs-comment"># This is the Llama 3 model. Again, the class name is maintained as Transformer to match with Meta Llama 3 model.</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title.class">Transformer</span>(nn.Module):<br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">__init__</span>(<span class="hljs-params">self, params: ModelArgs</span>):<br>    <span class="hljs-built_in">super</span>().__init__()<br>    <span class="hljs-comment"># set all the ModelArgs in params variable</span><br>    self.params = params<br>    <span class="hljs-comment"># Initilizate embedding class from the input block</span><br>    self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)<br><br>    <span class="hljs-comment"># Initialize the decoder block and store it inside the ModuleList. </span><br>    <span class="hljs-comment"># This is because we've 4 decoder blocks in our Llama 3 model. (Official Llama 3 has 32 blocks)</span><br>    self.layers = nn.ModuleList()<br>    <span class="hljs-keyword">for</span> layer_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(params.n_layers):<br>      self.layers.append(TransformerBlock(args=params))<br><br>    <span class="hljs-comment"># Initilizate RMSNorm for the output block</span><br>    self.norm = RMSNorm(params.dim, eps = params.norm_eps)<br>    <br>    <span class="hljs-comment"># Initilizate linear layer at the output block.</span><br>    self.output = nn.Linear(params.dim, params.vocab_size, bias=<span class="hljs-literal">False</span>)<br><br>  <span class="hljs-keyword">def</span> <span class="hljs-title.function">forward</span>(<span class="hljs-params">self, x, start_pos=<span class="hljs-number">0</span>, targets=<span class="hljs-literal">None</span></span>):<br>    <br>    <span class="hljs-comment"># start_pos = token position for inference mode, inference = True for inference and False for training mode</span><br>    <span class="hljs-comment"># x is the batch of token_ids generated from the texts or prompts using tokenizers.</span><br>    <span class="hljs-comment"># x[bsz, seq_len] -&gt; h[bsz, seq_len, dim]</span><br>    h = self.tok_embeddings(x)<br><br>    <span class="hljs-comment"># If the target is none, Inference mode is activated and set to "True" and "False" if Training mode is activated.</span><br>    <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>      inference = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">else</span>:<br>      inference = <span class="hljs-literal">False</span><br><br>    <span class="hljs-comment"># The embeddings (h) will then pass though all the decoder blocks.</span><br>    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>      h = layer(h, start_pos, inference)<br><br>    <span class="hljs-comment"># The output from the final decoder block will feed into the RMSNorm</span><br>    h = self.norm(h)<br><br>    <span class="hljs-comment"># After normalized, the embedding h will then feed into the Linear layer. </span><br>    <span class="hljs-comment"># The main task of the Linear layer is to generate logits that maps the embeddings with the vocabulary size.</span><br>    <span class="hljs-comment"># h[bsz, seq_len, dim] -&gt; logits[bsz, seq_len, vocab_size]</span><br>    logits = self.output(h).<span class="hljs-built_in">float</span>()<br>    loss = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># Inference mode is activated if the targets is not available</span><br>    <span class="hljs-keyword">if</span> targets <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>      loss = <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># Training mode is activated if the targets are available. And Loss will be calculated for further model training. </span><br>    <span class="hljs-keyword">else</span>:<br>      loss = F.cross_entropy(logits.view(-<span class="hljs-number">1</span>, self.params.vocab_size), targets.view(-<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">return</span> logits, loss<br><br><br><span class="hljs-comment">### Test: Transformer (Llama Model) ###</span><br><span class="hljs-comment"># You need take out the triple quotes below to perform testing</span><br><span class="hljs-string">"""<br>model = Transformer(ModelArgs).to(ModelArgs.device)<br>print(model)<br>"""</span></span></pre><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qi"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WG1DxQDPocze_Y0nurNdFQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*WG1DxQDPocze_Y0nurNdFQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*WG1DxQDPocze_Y0nurNdFQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*WG1DxQDPocze_Y0nurNdFQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*WG1DxQDPocze_Y0nurNdFQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*WG1DxQDPocze_Y0nurNdFQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*WG1DxQDPocze_Y0nurNdFQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*WG1DxQDPocze_Y0nurNdFQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="348" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_WG1DxQDPocze_Y0nurNdFQ.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph="">[Image by Write]: LLama 3 layered architecture</figcaption></figure><p id="c46f" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">The Llama 3 model we’ve just built looks perfect. We’re now ready to start our training process.</p><h2 id="c441" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">Step 4: Train our Llama 3 Model:</h2><p id="4db3" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">The training flow is provided in the output block flow diagram(step 3). Please refer to that flow again if you would like to have more clarity before starting training. Let’s begin writing the training code. I’ll also provide the necessary explanation within the code block as well.</p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="0f96" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step 4: Train Llama 3 Model:</span><br><br><span class="hljs-comment"># Create a dataset by encoding the entire tiny_shakespeare data token_ids list using the tokenizer's encode function that we've built at the input block section</span><br>dataset = torch.tensor(encode(data), dtype=torch.<span class="hljs-built_in">int</span>).to(ModelArgs.device)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"dataset-shape: <span class="hljs-subst">{dataset.shape}</span>"</span>)<br><br><span class="hljs-comment"># Define function to generate batches from the given dataset</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">get_dataset_batch</span>(<span class="hljs-params">data, split, args:ModelArgs</span>):<br>  seq_len = args.max_seq_len<br>  batch_size = args.max_batch_size<br>  device = args.device<br><br>  train = data[:<span class="hljs-built_in">int</span>(<span class="hljs-number">0.8</span> * <span class="hljs-built_in">len</span>(data))]<br>  val = data[<span class="hljs-built_in">int</span>(<span class="hljs-number">0.8</span> * <span class="hljs-built_in">len</span>(data)): <span class="hljs-built_in">int</span>(<span class="hljs-number">0.9</span> * <span class="hljs-built_in">len</span>(data))]<br>  test = data[<span class="hljs-built_in">int</span>(<span class="hljs-number">0.9</span> * <span class="hljs-built_in">len</span>(data)):]<br><br>  batch_data = train<br>  <span class="hljs-keyword">if</span> split == <span class="hljs-string">"val"</span>:<br>    batch_data = val<br><br>  <span class="hljs-keyword">if</span> split == <span class="hljs-string">"test"</span>:<br>    batch_data = test<br>  <br>  <span class="hljs-comment"># Picking random starting points from the dataset to give random samples for training, validation and testing.</span><br>  <br>  ix = torch.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(batch_data) - seq_len - <span class="hljs-number">3</span>, (batch_size,)).to(device)<br>  x = torch.stack([torch.cat([token_bos, batch_data[i:i+seq_len-<span class="hljs-number">1</span>]]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ix]).long().to(device)<br>  y = torch.stack([torch.cat([batch_data[i+<span class="hljs-number">1</span>:i+seq_len], token_eos]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ix]).long().to(device)<br>  <br>  <span class="hljs-keyword">return</span> x,y<br><br><span class="hljs-comment">### Test: get_dataset function ###</span><br><span class="hljs-string">"""<br>xs, ys = get_dataset_batch(dataset, split="train", args=ModelArgs)<br>print([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])<br>"""</span><br><br><span class="hljs-comment"># Define a evaluate loss function to calculate and store training and validation loss for logging and plotting</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">evaluate_loss</span>(<span class="hljs-params">model, args:ModelArgs</span>):<br>  out = {}<br>  model.<span class="hljs-built_in">eval</span>()<br><br>  <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">"train"</span>, <span class="hljs-string">"val"</span>]:<br>    losses = []<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):      <br>      xb, yb = get_dataset_batch(dataset, split, args)<br>      _, loss = model(x=xb, targets=yb)<br>      losses.append(loss.item())<br>    out[split] = np.mean(losses)<br><br>  model.train()<br>  <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment"># Define a training function to perform model training</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">train</span>(<span class="hljs-params">model, optimizer, args:ModelArgs</span>):<br>    epochs = args.epochs<br>    log_interval = args.log_interval<br>    device = args.device<br>    losses = []   <br>    start_time = time.time()<br><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        optimizer.zero_grad()<br>        <br>        xs, ys = get_dataset_batch(dataset, <span class="hljs-string">'train'</span>, args)<br>        xs = xs.to(device)<br>        ys = ys.to(device)<br>        logits, loss = model(x=xs, targets=ys)<br>        loss.backward()<br>        optimizer.step()<br><br>        <span class="hljs-keyword">if</span> epoch % log_interval == <span class="hljs-number">0</span>:<br>            batch_time = time.time() - start_time<br>            x = evaluate_loss(model, args)<br>            losses += [x]            <br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Epoch <span class="hljs-subst">{epoch}</span> | val loss <span class="hljs-subst">{x[<span class="hljs-string">'val'</span>]:<span class="hljs-number">.3</span>f}</span> | Time <span class="hljs-subst">{batch_time:<span class="hljs-number">.3</span>f}</span>"</span>)<br>            start_time = time.time()<br>    <br>    <span class="hljs-comment"># Print the final validation loss</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"validation loss: "</span>, losses[-<span class="hljs-number">1</span>][<span class="hljs-string">'val'</span>])<br>    <span class="hljs-comment"># Display the interval losses in plot </span><br>    <span class="hljs-keyword">return</span> pd.DataFrame(losses).plot()</span></pre><p id="3390" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">Now, that we’ve defined the training function. Let’s start training with the following code block and observe the training results in the plot once the training is completed.</p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="35b7" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Start training our Llama 3 model</span><br>model = Transformer(ModelArgs).to(ModelArgs.device)<br>optimizer = torch.optim.Adam(model.parameters())<br><br>train(model, optimizer, ModelArgs)</span></pre><figure class="nz oa ob oc od oe nw nx paragraph-image"><div role="button" tabindex="0" class="of og ee oh bh oi"><span class="eo oj ok an ol es om eu on speechify-ignore">Press enter or click to view image in full size</span><div class="nw nx qj"><picture><source srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JS08OxC3GYrUiisOiks1TQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" type="image/webp"><source data-testid="og" srcset="https://miro.medium.com/v2/resize:fit:640/1*JS08OxC3GYrUiisOiks1TQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*JS08OxC3GYrUiisOiks1TQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*JS08OxC3GYrUiisOiks1TQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*JS08OxC3GYrUiisOiks1TQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*JS08OxC3GYrUiisOiks1TQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*JS08OxC3GYrUiisOiks1TQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*JS08OxC3GYrUiisOiks1TQ.png 1400w" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px"><img alt="" class="bh er oo c" width="700" height="574" loading="lazy" role="presentation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JS08OxC3GYrUiisOiks1TQ.png"></picture></div></div><figcaption class="op oq or nw nx os ot bf b bg ab dx" data-selectable-paragraph="">[image by writer]: Training vs Validation loss graph</figcaption></figure><p id="7eca" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">The above image displays the training and validation loss graph. The training has been conducted over 2500 epochs. It took around 10 min to complete the training process using Google Colab with default GPU and RAM settings which is very fast. The validation loss at the final epoch is 2.19 which is considered okay given the amount of training data we’re using and the number of epochs. To reduce the losses significantly, we will have to increase the size of the training data, higher number of epochs and higher GPU or processing power.</p><p id="b3cf" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Now that we’ve completed our training. Let’s head into our final step — Inference and see how well the model generates the output texts given new input prompts.</strong></p><h2 id="9f04" class="ov ow hq bf ou ff ox fg fh fi oy fj fk fl oz fm fn fo pa fp fq fr pb fs ft pc bk" data-selectable-paragraph="">Step 5: Inference Llama 3 Model:</h2><p id="7063" class="pw-post-body-paragraph nb nc hq nd b ne pd ng nh ni pe nk nl fl pf nn no fo pg nq nr fr ph nt nu nv gs bk" data-selectable-paragraph="">The inference flow is provided in the output block flow diagram(step 3). Let’s begin writing the inference code.</p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="5037" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Step 5: Inference Llama 3 Model:</span><br><span class="hljs-comment"># This function generates text sequences based on provided prompts using the LLama 3 model we've built and trained.</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">generate</span>(<span class="hljs-params">model, prompts: <span class="hljs-built_in">str</span>, params: ModelArgs, max_gen_len: <span class="hljs-built_in">int</span>=<span class="hljs-number">500</span>, temperature: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.6</span>, top_p: <span class="hljs-built_in">float</span> = <span class="hljs-number">0.9</span></span>):<br><br>    <span class="hljs-comment"># prompt_tokens: List of user input texts or prompts</span><br>    <span class="hljs-comment"># max_gen_len: Maximum length of the generated text sequence.</span><br>    <span class="hljs-comment"># temperature: Temperature value for controlling randomness in sampling. Defaults to 0.6.</span><br>    <span class="hljs-comment"># top_p: Top-p probability threshold for sampling prob output from the logits. Defaults to 0.9.</span><br>    <span class="hljs-comment"># prompt_tokens = [0]</span><br>    bsz = <span class="hljs-number">1</span>  <span class="hljs-comment">#For inferencing, in general user just input one prompt which we'll take it as 1-batch</span><br>    prompt_tokens = token_bos.tolist() + encode(prompts)<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(prompt_tokens) &lt;= params.max_seq_len, <span class="hljs-string">"prompt token length should be small than max_seq_len"</span><br>    total_len = <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(prompt_tokens)+max_gen_len, params.max_seq_len)   <br><br>    <span class="hljs-comment"># this tokens matrix is to store the input prompts and all the output that is generated by model.</span><br>    <span class="hljs-comment"># later we'll use the tokenizers decode function to decode this token to view results in text format</span><br>    tokens = torch.full((bsz,total_len), fill_value=token_pad.item(), dtype=torch.long, device=params.device)<br><br>    <span class="hljs-comment"># fill in the prompt tokens into the token matrix</span><br>    tokens[:,:<span class="hljs-built_in">len</span>(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=params.device)<br><br>    <span class="hljs-comment">#create a prompt_mask_token for later use to identify if the token is a prompt token or a padding token</span><br>    <span class="hljs-comment"># True if it is a prompt token, False if it is a padding token</span><br>    input_text_mask = tokens != token_pad.item()<br><br>    <span class="hljs-comment">#now we can start inferencing using one token at a time from the prompt_tokens list starting with the first position.</span><br>    prev_pos = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> cur_pos <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, total_len):<br>      <span class="hljs-keyword">with</span> torch.no_grad():<br>        logits, _ = model(x=tokens[:,prev_pos:cur_pos], start_pos=prev_pos)<br>      <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0</span>:      <br>        probs = torch.softmax(logits[:, -<span class="hljs-number">1</span>]/temperature, dim=-<span class="hljs-number">1</span>)<br>        next_token = sample_top_p(probs, top_p)        <br>      <span class="hljs-keyword">else</span>:<br>        next_token = torch.argmax(logits[:, -<span class="hljs-number">1</span>], dim=-<span class="hljs-number">1</span>)        <br><br>      next_token = next_token.reshape(-<span class="hljs-number">1</span>)<br><br>      <span class="hljs-comment"># only replace the token if it's a padding token</span><br>      next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)<br>      tokens[:, cur_pos] = next_token<br><br>      prev_pos = cur_pos<br>      <span class="hljs-keyword">if</span> tokens[:,cur_pos]==token_pad.item() <span class="hljs-keyword">and</span> next_token == token_eos.item():<br>        <span class="hljs-keyword">break</span><br><br>    output_tokens, output_texts = [], []    <br><br>    <span class="hljs-keyword">for</span> i, toks <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokens.tolist()):<br>      <span class="hljs-comment"># eos_idx = toks.index(token_eos.item())</span><br>      <span class="hljs-keyword">if</span> token_eos.item() <span class="hljs-keyword">in</span> toks:<br>        eos_idx = toks.index(token_eos.item())<br>        toks = toks[:eos_idx]<br><br>      output_tokens.append(toks)<br>      output_texts.append(decode(toks))<br>    <span class="hljs-keyword">return</span> output_tokens, output_texts<br><br><span class="hljs-comment"># Perform top-p (nucleus) sampling on a probability distribution.</span><br><span class="hljs-comment"># probs (torch.Tensor): Probability distribution tensor derived from the logits.</span><br><span class="hljs-comment"># p: Probability threshold for top-p sampling.</span><br><span class="hljs-comment"># According to the paper, Top-p sampling selects the smallest set of tokens whose cumulative probability mass exceeds the threshold p. </span><br><span class="hljs-comment"># The distribution is renormalized based on the selected tokens.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title.function">sample_top_p</span>(<span class="hljs-params">probs, p</span>):<br>    probs_sort, prob_idx = torch.sort(probs, dim=-<span class="hljs-number">1</span>, descending=<span class="hljs-literal">True</span>)<br>    probs_sum = torch.cumsum(probs_sort, dim=-<span class="hljs-number">1</span>)<br>    mask = probs_sum - probs_sort &gt; p<br>    probs_sort[mask] = <span class="hljs-number">0.0</span><br>    probs_sort.div_(probs_sort.<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>))<br>    next_token = torch.multinomial(probs_sort, num_samples=<span class="hljs-number">1</span>)<br>    next_token = torch.gather(prob_idx, -<span class="hljs-number">1</span>, next_token)    <br>    <span class="hljs-comment"># Sampled token indices from the vocabular is returned </span><br>    <span class="hljs-keyword">return</span> next_token</span></pre><p id="e384" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">Let’s perform inferencing on new Prompts and check the generated output</p><pre class="nz oa ob oc od ps pt pu bp pv bb bk"><span id="0e5c" class="pw ow hq pt b bg px py m pz qa" data-selectable-paragraph=""><span class="hljs-comment">## Perform the inferencing on user input prompts</span><br>prompts = <span class="hljs-string">"Consider you what services he has done"</span><br>output_tokens, output_texts = generate(model, prompts, ModelArgs)<br>output_texts = output_texts[<span class="hljs-number">0</span>].replace(<span class="hljs-string">"&lt;|begin_of_text|&gt;"</span>, <span class="hljs-string">""</span>)<br><span class="hljs-built_in">print</span>(output_texts)<br><br><span class="hljs-comment">## Output ##</span><br><span class="hljs-string">"""<br>Consider you what services he has done o eretrane<br>adetranytnn i eey i ade hs rcuh i eey,ad hsatsTns rpae,T<br>eon o i hseflns o i eee ee hs ote i ocal ersl,Bnnlnface<br>o i hmr a il nwye ademto nt i a ere<br>h i ees.<br>Frm oe o etrane o oregae,alh,t orede i oeral<br>"""</span></span></pre><p id="260a" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">And yes, we can see that our Llama 3 model is able to perform inference and generate texts on new prompts, though the output does not seem great given the amount of training data and epochs we’ve used for training. I am sure with much larger training data, we’ll achieve much better accuracy.</p><p id="0033" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">And this is it! </strong>we have successfully built our own Llama 3 model from scratch. We’ve also successfully trained the model and managed to perform inferencing to generate new texts within a very short amount of time using Google Colab Notebook with given free GPU and RAM. If you have followed along so far, I would personally congratulate you for the great effort you’ve put in.</p><p id="c75f" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">My final thoughts</strong></p><p id="e91c" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph="">Llama 3 and its other variances are the most popular open-source LLM currently available in the LLM space. I believe the ability to build Llama 3 from scratch provides all the necessary foundation to build a lot of new exciting LLM-based applications. I truly believe that knowledge should be free to all. Feel free to use the source code and update it to build your personal or professional projects. Good luck to you all.</p><p id="c41f" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">Thanks a lot for reading!</strong></p><p id="9d8d" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><a class="ag gq" href="https://github.com/tamangmilan/llama3/blob/main/build_llama3_from_scratch.ipynb" rel="noopener ugc nofollow" target="_blank">Link to Google Colab notebook</a></p><p id="dea4" class="pw-post-body-paragraph nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv gs bk" data-selectable-paragraph=""><strong class="nd hr">References</strong></p><ul class=""><li id="0d54" class="nb nc hq nd b ne nf ng nh ni nj nk nl fl nm nn no fo np nq nr fr ns nt nu nv pq pj pk bk" data-selectable-paragraph="">Meta Llama3 Github:<em class="qk"> </em><a class="ag gq" href="https://github.com/meta-llama/llama3" rel="noopener ugc nofollow" target="_blank">https://github.com/meta-llama/llama3</a></li></ul></div></div></div></div></section></div></div></article></div><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="ql qm ac jn"><div class="qn ac"><a class="qo aj an ap" href="https://medium.com/tag/artificial-intelligence?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="qp ee cx qq hd qr qs bf b bg ab bk eu">Artificial Intelligence</div></a></div><div class="qn ac"><a class="qo aj an ap" href="https://medium.com/tag/machine-learning?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="qp ee cx qq hd qr qs bf b bg ab bk eu">Machine Learning</div></a></div><div class="qn ac"><a class="qo aj an ap" href="https://medium.com/tag/large-language-models?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="qp ee cx qq hd qr qs bf b bg ab bk eu">Large Language Models</div></a></div><div class="qn ac"><a class="qo aj an ap" href="https://medium.com/tag/deep-learning?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="qp ee cx qq hd qr qs bf b bg ab bk eu">Deep Learning</div></a></div><div class="qn ac"><a class="qo aj an ap" href="https://medium.com/tag/data-science?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="qp ee cx qq hd qr qs bf b bg ab bk eu">Data Science</div></a></div></div></div></div><div class="m"></div><footer class="qt qu qv qw qx ac r qy qz c"><div class="m af"><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="ac cp ra"><div class="ac r kw"><div class="rb m"><span class="m rc rd re f e"><div class="ac r kw kx"><div class="pw-multi-vote-icon ee ky kz la lb"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="86" aria-labelledby="86" role="tooltip"><div tabindex="-1" class="be"><button class="lc ap le yp yq li an lj lk ll lb" data-testid="footerClapButton"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="pw-multi-vote-count m lm ln lo lp lq lr ls"><div><div class="bm" aria-hidden="false" aria-describedby="87" aria-labelledby="87" role="tooltip"><div tabindex="-1" class="be"><p class="bf b dy ab dx"><button class="ag ah ai ed ak al am an ao ap aq ar as at au yr tv">1K<span class="m i h g rf rg"></span></button></p></div></div></div></div></div></span><span class="m i h g rf rg"><div class="ac r kw kx"><div class="pw-multi-vote-icon ee ky kz la lb"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="88" aria-labelledby="88" role="tooltip"><div tabindex="-1" class="be"><button class="lc ap le yp yq li an lj lk ll lb" data-testid="footerClapButton"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="pw-multi-vote-count m lm ln lo lp lq lr ls"><div><div class="bm" aria-hidden="false" aria-describedby="89" aria-labelledby="89" role="tooltip"><div tabindex="-1" class="be"><p class="bf b dy ab dx"><button class="ag ah ai ed ak al am an ao ap aq ar as at au yr tv">1K</button></p></div></div></div></div></div></span></div><div class="ay ac"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="8" aria-labelledby="8"><div tabindex="-1" class="be"><button class="ap lc lv lw ac r ef lx ly" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="lu"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b bg ab dx"><span class="pw-responses-count lt lu">4</span></p></button></div></div></div></div></div><div class="ac r"><div class="rh m ri"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="9" aria-labelledby="9"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="footerBookmarkButton" class="ag ef ai ed ak al am ma ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="rh m ri"><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="10" aria-labelledby="10"><div tabindex="-1" class="be"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="footerSocialShareButton" class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="93" aria-labelledby="93" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" data-testid="footerStoryOptionsButton" class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div></footer><div class="rj m"><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="rk m"><div class="ac ja iy iw rl rm"><div class="rn ro rp rq rr rs rt ru rv rw ac cp"><div class="i l"><a href="https://pub.towardsai.net/?source=post_page---post_publication_info--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="ee"><img alt="Towards AI" class="cx gl m ry rx" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JyIThO-cLjlChQLb6kSlVQ(1).png" width="48" height="48" loading="lazy"><div class="gl m rx ry eo o em go"></div></div></a></div><div class="k j e"><a href="https://pub.towardsai.net/?source=post_page---post_publication_info--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="ee"><img alt="Towards AI" class="cx gl m sa rz" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JyIThO-cLjlChQLb6kSlVQ(2).png" width="64" height="64" loading="lazy"><div class="gl m rz sa eo o em go"></div></div></a></div><div class="k j e sb ri"><div class="ac"><button style="border: 1px solid rgb(36, 36, 36);" class="tt uv ap ac cb r uw kv ux"><span class="bf b bg ab bk bh"><span class="bm uy">Follow</span></span></button></div></div></div><div class="ac co ca"><div class="sc sd se sf sg m"><a class="ag ah ai ak al am an ao ap aq ar as at au ac r" href="https://pub.towardsai.net/?source=post_page---post_publication_info--2ce1ecaa901c---------------------------------------" rel="noopener follow"><h2 class="pw-author-name bf fe si sj sk sl sm sn fl fm fn fo fp fq fr fs ft bk"><span class="gs sh">Published in <!-- -->Towards AI</span></h2></a><div class="qn ac jc"><div class="m ri"><span class="pw-follower-count bf b bg ab dx"><a class="ag ah ai ed ak al am an ao ap aq ar as jl" rel="noopener follow" href="https://pub.towardsai.net/followers?source=post_page---post_publication_info--2ce1ecaa901c---------------------------------------" data-discover="true">85K followers</a></span></div><div class="bf b bg ab dx ac so"><span class="fu m" aria-hidden="true"><span class="bf b bg ab dx">·</span></span><a class="ag ah ai ed ak al am an ao ap aq ar as jl" rel="noopener follow" href="https://pub.towardsai.net/new-study-ai-models-fail-at-basic-human-psychology-despite-50-million-investment-e6187805ffd5?source=post_page---post_publication_info--2ce1ecaa901c---------------------------------------" data-discover="true">Last published&nbsp;<!-- -->1 day ago</a></div></div><div class="gp m"><p class="bf b bg ab bk">The leading AI community and content platform focused on making AI accessible to all. Check out our new course platform: <a class="ag ah ai ed ak al am an ao ap aq ar as gq gr gs" href="https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev" rel="noopener  ugc nofollow">https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev</a></p></div></div></div><div class="i l"><div class="ac"><button style="border: 1px solid rgb(36, 36, 36);" class="tt uv ap ac cb r uw kv ux"><span class="bf b bg ab bk bh"><span class="bm uy">Follow</span></span></button></div></div></div></div><div class="ac ja iy iw rl rm"><div class="rn ro rp rq rr rs rt ru rv rw ac cp"><div class="i l"><a tabindex="0" href="https://medium.com/@tamangmilan?source=post_page---post_author_info--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="m ee"><img alt="Milan Tamang" class="m eq bx rx ry cx" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_mfDWMsklE8l0-1rrnhbJ4g(1).jpg" width="48" height="48" loading="lazy"><div class="em bx m rx ry eo o aj go"></div></div></a></div><div class="k j e"><a tabindex="0" href="https://medium.com/@tamangmilan?source=post_page---post_author_info--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="m ee"><img alt="Milan Tamang" class="m eq bx rz sa cx" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_mfDWMsklE8l0-1rrnhbJ4g(2).jpg" width="64" height="64" loading="lazy"><div class="em bx m rz sa eo o aj go"></div></div></a></div><div class="k j e sb ri"><div class="ac"><div class="bm" aria-hidden="false"><button class="tt uv ap ac cb r uw kv ux" style="border: 1px solid rgb(36, 36, 36);"><span class="bf b bg ab bk bh"><span class="bm uy">Follow</span></span></button></div></div></div></div><div class="ac co ca"><div class="sc sd se sf sg m"><a class="ag ah ai ak al am an ao ap aq ar as at au ac r" href="https://medium.com/@tamangmilan?source=post_page---post_author_info--2ce1ecaa901c---------------------------------------" rel="noopener follow"><h2 class="pw-author-name bf fe si sj sk sl sm sn fl fm fn fo fp fq fr fs ft bk"><span class="gs sh">Written by <!-- -->Milan Tamang</span></h2></a><div class="qn ac jc"><div class="m ri"><span class="pw-follower-count bf b bg ab dx"><a class="ag ah ai ed ak al am an ao ap aq ar as jl" href="https://medium.com/@tamangmilan/followers?source=post_page---post_author_info--2ce1ecaa901c---------------------------------------" rel="noopener follow">2.5K followers</a></span></div><div class="bf b bg ab dx ac so"><span class="fu m" aria-hidden="true"><span class="bf b bg ab dx">·</span></span><a class="ag ah ai ed ak al am an ao ap aq ar as jl" href="https://medium.com/@tamangmilan/following?source=post_page---post_author_info--2ce1ecaa901c---------------------------------------" rel="noopener follow">72 following</a></div></div><div class="gp m"><p class="bf b bg ab bk">AI Architect <a class="ag ah ai ed ak al am an ao ap aq ar as gq gr gs" href="https://www.linkedin.com/in/tamangmilan" rel="noopener  ugc nofollow">https://www.linkedin.com/in/tamangmilan</a></p></div></div></div><div class="i l"><div class="ac"><div class="bm" aria-hidden="false"><button class="tt uv ap ac cb r uw kv ux" style="border: 1px solid rgb(36, 36, 36);"><span class="bf b bg ab bk bh"><span class="bm uy">Follow</span></span></button></div></div></div></div></div></div></div><div class="sp sq sr ss st m"><div class="su bh s rj"></div><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="ac r cp"><h2 class="bf fe sv sw fh sx sy fk sz ta tb tc td te tf tg th bk">Responses (<!-- -->4<!-- -->)</h2><div class="ac ti"><div><div class="bm" aria-hidden="false" role="tooltip" aria-describedby="12" aria-labelledby="12"><div tabindex="-1" class="be"><a class="tj tk" href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--2ce1ecaa901c---------------------------------------" rel="noopener follow" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" aria-label="Shield with a checkmark" viewBox="0 0 25 25"><path fill-rule="evenodd" d="M11.987 5.036a.754.754 0 0 1 .914-.01c.972.721 1.767 1.218 2.6 1.543.828.322 1.719.485 2.887.505a.755.755 0 0 1 .741.757c-.018 3.623-.43 6.256-1.449 8.21-1.034 1.984-2.662 3.209-4.966 4.083a.75.75 0 0 1-.537-.003c-2.243-.874-3.858-2.095-4.897-4.074-1.024-1.951-1.457-4.583-1.476-8.216a.755.755 0 0 1 .741-.757c1.195-.02 2.1-.182 2.923-.503.827-.322 1.6-.815 2.519-1.535m.468.903c-.897.69-1.717 1.21-2.623 1.564-.898.35-1.856.527-3.026.565.037 3.45.469 5.817 1.36 7.515.884 1.684 2.25 2.762 4.284 3.571 2.092-.81 3.465-1.89 4.344-3.575.886-1.698 1.299-4.065 1.334-7.512-1.149-.039-2.091-.217-2.99-.567-.906-.353-1.745-.873-2.683-1.561m-.009 9.155a2.672 2.672 0 1 0 0-5.344 2.672 2.672 0 0 0 0 5.344m0 1a3.672 3.672 0 1 0 0-7.344 3.672 3.672 0 0 0 0 7.344m-1.813-3.777.525-.526.916.917 1.623-1.625.526.526-2.149 2.152z" clip-rule="evenodd"></path></svg></a></div></div></div></div></div><div class="tl ev tm tn to tp tq m"><div><div class="bf b bg ab bk"><div class="fx"><div class="xt m"><div class="xu ac r"><div class="m ee"><div class="m ee"><img alt="skr3178" class="m eq bx by bz cx" width="32" height="32" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_dmbNkD5D-u45r44go_cf0g(1).png"><div class="em bx m by bz eo o aj go"></div></div></div><div class="bn ac cn co cb"><div class="ac jc jn"><span class="bf b bg ab bk gs sh">skr3178</span></div></div></div><div class="cx bp ac co xi xj"><div class="ac co ee"><div class="xk xl xm xn"><div role="textbox" aria-multiline="true" data-slate-editor="true" data-slate-node="value" contenteditable="true" zindex="-1" style="position: relative; white-space: pre-wrap; overflow-wrap: break-word; outline: none; min-height: 24px;"><div data-slate-node="element"><p><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-placeholder="true" contenteditable="false" style="position: absolute; top: 0px; pointer-events: none; width: 100%; max-width: 100%; display: block; opacity: 0.333; user-select: none; text-decoration: none;">What are your thoughts?</span><span data-slate-zero-width="n" data-slate-length="0">﻿<br></span></span></span></p></div></div></div><div class="cp xo dx ac xp gi xq"><span class="bf b bg ab dx"><div class="ac"><div><div class="bm" aria-hidden="false" aria-describedby="25" aria-labelledby="25" role="tooltip"><div tabindex="-1" class="be"><span role="button" aria-label="Bold (⌘B)" class="xv xw xx xy bp me cb xz ya yb" tabindex="0"><svg width="21" height="21"><path fill-rule="evenodd" d="M10.308 17.993h-5.92l.11-.894.783-.12c.56-.11.79-.224.79-.448V5.37c0-.225-.113-.336-.902-.448H4.5l-.114-.894h6.255c4.02 0 5.58 1.23 5.58 3.13 0 1.896-1.78 3.125-3.79 3.463v.11c2.69.34 4.25 1.56 4.25 3.57 0 2.35-2.01 3.69-6.37 3.69l.02.01h-.02zm-.335-12.96H8.967V10.5h1.23c1.788 0 2.79-1.23 2.79-2.683 0-1.685-1.004-2.803-3.006-2.803v.02zm-.223 6.36h-.783v5.588l1.225.23h.22c1.67 0 3.01-1.004 3.01-2.792 0-2.122-1.566-3.016-3.69-3.016h.018z"></path></svg></span></div></div></div><div><div class="bm" aria-hidden="false" aria-describedby="26" aria-labelledby="26" role="tooltip"><div tabindex="-1" class="be"><span role="button" aria-label="Italic (⌘I)" class="xv xw xx xy bp me cb xz ya yb" tabindex="0"><svg width="21" height="21"><path fill-rule="evenodd" d="M9.847 18.04c-.533 0-2.027-.64-1.92-.853l2.027-7.68-.64-.214-1.387 1.494-.427-.427c.534-1.173 1.707-2.667 2.774-2.667.533 0 2.24.534 2.133.854l-2.133 7.786.533.214 1.6-1.067.427.427c-.64 1.066-1.92 2.133-2.987 2.133m2.347-11.733c-.96 0-1.387-.64-1.387-1.387 0-1.067.747-1.92 1.493-1.92.854 0 1.387.64 1.387 1.493-.107 1.067-.747 1.814-1.493 1.814"></path></svg></span></div></div></div></div></span><div class="xr sb ac xp gi xq"><div class="xs"><button class="bf b dy ab bk yc ts tt tu tv lx tw tx ty ga tz ua ub uc ui uj eq bm uk oq" data-testid="CancelResponseButton">Cancel</button></div><button class="bf b dy ab yd yc ye yf yg yh tw tx ty yi yj yk uc ui uj eq bm uk oq" disabled="" data-testid="ResponseRespondButton">Respond</button></div></div></div></div></div></div></div></div></div><div class="ev m"><div class="bh dz"><div class="yy yz m"><div class="ac cp"><div class="ac r"><div><div class="bm" aria-hidden="false" aria-describedby="101" aria-labelledby="101" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://carlosaguayo81.medium.com/?source=post_page---post_responses--2ce1ecaa901c----0-----------------------------------" rel="noopener follow"><div class="m ee"><div class="m ee"><img alt="Carlos Aguayo" class="m eq bx by bz cx" width="32" height="32" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_MO3akW3IrFmuS8Day6at3g.jpg"><div class="em bx m by bz eo o aj ep"></div></div></div></a></div></div></div><div class="za m"><div class="ac r"><div><div class="bm" aria-hidden="false" aria-describedby="102" aria-labelledby="102" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as at au ac r zb zc" href="https://carlosaguayo81.medium.com/?source=post_page---post_responses--2ce1ecaa901c----0-----------------------------------" rel="noopener follow"><div class="ac jc zb"><p class="bf b bg ab es zd et ze zf zg zh zi bk">Carlos Aguayo</p></div></a></div></div></div></div><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://carlosaguayo81.medium.com/great-tutorial-thank-you-763c6ab05525?source=post_page---post_responses--2ce1ecaa901c----0-----------------------------------" rel="noopener follow"><p class="bf b dy ab dx"><span>Oct 17, 2024</span></p></a></div></div><div class="bm" aria-hidden="false"><button class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div><div class="zj m gs"><pre class="so"><div class="zk m"><div class="bf b bg ab bk"><div class="fx">Great tutorial, thank you!</div></div></div><div class="zk m"><div class="bf b bg ab bk"><div class="fx">You have a tiny, but relevant bug, see here:</div></div></div><div class="zk m"><div class="bf b bg ab bk"><div class="fx"><a class="ag gq" href="https://github.com/tamangmilan/llama3/pull/1" rel="noopener ugc nofollow" target="_blank">https://github.com/tamangmilan/llama3/pull/1</a></div></div></div></pre></div><div class="zl ac r kw cp"><div class="ac r zm"><div class="ac r kw kx"><div class="pw-multi-vote-icon ee ky kz la lb"><div class=""><button class="lc ap le yp yq li an lj lk ll lb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></button></div></div><div class="pw-multi-vote-count m lm ln lo lp lq lr ls"><p class="bf b dy ab dx">2</p></div></div><button class="ag ef ai ed ak al am an ao ap aq ar as at au ac r zb tv lx ly"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="responses"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b dy ab dx">1 reply</p></button><p class="bf b dy ab bk"><button class="ag ah ai ed ak al am an ao ap aq ar as gq">Reply</button></p></div></div></div></div></div><div class="ev m"><div class="bh dz"><div class="yy yz m"><div class="ac cp"><div class="ac r"><div><div class="bm" aria-hidden="false" aria-describedby="103" aria-labelledby="103" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@anatole.martins6730?source=post_page---post_responses--2ce1ecaa901c----1-----------------------------------" rel="noopener follow"><div class="m ee"><div class="m ee"><img alt="Anatole Martins" class="m eq bx by bz cx" width="32" height="32" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_1zCB32lXcQHi4i0VncopWg.png"><div class="em bx m by bz eo o aj ep"></div></div></div></a></div></div></div><div class="za m"><div class="ac r"><div><div class="bm" aria-hidden="false" aria-describedby="104" aria-labelledby="104" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as at au ac r zb zc" href="https://medium.com/@anatole.martins6730?source=post_page---post_responses--2ce1ecaa901c----1-----------------------------------" rel="noopener follow"><div class="ac jc zb"><p class="bf b bg ab es zd et ze zf zg zh zi bk">Anatole Martins</p></div></a></div></div></div></div><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@anatole.martins6730/this-article-is-an-impressive-guide-to-building-the-llama-3-model-5df36b56cb43?source=post_page---post_responses--2ce1ecaa901c----1-----------------------------------" rel="noopener follow"><p class="bf b dy ab dx"><span>Oct 1, 2024</span></p></a></div></div><div class="bm" aria-hidden="false"><button class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div><div class="zj m gs"><pre class="so"><div class="zk m"><div class="bf b bg ab bk"><div class="fx">This article is an impressive guide to building the Llama 3 model! It's rich in detail and sure to inspire AI enthusiasts. Great work!</div></div></div></pre></div><div class="zl ac r kw cp"><div class="ac r zm"><div class="ac r kw kx"><div class="pw-multi-vote-icon ee ky kz la lb"><div class=""><button class="lc ap le yp yq li an lj lk ll lb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></button></div></div><div class="pw-multi-vote-count m lm ln lo lp lq lr ls"><p class="bf b dy ab dx">2</p></div></div><button class="ag ef ai ed ak al am an ao ap aq ar as at au ac r zb tv lx ly"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="responses"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"></path></svg><p class="bf b dy ab dx">1 reply</p></button><p class="bf b dy ab bk"><button class="ag ah ai ed ak al am an ao ap aq ar as gq">Reply</button></p></div></div></div></div></div><div class="ev m"><div class="bh dz"><div class="yy yz m"><div class="ac cp"><div class="ac r"><div><div class="bm" aria-hidden="false" aria-describedby="105" aria-labelledby="105" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@boyu.liu_11648?source=post_page---post_responses--2ce1ecaa901c----2-----------------------------------" rel="noopener follow"><div class="m ee"><div class="m ee"><img alt="Boyu Liu" class="m eq bx by bz cx" width="32" height="32" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_dmbNkD5D-u45r44go_cf0g(1).png"><div class="em bx m by bz eo o aj ep"></div></div></div></a></div></div></div><div class="za m"><div class="ac r"><div><div class="bm" aria-hidden="false" aria-describedby="106" aria-labelledby="106" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as at au ac r zb zc" href="https://medium.com/@boyu.liu_11648?source=post_page---post_responses--2ce1ecaa901c----2-----------------------------------" rel="noopener follow"><div class="ac jc zb"><p class="bf b bg ab es zd et ze zf zg zh zi bk">Boyu Liu</p></div></a></div></div></div></div><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@boyu.liu_11648/impressive-tutorial-b33f805b6867?source=post_page---post_responses--2ce1ecaa901c----2-----------------------------------" rel="noopener follow"><p class="bf b dy ab dx"><span>Feb 14</span></p></a></div></div><div class="bm" aria-hidden="false"><button class="ag ef ai ed ak al am ma ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div><div class="zj m gs"><pre class="so"><div class="zk m"><div class="bf b bg ab bk"><div class="fx">Impressive tutorial! But I have a question regarding the training dataset. Since there is bos_token, eos_token and pad_token, how's the model suppose to learn the embedding for these three special tokens?</div></div></div></pre></div><div class="zl ac r kw cp"><div class="ac r zm"><div class="ac r kw kx"><div class="pw-multi-vote-icon ee afn kz la lb"><div class=""><button class="lc ap le yp yq li an lj lk ll lb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM13.916 3.953l1.523-2.112-1.184-.39zM8.589 1.84l1.522 2.112-.337-2.501zM18.523 18.92c-.86.86-1.75 1.246-2.62 1.33a6 6 0 0 0 .407-.372c2.388-2.389 2.86-4.951 1.399-7.623l-.912-1.603-.79-1.672c-.26-.56-.194-.98.203-1.288a.7.7 0 0 1 .546-.132c.283.046.546.231.728.5l2.363 4.157c.976 1.624 1.141 4.237-1.324 6.702m-10.999-.438L3.37 14.328a.828.828 0 0 1 .585-1.408.83.83 0 0 1 .585.242l2.158 2.157a.365.365 0 0 0 .516-.516l-2.157-2.158-1.449-1.449a.826.826 0 0 1 1.167-1.17l3.438 3.44a.363.363 0 0 0 .516 0 .364.364 0 0 0 0-.516L5.293 9.513l-.97-.97a.826.826 0 0 1 0-1.166.84.84 0 0 1 1.167 0l.97.968 3.437 3.436a.36.36 0 0 0 .517 0 .366.366 0 0 0 0-.516L6.977 7.83a.82.82 0 0 1-.241-.584.82.82 0 0 1 .824-.826c.219 0 .43.087.584.242l5.787 5.787a.366.366 0 0 0 .587-.415l-1.117-2.363c-.26-.56-.194-.98.204-1.289a.7.7 0 0 1 .546-.132c.283.046.545.232.727.501l2.193 3.86c1.302 2.38.883 4.59-1.277 6.75-1.156 1.156-2.602 1.627-4.19 1.367-1.418-.236-2.866-1.033-4.079-2.246M10.75 5.971l2.12 2.12c-.41.502-.465 1.17-.128 1.89l.22.465-3.523-3.523a.8.8 0 0 1-.097-.368c0-.22.086-.428.241-.584a.847.847 0 0 1 1.167 0m7.355 1.705c-.31-.461-.746-.758-1.23-.837a1.44 1.44 0 0 0-1.11.275c-.312.24-.505.543-.59.881a1.74 1.74 0 0 0-.906-.465 1.47 1.47 0 0 0-.82.106l-2.182-2.182a1.56 1.56 0 0 0-2.2 0 1.54 1.54 0 0 0-.396.701 1.56 1.56 0 0 0-2.21-.01 1.55 1.55 0 0 0-.416.753c-.624-.624-1.649-.624-2.237-.037a1.557 1.557 0 0 0 0 2.2c-.239.1-.501.238-.715.453a1.56 1.56 0 0 0 0 2.2l.516.515a1.556 1.556 0 0 0-.753 2.615L7.01 19c1.32 1.319 2.909 2.189 4.475 2.449q.482.08.971.08c.85 0 1.653-.198 2.393-.579.231.033.46.054.686.054 1.266 0 2.457-.52 3.505-1.567 2.763-2.763 2.552-5.734 1.439-7.586z" clip-rule="evenodd"></path></svg></button></div></div></div><p class="bf b dy ab bk"><button class="ag ah ai ed ak al am an ao ap aq ar as gq">Reply</button></p></div></div></div></div></div><div class="tr m"><button class="bf b bg ab bk qp ts tt tu tv lx tw tx ty ga tz ua ub uc ud ue uf ug uh ui uj eq bm uk oq">See all responses</button></div></div></div></div><div class="ul um un uo up m bw"><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="zn zo dm dn do gp m"><h2 class="bf fe sv sw fh sx sy fk sz ta tb tc td te tf tg th bk">More from Milan Tamang and Towards AI</h2></div><div class="zp ac kw jn zq zr zs zt zu zv zw zx zy zz aba abb abc abd abe"><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://pub.towardsai.net/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858" tabindex="0"><div class="acq"><div aria-label="Build your own Large Language Model (LLM) From Scratch Using PyTorch"><div class="acs act acu acv ys"><img alt="Build your own Large Language Model (LLM) From Scratch Using PyTorch" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_vdfpI0sZvg_JqG_scSK-nA.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div><div class="m" aria-hidden="false" aria-describedby="133" aria-labelledby="133" role="tooltip"><div tabindex="-1" class="be"><a href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c----0---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><div class="ee"><img alt="Towards AI" class="cx gl m adj adi" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JyIThO-cLjlChQLb6kSlVQ(3).png"><div class="gl m adi adj eo o em ep"></div></div></a></div></div></div></div><div class="sh m eu"><p class="bf b dy ab dx">In</p></div><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="134" aria-labelledby="134" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c----0---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Towards AI</p></a></div></div></div></div><div class="adk m"><p class="bf b dy ab dx">by</p></div><div><div class="m" aria-hidden="false" aria-describedby="135" aria-labelledby="135" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@tamangmilan?source=post_page---author_recirc--2ce1ecaa901c----0---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Milan Tamang</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" rel="noopener follow" href="https://pub.towardsai.net/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858?source=post_page---author_recirc--2ce1ecaa901c----0---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" data-discover="true"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">Build your own Large Language Model (LLM) From Scratch Using PyTorch</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">A Step-by-Step guide to build and train an LLM named MalayGPT. This model’s task is to translate texts from English to Malay language.</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><span>Jun 5, 2024</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" rel="noopener follow" href="https://pub.towardsai.net/build-your-own-large-language-model-llm-from-scratch-using-pytorch-9e9945c24858?source=post_page---author_recirc--2ce1ecaa901c----0---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" data-discover="true"><div><div class="ac" aria-hidden="false" aria-describedby="283" aria-labelledby="283" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>1.95K</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="136" aria-labelledby="136" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>17</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="137" aria-labelledby="137" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="285" aria-labelledby="285" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://pub.towardsai.net/the-death-of-vector-databases-how-agentic-rag-is-revolutionizing-information-retrieval-79f0d1f2f118" tabindex="0"><div class="acq"><div aria-label="🚀 The Death of Vector Databases? How Agentic RAG is Revolutionizing Information Retrieval"><div class="acs act acu acv ys"><img alt="🚀 The Death of Vector Databases? How Agentic RAG is Revolutionizing Information Retrieval" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_oAcFSDJi_m9Z-6WUSLBFKQ.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div><div class="m" aria-hidden="false" aria-describedby="139" aria-labelledby="139" role="tooltip"><div tabindex="-1" class="be"><a href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c----1---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><div class="ee"><img alt="Towards AI" class="cx gl m adj adi" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JyIThO-cLjlChQLb6kSlVQ(3).png"><div class="gl m adi adj eo o em ep"></div></div></a></div></div></div></div><div class="sh m eu"><p class="bf b dy ab dx">In</p></div><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="140" aria-labelledby="140" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c----1---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Towards AI</p></a></div></div></div></div><div class="adk m"><p class="bf b dy ab dx">by</p></div><div><div class="m" aria-hidden="false" aria-describedby="141" aria-labelledby="141" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@mahendramedapati?source=post_page---author_recirc--2ce1ecaa901c----1---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">MahendraMedapati</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" rel="noopener follow" href="https://pub.towardsai.net/the-death-of-vector-databases-how-agentic-rag-is-revolutionizing-information-retrieval-79f0d1f2f118?source=post_page---author_recirc--2ce1ecaa901c----1---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" data-discover="true"><div title="🚀 The Death of Vector Databases? How Agentic RAG is Revolutionizing Information Retrieval"><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">🚀 The Death of Vector Databases? How Agentic RAG is Revolutionizing Information Retrieval</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">Imagine querying a 900-page legal document and getting precise answers in minutes — without embeddings, without vector stores, without the…</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="142" aria-labelledby="142"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="143" aria-labelledby="143" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div><span>Aug 6</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" rel="noopener follow" href="https://pub.towardsai.net/the-death-of-vector-databases-how-agentic-rag-is-revolutionizing-information-retrieval-79f0d1f2f118?source=post_page---author_recirc--2ce1ecaa901c----1---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" data-discover="true"><div><div class="ac" aria-hidden="false" aria-describedby="286" aria-labelledby="286" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>684</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="144" aria-labelledby="144" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>23</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="145" aria-labelledby="145" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="288" aria-labelledby="288" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://pub.towardsai.net/the-ultimate-guide-to-agentic-ai-frameworks-in-2025-which-one-should-you-choose-to-build-the-a1f861f403d8" tabindex="0"><div class="acq"><div aria-label="🤖 The Ultimate Guide to Agentic AI Frameworks in 2025: Which One Should You Choose to Build the…"><div class="acs act acu acv ys"><img alt="🤖 The Ultimate Guide to Agentic AI Frameworks in 2025: Which One Should You Choose to Build the…" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_Svk1EncbnW9k7NYHhiPLPQ.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div><div class="m" aria-hidden="false" aria-describedby="147" aria-labelledby="147" role="tooltip"><div tabindex="-1" class="be"><a href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c----2---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><div class="ee"><img alt="Towards AI" class="cx gl m adj adi" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JyIThO-cLjlChQLb6kSlVQ(3).png"><div class="gl m adi adj eo o em ep"></div></div></a></div></div></div></div><div class="sh m eu"><p class="bf b dy ab dx">In</p></div><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="148" aria-labelledby="148" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c----2---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Towards AI</p></a></div></div></div></div><div class="adk m"><p class="bf b dy ab dx">by</p></div><div><div class="m" aria-hidden="false" aria-describedby="149" aria-labelledby="149" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@mahendramedapati?source=post_page---author_recirc--2ce1ecaa901c----2---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">MahendraMedapati</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" rel="noopener follow" href="https://pub.towardsai.net/the-ultimate-guide-to-agentic-ai-frameworks-in-2025-which-one-should-you-choose-to-build-the-a1f861f403d8?source=post_page---author_recirc--2ce1ecaa901c----2---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" data-discover="true"><div title="🤖 The Ultimate Guide to Agentic AI Frameworks in 2025: Which One Should You Choose to Build the…"><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">🤖 The Ultimate Guide to Agentic AI Frameworks in 2025: Which One Should You Choose to Build the…</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">From zero to AI agent hero — the complete roadmap that 10,000+ developers are using to build production-ready AI systems</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="150" aria-labelledby="150"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="151" aria-labelledby="151" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div><span>Jul 25</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" rel="noopener follow" href="https://pub.towardsai.net/the-ultimate-guide-to-agentic-ai-frameworks-in-2025-which-one-should-you-choose-to-build-the-a1f861f403d8?source=post_page---author_recirc--2ce1ecaa901c----2---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" data-discover="true"><div><div class="ac" aria-hidden="false" aria-describedby="289" aria-labelledby="289" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>352</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="152" aria-labelledby="152" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>11</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="153" aria-labelledby="153" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="291" aria-labelledby="291" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://medium.com/data-science/build-a-tokenizer-for-the-thai-language-from-scratch-0e4ea5f2a8b3" tabindex="0"><div class="acq"><div aria-label="Build a Tokenizer for the Thai Language from Scratch"><div class="acs act acu acv ys"><img alt="Build a Tokenizer for the Thai Language from Scratch" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_C5v8YO8zSEDZ5fpDVZPEHQ.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div><div class="m" aria-hidden="false" aria-describedby="155" aria-labelledby="155" role="tooltip"><div tabindex="-1" class="be"><a href="https://medium.com/data-science?source=post_page---author_recirc--2ce1ecaa901c----3---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><div class="ee"><img alt="TDS Archive" class="cx gl m adj adi" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_JEuS4KBdakUcjg9sC7Wo4A.png"><div class="gl m adi adj eo o em ep"></div></div></a></div></div></div></div><div class="sh m eu"><p class="bf b dy ab dx">In</p></div><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="156" aria-labelledby="156" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/data-science?source=post_page---author_recirc--2ce1ecaa901c----3---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">TDS Archive</p></a></div></div></div></div><div class="adk m"><p class="bf b dy ab dx">by</p></div><div><div class="m" aria-hidden="false" aria-describedby="157" aria-labelledby="157" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@tamangmilan?source=post_page---author_recirc--2ce1ecaa901c----3---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Milan Tamang</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/data-science/build-a-tokenizer-for-the-thai-language-from-scratch-0e4ea5f2a8b3?source=post_page---author_recirc--2ce1ecaa901c----3---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">Build a Tokenizer for the Thai Language from Scratch</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">A step-by-step guide to building a Thai multilingual sub-word tokenizer based on a BPE algorithm trained on Thai and English datasets</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><span>Sep 14, 2024</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://medium.com/data-science/build-a-tokenizer-for-the-thai-language-from-scratch-0e4ea5f2a8b3?source=post_page---author_recirc--2ce1ecaa901c----3---------------------9d18e82c_6d4c_429c_80be_d90219d5d65d--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="292" aria-labelledby="292" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>42</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="158" aria-labelledby="158" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>2</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="159" aria-labelledby="159" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="294" aria-labelledby="294" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span></div></div></div></div></div></div></article></div></div></div><div class="su bh uq dk dl aeu aev aew"></div><div class="ac rm rl iw iy ja"><a class="bf b bg ab bk qp ts tt tu tv lx tw tx ty ga tz ua ub uc ud ue uf ug uh ui uj eq bm uk oq" href="https://medium.com/@tamangmilan?source=post_page---author_recirc--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="m oq">See all from Milan Tamang</div></a><div class="aex aey aez afa afb afc afd afe aff ls m"><a class="bf b bg ab bk qp ts tt tu tv lx tw tx ty ga tz ua ub uc ud ue uf ug uh ui uj eq bm uk oq" href="https://pub.towardsai.net/?source=post_page---author_recirc--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="m oq">See all from Towards AI</div></a></div></div></div></div><div class="su bh uq afg afh afi afj afk"></div><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="afl afm m"><h2 class="bf fe sv sw fh sx sy fk sz ta tb tc td te tf tg th bk">Recommended from Medium</h2><div class="nz oa ob oc od m"><div class="zp ac kw jn zq zr zs zt zu zv zw zx zy zz aba abb abc abd abe"><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://levelup.gitconnected.com/building-the-entire-rag-ecosystem-and-optimizing-every-component-8f23349b96a4" tabindex="0"><div class="acq"><div aria-label="Building the Entire RAG Ecosystem and Optimizing Every Component"><div class="acs act acu acv ys"><img alt="Building the Entire RAG Ecosystem and Optimizing Every Component" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_ZjozYulECfqrzgMaTEZ-Rg.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div><div class="m" aria-hidden="false" aria-describedby="161" aria-labelledby="161" role="tooltip"><div tabindex="-1" class="be"><a href="https://levelup.gitconnected.com/?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div class="ee"><img alt="Level Up Coding" class="cx gl m adj adi" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_5D9oYBd58pyjMkV_5-zXXQ.jpg"><div class="gl m adi adj eo o em ep"></div></div></a></div></div></div></div><div class="sh m eu"><p class="bf b dy ab dx">In</p></div><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="162" aria-labelledby="162" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://levelup.gitconnected.com/?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Level Up Coding</p></a></div></div></div></div><div class="adk m"><p class="bf b dy ab dx">by</p></div><div><div class="m" aria-hidden="false" aria-describedby="163" aria-labelledby="163" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@fareedkhandev?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Fareed Khan</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://levelup.gitconnected.com/building-the-entire-rag-ecosystem-and-optimizing-every-component-8f23349b96a4?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">Building the Entire RAG Ecosystem and Optimizing Every Component</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">Routing, Indexing, Retrieval, Transformation and more.</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="164" aria-labelledby="164"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="165" aria-labelledby="165" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div><span>Aug 11</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://levelup.gitconnected.com/building-the-entire-rag-ecosystem-and-optimizing-every-component-8f23349b96a4?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="295" aria-labelledby="295" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>1.1K</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="166" aria-labelledby="166" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>11</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="167" aria-labelledby="167" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="297" aria-labelledby="297" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://medium.com/@mlshark/killing-lmms-with-a-tiny-27m-model-hierarchical-reasoning-model-explained-hrm-briefly-explained-bf1121a97e77" tabindex="0"><div class="acq"><div aria-label="Killing LMMs with a tiny 27M model: Hierarchical Reasoning Model Explained (HRM) Briefly Explained"><div class="acs act acu acv ys"><img alt="Killing LMMs with a tiny 27M model: Hierarchical Reasoning Model Explained (HRM) Briefly Explained" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_LaksqH0opQHOVMRTYjzQDQ.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="169" aria-labelledby="169" role="tooltip"><div tabindex="-1" class="be"><a tabindex="-1" href="https://medium.com/@mlshark?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div class="m ee"><img alt="Allen Liang" class="m eq bx adi adj cx" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_OCiUtb5h4mkLrKKUa1FcFg.jpg"><div class="em bx m adi adj eo o aj ep"></div></div></a></div></div></div></div></div><div><div class="m" aria-hidden="false" aria-describedby="170" aria-labelledby="170" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@mlshark?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Allen Liang</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@mlshark/killing-lmms-with-a-tiny-27m-model-hierarchical-reasoning-model-explained-hrm-briefly-explained-bf1121a97e77?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div title="Killing LMMs with a tiny 27M model: Hierarchical Reasoning Model Explained (HRM) Briefly Explained"><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">Killing LMMs with a tiny 27M model: Hierarchical Reasoning Model Explained (HRM) Briefly Explained</h2></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="171" aria-labelledby="171"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="172" aria-labelledby="172" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div><span>Aug 3</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://medium.com/@mlshark/killing-lmms-with-a-tiny-27m-model-hierarchical-reasoning-model-explained-hrm-briefly-explained-bf1121a97e77?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="298" aria-labelledby="298" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>164</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="173" aria-labelledby="173" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>2</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="174" aria-labelledby="174" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="300" aria-labelledby="300" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span></div></div></div></div></div></div></article></div></div></div></div><div class="zp ac kw jn zq zr zs zt zu zv zw zx zy zz aba abb abc abd abe"><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://medium.com/@shravankoninti/build-a-small-language-model-slm-from-scratch-3ddd13fa6470" tabindex="0"><div class="acq"><div aria-label="Build a Small Language Model (SLM) From Scratch"><div class="acs act acu acv ys"><img alt="Build a Small Language Model (SLM) From Scratch" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_O7cNUDV_wSN7y8XjHtG2hQ.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="176" aria-labelledby="176" role="tooltip"><div tabindex="-1" class="be"><a tabindex="-1" href="https://medium.com/@shravankoninti?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div class="m ee"><img alt="Shravan Kumar" class="m eq bx adi adj cx" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_bUTiYqYPpDticWBePxbPcA.jpg"><div class="em bx m adi adj eo o aj ep"></div></div></a></div></div></div></div></div><div><div class="m" aria-hidden="false" aria-describedby="177" aria-labelledby="177" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@shravankoninti?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Shravan Kumar</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@shravankoninti/build-a-small-language-model-slm-from-scratch-3ddd13fa6470?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">Build a Small Language Model (SLM) From Scratch</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">At this current phase of AI evolution, any model with fewer than 1 billion parameters can be called a small language model. If we look at…</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><span>Jul 25</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://medium.com/@shravankoninti/build-a-small-language-model-slm-from-scratch-3ddd13fa6470?source=post_page---read_next_recirc--2ce1ecaa901c----0---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="301" aria-labelledby="301" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>553</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="178" aria-labelledby="178" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>18</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="179" aria-labelledby="179" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="303" aria-labelledby="303" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://medium.com/@tam.tamanna18/fireplexity-vs-perplexity-ai-in-open-source-answer-engines-c456f394d1fe" tabindex="0"><div class="acq"><div aria-label="Fireplexity vs Perplexity AI in Open Source Answer Engines"><div class="acs act acu acv ys"><img alt="Fireplexity vs Perplexity AI in Open Source Answer Engines" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/0_p_Q31RadnPjtL8Jk.jpg"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="181" aria-labelledby="181" role="tooltip"><div tabindex="-1" class="be"><a tabindex="-1" href="https://medium.com/@tam.tamanna18?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div class="m ee"><img alt="Tamanna" class="m eq bx adi adj cx" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_vzarYR3cSpNFC0waLC_jLw.jpg"><div class="em bx m adi adj eo o aj ep"></div></div></a></div></div></div></div></div><div><div class="m" aria-hidden="false" aria-describedby="182" aria-labelledby="182" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@tam.tamanna18?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Tamanna</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/@tam.tamanna18/fireplexity-vs-perplexity-ai-in-open-source-answer-engines-c456f394d1fe?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">Fireplexity vs Perplexity AI in Open Source Answer Engines</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">In the world of AI-powered search tools, two names stand out: Fireplexity and Perplexity AI. These “answer engines” go beyond traditional…</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="183" aria-labelledby="183"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="184" aria-labelledby="184" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div>2d ago<div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://medium.com/@tam.tamanna18/fireplexity-vs-perplexity-ai-in-open-source-answer-engines-c456f394d1fe?source=post_page---read_next_recirc--2ce1ecaa901c----1---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="304" aria-labelledby="304" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>1.1K</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="185" aria-labelledby="185" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="305" aria-labelledby="305" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://albertoromgar.medium.com/gpt-5-is-here-theres-only-one-feature-worth-writing-about-4aa4be1df15b" tabindex="0"><div class="acq"><div aria-label="GPT-5 Is Here: There’s Only One Feature Worth Writing About"><div class="acs act acu acv ys"><img alt="GPT-5 Is Here: There’s Only One Feature Worth Writing About" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_xWmRdQIj8Hwrfim0Z6YfAA.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="187" aria-labelledby="187" role="tooltip"><div tabindex="-1" class="be"><a tabindex="-1" href="https://albertoromgar.medium.com/?source=post_page---read_next_recirc--2ce1ecaa901c----2---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div class="m ee"><img alt="Alberto Romero" class="m eq bx adi adj cx" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1__BorRAHo8o40sBLbZ7VE3Q.jpg"><div class="em bx m adi adj eo o aj ep"></div></div></a></div></div></div></div></div><div><div class="m" aria-hidden="false" aria-describedby="188" aria-labelledby="188" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://albertoromgar.medium.com/?source=post_page---read_next_recirc--2ce1ecaa901c----2---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Alberto Romero</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://albertoromgar.medium.com/gpt-5-is-here-theres-only-one-feature-worth-writing-about-4aa4be1df15b?source=post_page---read_next_recirc--2ce1ecaa901c----2---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">GPT-5 Is Here: There’s Only One Feature Worth Writing About</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">My short review of OpenAI’s new flagship model</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="189" aria-labelledby="189"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="190" aria-labelledby="190" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div><span>Aug 8</span><div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://albertoromgar.medium.com/gpt-5-is-here-theres-only-one-feature-worth-writing-about-4aa4be1df15b?source=post_page---read_next_recirc--2ce1ecaa901c----2---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="306" aria-labelledby="306" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>2.4K</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="191" aria-labelledby="191" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>107</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="192" aria-labelledby="192" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="308" aria-labelledby="308" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span><div class="k j e"><div class="su bh uq lu"></div></div></div></div></div></div></div></div></article></div></div><div class="abf abg abh sg abi abj abk sf abl abm abn abo abp abq abr abs abt abu abv abw abx"><div class="aby abz aca acb acc dz m"><article class="dz" data-testid="post-preview"><div class="dz qx m"><div class="bh dz"><div class="dz m"><div class="ee dz acd ace acf acg ach aci acj ack acl acm acn aco acp" role="link" data-href="https://python.plainenglish.io/the-python-tool-i-built-in-a-weekend-that-now-pays-my-rent-bbc5b81a0bdd" tabindex="0"><div class="acq"><div aria-label="The Python Tool I Built in a Weekend That Now Pays My Rent"><div class="acs act acu acv ys"><img alt="The Python Tool I Built in a Weekend That Now Pays My Rent" class="bh acw acx acy bw" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_uqWjBh7BUL2Wa9SFFCBBAg.png"></div></div></div><div class="acr ac cb co"><div class="ac co xe bh acz ada adb adc"><div class="add ade adf adg adh ac r"><div class="qo m"><div><div class="m" aria-hidden="false" aria-describedby="194" aria-labelledby="194" role="tooltip"><div tabindex="-1" class="be"><a href="https://python.plainenglish.io/?source=post_page---read_next_recirc--2ce1ecaa901c----3---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div class="ee"><img alt="Python in Plain English" class="cx gl m adj adi" width="20" height="20" loading="lazy" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1_VA3oGfprJgj5fRsTjXp6fA@2x.png"><div class="gl m adi adj eo o em ep"></div></div></a></div></div></div></div><div class="sh m eu"><p class="bf b dy ab dx">In</p></div><div class="m"><div><div class="m" aria-hidden="false" aria-describedby="195" aria-labelledby="195" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://python.plainenglish.io/?source=post_page---read_next_recirc--2ce1ecaa901c----3---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Python in Plain English</p></a></div></div></div></div><div class="adk m"><p class="bf b dy ab dx">by</p></div><div><div class="m" aria-hidden="false" aria-describedby="196" aria-labelledby="196" role="tooltip"><div tabindex="-1" class="be"><a class="ag ah ai ed ak al am an ao ap aq ar as jl ac r" href="https://medium.com/@SulemanSafdar?source=post_page---read_next_recirc--2ce1ecaa901c----3---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><p class="bf b dy ab es zd et ze zf zg zh zi bk">Suleman Safdar</p></a></div></div></div></div><div class="adl m adm adn ado adp adq gs"><div class="adr ads adt adu adv adw adx ady"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://python.plainenglish.io/the-python-tool-i-built-in-a-weekend-that-now-pays-my-rent-bbc5b81a0bdd?source=post_page---read_next_recirc--2ce1ecaa901c----3---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div title=""><h2 class="bf hr sv sw adz aea fh sx sy aeb aec fk fl fm aed aee fn fo fp aef aeg fq fr fs aeh aei ft es et ze zg zi bk">The Python Tool I Built in a Weekend That Now Pays My Rent</h2></div><div class="aej m"><h3 class="bf b fw ab es aek et ze ael zg zi dx">How I turned a tiny automation script into a paid product using libraries, clean OOP, and a little C++ speed where it mattered</h3></div></a></div></div><span class="bf b dy ab dx"><div class="rx ac cp af"><div class="ac r aeo"><div class="qx ac"><div class="bm" aria-hidden="false" aria-describedby="197" aria-labelledby="197"><button class="m aj ap an" aria-label="Member-only story"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="198" aria-labelledby="198" role="tooltip"><div tabindex="-1" class="be"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewBox="0 0 64 64"><path fill="#FFC017" d="m39.637 40.831-5.771 15.871a1.99 1.99 0 0 1-3.732 0l-5.771-15.87a2.02 2.02 0 0 0-1.194-1.195L7.298 33.866a1.99 1.99 0 0 1 0-3.732l15.87-5.771a2.02 2.02 0 0 0 1.195-1.194l5.771-15.871a1.99 1.99 0 0 1 3.732 0l5.771 15.87a2.02 2.02 0 0 0 1.194 1.195l15.871 5.771a1.99 1.99 0 0 1 0 3.732l-15.87 5.771a2.02 2.02 0 0 0-1.195 1.194"></path></svg></div></div></div></div></button></div></div>6d ago<div class=""><div class="ee aem dh ac r"><div class="eo gi aen ac r aeo"><div class="aep dh dj m cx"></div></div><a class="eo lv aen ac r aeo" tabindex="-1" href="https://python.plainenglish.io/the-python-tool-i-built-in-a-weekend-that-now-pays-my-rent-bbc5b81a0bdd?source=post_page---read_next_recirc--2ce1ecaa901c----3---------------------26d401da_8f13_410f_96a5_d816c329137e--------------" rel="noopener follow"><div><div class="ac" aria-hidden="false" aria-describedby="309" aria-labelledby="309" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" aria-labelledby="clap-filled-static-desc" viewBox="0 0 16 16"><desc id="clap-filled-static-desc">A clap icon</desc><path fill="#6B6B6B" fill-rule="evenodd" d="m3.672 10.167 2.138 2.14h-.002c1.726 1.722 4.337 2.436 5.96.81 1.472-1.45 1.806-3.68.76-5.388l-1.815-3.484c-.353-.524-.849-1.22-1.337-.958-.49.261 0 1.56 0 1.56l.78 1.932L6.43 2.866c-.837-.958-1.467-1.108-1.928-.647-.33.33-.266.856.477 1.598.501.503 1.888 1.957 1.888 1.957.17.174.083.485-.093.655a.56.56 0 0 1-.34.163.43.43 0 0 1-.317-.135s-2.4-2.469-2.803-2.87c-.344-.346-.803-.54-1.194-.15-.408.406-.273 1.065.11 1.447.345.346 2.31 2.297 2.685 2.67l.062.06c.17.175.269.628.093.8-.193.188-.453.33-.678.273a.9.9 0 0 1-.446-.273S2.501 6.84 1.892 6.23c-.407-.406-.899-.333-1.229 0-.525.524.263 1.28 1.73 2.691.384.368.814.781 1.279 1.246m8.472-7.219c.372-.29.95-.28 1.303.244V3.19l1.563 3.006.036.074c.885 1.87.346 4.093-.512 5.159l-.035.044c-.211.264-.344.43-.74.61 1.382-1.855.963-3.478-.248-5.456L11.943 3.88l-.002-.037c-.017-.3-.039-.71.203-.895" clip-rule="evenodd"></path></svg><span>170</span></div></div></div></div><div><div class="ac" aria-hidden="false" aria-describedby="199" aria-labelledby="199" role="tooltip"><div tabindex="-1" class="be"><div class="ac r zb"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="#6B6B6B" aria-labelledby="response-filled-16px-desc" viewBox="0 0 16 16"><desc id="response-filled-16px-desc">A response icon</desc><path fill="#6B6B6B" d="M12.344 11.458A5.28 5.28 0 0 0 14 7.526C14 4.483 11.391 2 8.051 2S2 4.483 2 7.527c0 3.051 2.712 5.526 6.059 5.526a6.6 6.6 0 0 0 1.758-.236q.255.223.554.414c.784.51 1.626.768 2.512.768a.37.37 0 0 0 .355-.214.37.37 0 0 0-.03-.384 4.7 4.7 0 0 1-.857-1.958v.014z"></path></svg><span>2</span></div></div></div></div></a></div></div></div><div class="ac r aeq aer"><div class=""><div><div class="bm" aria-hidden="false" aria-describedby="200" aria-labelledby="200" role="tooltip"><div tabindex="-1" class="be"><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="ag ef ai ed ak al am aes ao ap aq ga mb mc md"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24" class="aw"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"></path></svg></button></div></div></div></div></div><div class="aet m"><div class="bm" aria-hidden="false"><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false" aria-describedby="311" aria-labelledby="311" role="tooltip"><div tabindex="-1" class="be"><button aria-label="More options" class="ag ef ai ed ak al am aes ao ap aq ga mk ml ly mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></span></div></div></div></div></div></div></article></div></div></div><div class="su bh uq dk dl aeu aev aew"></div><a class="bf b bg ab bk qp ts tt tu tv lx tw tx ty ga tz ua ub uc ud ue uf ug uh ui uj eq bm uk oq" href="https://medium.com/?source=post_page---read_next_recirc--2ce1ecaa901c---------------------------------------" rel="noopener follow"><div class="m oq">See more recommendations</div></a></div></div></div><div class="i l k"><div class="su bh uq ur"></div><div class="ac cb"><div class="ci bh gy gz ha hb"><div class="us ac kw jn"><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://help.medium.com/hc/en-us?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Help</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.statuspage.io/?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Status</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/about?autoplay=1&amp;source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">About</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Careers</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="mailto:pressinquiries@medium.com" rel="noopener follow"><p class="bf b dy ab dx">Press</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://blog.medium.com/?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Blog</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Privacy</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Rules</p></a></div><div class="ut uu m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Terms</p></a></div><div class="ut m"><a class="ag ah ai ed ak al am an ao ap aq ar as at au" href="https://speechify.com/medium?source=post_page-----2ce1ecaa901c---------------------------------------" rel="noopener follow"><p class="bf b dy ab dx">Text to speech</p></a></div></div></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20250818-074158-077215da89"</script><script>window.__GRAPHQL_URI__ = "https://pub.towardsai.net/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"cache":{"experimentGroupSet":true,"reason":"User is logged in","group":"disabled","tags":["group-edgeCachePosts","post-2ce1ecaa901c","user-141fa70b60b6","collection-98111c9905da"],"serverVariantState":"","middlewareEnabled":true,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"pubFeaturingPostPageLabelEnabled":false,"shouldFollowPostQueryEnabled":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"98111c9905da","explicit":true},"viewerIsBot":false},"debug":{"requestId":"f738d101-e32e-48ce-8cac-9cdf74630e20","requestTag":"","hybridDevServices":[],"originalSpanCarrier":{"traceparent":"00-a0d0389d5726942a20f31973eefbe821-6f4d7d677e5c1028-01"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fpub.towardsai.net\u002Fbuild-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c","host":"pub.towardsai.net","hostname":"pub.towardsai.net","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"partnerProgram":{"selectedCountryCode":null},"staticRouterContext":{"route":{"name":"ShowPostUnderCollection"},"statusCode":200},"toastQueue":[],"currentToast":null},"config":{"nodeEnv":"production","version":"main-20250818-074158-077215da89","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","iosAppId":"828256236","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","recaptchaEnterpriseKeyId":"6Le-uGgpAAAAAPprRaokM8AKthQ9KNGdoxaGUvVp","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20250818-074158-077215da89","commit":"077215da89214955d2bc34962dc1231c03103146"}},"datacenter":"us"},"googleAdsCode":"AW-17106321204","googleAnalyticsCode":"G-7JY7T788PK","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumMastodonDomainName":"me.dm","mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"7*V1_7XP4snlmqrc_0Njontw.png","height":110,"width":500},"postLogo":{"imageId":"167cff2a3d17ac1e64d0762539978f2d54c0058886e8b3c8a03a725a83012ec0","height":630,"width":1200},"postPreviewImage":{"imageId":"bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632","height":630,"width":1200}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyV2":"e8a5e126-792b-4ee6-8fba-d574c1b02fc5","monthlyWithTrial":"d5ee3dbe3db8","monthlyPremium":"fa741a9b47a2","yearly":"a40ad4a43185","yearlyV2":"3815d7d6-b8ca-4224-9b8c-182f9047866e","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","yearlyPremium":"e21bd2c12166","monthlyOneYearFree":"e6c0637a-2bad-4171-ab4f-3c268633d83c","monthly25PercentOffFirstYear":"235ecc62-0cdb-49ae-9378-726cd21c504b","monthly20PercentOffFirstYear":"ba518864-9c13-4a99-91ca-411bf0cac756","monthly15PercentOffFirstYear":"594c029b-9f89-43d5-88f8-8173af4e070e","monthly10PercentOffFirstYear":"c6c7bc9a-40f2-4b51-8126-e28511d5bdb0","monthlyForStudents":"629ebe51-da7d-41fd-8293-34cd2f2030a8","yearlyOneYearFree":"78ba7be9-0d9f-4ece-aa3e-b54b826f2bf1","yearly25PercentOffFirstYear":"2dbb010d-bb8f-4eeb-ad5c-a08509f42d34","yearly20PercentOffFirstYear":"47565488-435b-47f8-bf93-40d5fbe0ebc8","yearly15PercentOffFirstYear":"8259809b-0881-47d9-acf7-6c001c7f720f","yearly10PercentOffFirstYear":"9dd694fb-96e1-472c-8d9e-3c868d5c1506","yearlyForStudents":"e29345ef-ab1c-4234-95c5-70e50fe6bc23","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06","fiftyPercentOffOneYear":"FIFTY_PERCENT_OFF_ONE_YEAR"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","q8qw":"usd","d9y6":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"},"imageUploadMaxSizeMb":25,"staffPicks":{"title":"Staff Picks","catalogId":"c7bc6e1ee00f"}},"session":{"xsrf":"210c936c05f6"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_pub_recs_ui","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_verified_book_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"num_post_bottom_responses_to_show","valueType":{"__typename":"VariantFlagNumber","value":3}},{"__typename":"VariantFlag","name":"enable_diversification_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_conversion_model_v2","valueType":{"__typename":"VariantFlagString","value":"group_2"}},{"__typename":"VariantFlag","name":"enable_lite_publications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_manage_membership_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ranker_v10","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_recommended_publishers_query","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mobile_newsletter_setting_in_publishing_details","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_see_pronouns","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_conversion_ranker_v2","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_story_page_nofollow_meta_tag","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kiln_enable_new_digest_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_intrinsic_automatic_actions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reengagement_notification_duration","valueType":{"__typename":"VariantFlagNumber","value":3}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_premium_plan","valueType":{"__typename":"VariantFlagString","value":"12a660186432"}},{"__typename":"VariantFlag","name":"enable_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_post_jsonld","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deviant_get_variant_flag_from_medium2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_reliable_follow_experience_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_bg_post_post","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lo_homepage","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_dynamic_paywall_optin","valueType":{"__typename":"VariantFlagString","value":"copy1"}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_ios_easy_resubscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_programming_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_maim_the_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_remove_twitter_onboarding_step","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_aggregator_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_syntax_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members_username_selection","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_switch_plan_premium_tier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_premium_tier_badge","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_verified_author","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_engagement_service_publish_response","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trust_service_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_yearly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_medium_com_canonical_urls","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_stripe_customers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_dynamic_aspirational_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_branch_openinapp_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_branch_openinapp_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_create_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_rating_prompt_stories_read_threshold","valueType":{"__typename":"VariantFlagNumber","value":2}},{"__typename":"VariantFlag","name":"enable_recaptcha_enterprise","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_gql_client_events","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_lists_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_abandoned_paywall_promotion_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_archive_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"mobile_custom_app_icon","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound-source-serif-pro"}},{"__typename":"VariantFlag","name":"available_annual_premium_plan","valueType":{"__typename":"VariantFlagString","value":"4a442ace1476"}},{"__typename":"VariantFlag","name":"enable_mastodon_for_members","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_simplified_digest_v2_b","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_eventstats_event_processing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ml_rank_enable_ranker_member_split_v1","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_smart_sidebar_order","valueType":{"__typename":"VariantFlagString","value":"lastPublishedAt"}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_editor_new_publishing_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_c","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_delinquency_and_forfeiture","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_new_push_notification_endpoint","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mastodon_avatar_upload","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pre_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_post_viewed_digest_filtering","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"disable_rex_pub_featuring_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_country_expansion","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_first_story","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_inline_comments","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_miro_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_bottom_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_offline_reading","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cache_less_following_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_entities_to_follow_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_aspiriational","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"rex_generator_max_candidates","valueType":{"__typename":"VariantFlagNumber","value":1000}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_verifications_service","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hybrid_ranking_model","valueType":{"__typename":"VariantFlagString","value":"experiment"}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mobile_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_topic_portals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_bottom_responses_native","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_model","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"rex_enable_filter_viewed_v2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sunset_lo_non_moc_upsell","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_cancellation_discount_v1_1","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_update_topic_portals_wtf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"goliath_externalsearch_enable_comment_deindexation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_update_explore_wtf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_dynamic_paywall_programming","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_bottom_responses_input","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"price_smoke_test_monthly","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_deprecate_legacy_providers_v3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_image_sharer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_winback_promotion_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_v4","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sharer_validate_post_share_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"get_highlights_from_engagement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_boost_experiment","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_configure_pronouns","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_postpage_banners","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_user_profile_nofollow_attribute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_publish_permission_check","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig_in_apps","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_enable_friend_links_creation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_moc_load_processor_all_recs_surfaces","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_abandoned_cart_promotion_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_remove_canonical_url_for_pub_editor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_members_only_audio","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_dense_post_preview","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"enable_dynamic_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_susi_redesign_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_lo_non_moc_upsell","valueType":{"__typename":"VariantFlagString","value":"control"}}],"viewer":{"__ref":"User:a67723e842b6"},"collectionByDomainOrSlug({\"domainOrSlug\":\"pub.towardsai.net\"})":{"__ref":"Collection:98111c9905da"},"postResult({\"id\":\"2ce1ecaa901c\"})":{"__ref":"Post:2ce1ecaa901c"},"collection({\"id\":\"98111c9905da\"})":{"__ref":"Collection:98111c9905da"}},"UserViewerEdge:userId:a67723e842b6-viewerId:a67723e842b6":{"__typename":"UserViewerEdge","id":"userId:a67723e842b6-viewerId:a67723e842b6","createdAt":1642404731354},"User:a67723e842b6":{"__typename":"User","id":"a67723e842b6","allowEmailAddressSharingEditorWriter":false,"dismissableFlags":[],"emailObfuscated":"sa•••••••••••••@gmail.com","geolocation":{"__typename":"Geolocation","country":"IN"},"hasGroupGiftingEnabled":false,"hasPastMemberships":true,"hasSubdomain":false,"imageId":"","isEligibleToImportEmails":false,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"membership":null,"name":"skr3178","partnerProgramEnrollment":null,"styleEditorOnboardingVersionSeen":0,"twitterScreenName":"","unverifiedEmail":"","username":"sangram.kr.rout","viewerEdge":{"__ref":"UserViewerEdge:userId:a67723e842b6-viewerId:a67723e842b6"},"pronouns":[]},"ImageMetadata:1*yPSiS-zAdm-XKV-t6eEuOg.png":{"__typename":"ImageMetadata","id":"1*yPSiS-zAdm-XKV-t6eEuOg.png"},"Collection:98111c9905da":{"__typename":"Collection","id":"98111c9905da","favicon":{"__ref":"ImageMetadata:1*yPSiS-zAdm-XKV-t6eEuOg.png"},"domain":"pub.towardsai.net","slug":"towards-artificial-intelligence","googleAnalyticsId":null,"name":"Towards AI","avatar":{"__ref":"ImageMetadata:1*JyIThO-cLjlChQLb6kSlVQ.png"},"description":"The leading AI community and content platform focused on making AI accessible to all. Check out our new course platform: https:\u002F\u002Facademy.towardsai.net\u002Fcourses\u002Fbeginner-to-advanced-llm-dev","subscriberCount":85448,"latestPostsConnection({\"paging\":{\"limit\":1}})":{"__typename":"PostConnection","posts":[{"__ref":"Post:e6187805ffd5"}]},"compatV3":{"__ref":"Publication:98111c9905da"},"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:98111c9905da-viewerId:a67723e842b6"},"twitterUsername":"towards_AI","facebookPageId":null},"ImageMetadata:1*JyIThO-cLjlChQLb6kSlVQ.png":{"__typename":"ImageMetadata","id":"1*JyIThO-cLjlChQLb6kSlVQ.png"},"User:c0be5b37b3d2":{"__typename":"User","id":"c0be5b37b3d2","customDomainState":null,"hasSubdomain":false,"username":"muthu10star"},"Post:e6187805ffd5":{"__typename":"Post","id":"e6187805ffd5","firstPublishedAt":1755388906854,"creator":{"__ref":"User:c0be5b37b3d2"},"collection":{"__ref":"Collection:98111c9905da"},"isSeries":false,"mediumUrl":"https:\u002F\u002Fpub.towardsai.net\u002Fnew-study-ai-models-fail-at-basic-human-psychology-despite-50-million-investment-e6187805ffd5","sequence":null,"uniqueSlug":"new-study-ai-models-fail-at-basic-human-psychology-despite-50-million-investment-e6187805ffd5"},"Publication:98111c9905da":{"__typename":"Publication","id":"98111c9905da","theme":{"__typename":"PublicationTheme","accentColor":{"__typename":"ColorValue","rgb":"#00ADFF"}}},"LinkedAccounts:141fa70b60b6":{"__typename":"LinkedAccounts","mastodon":null,"id":"141fa70b60b6"},"NewsletterV3:b733f2393e45":{"__typename":"NewsletterV3","id":"b733f2393e45","type":"NEWSLETTER_TYPE_AUTHOR","slug":"141fa70b60b6","name":"141fa70b60b6","collection":null,"user":{"__ref":"User:141fa70b60b6"}},"User:141fa70b60b6":{"__typename":"User","id":"141fa70b60b6","name":"Milan Tamang","username":"tamangmilan","newsletterV3":{"__ref":"NewsletterV3:b733f2393e45"},"linkedAccounts":{"__ref":"LinkedAccounts:141fa70b60b6"},"isSuspended":false,"imageId":"1*mfDWMsklE8l0-1rrnhbJ4g.jpeg","customDomainState":{"__typename":"CustomDomainState","live":null},"hasSubdomain":false,"verifications":{"__typename":"VerifiedInfo","isBookAuthor":false},"socialStats":{"__typename":"SocialStats","followerCount":2528,"followingCount":63,"collectionFollowingCount":9},"bio":"AI Architect https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Ftamangmilan","membership":{"__ref":"Membership:8a379452-a202-4fa7-b086-6430fcdc35ff"},"allowNotes":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:141fa70b60b6-viewerId:a67723e842b6"},"twitterScreenName":""},"Membership:8a379452-a202-4fa7-b086-6430fcdc35ff":{"__typename":"Membership","tier":"MEMBER","id":"8a379452-a202-4fa7-b086-6430fcdc35ff"},"Paragraph:7956b3655d6f_0":{"__typename":"Paragraph","id":"7956b3655d6f_0","name":"a79a","type":"H3","href":null,"layout":null,"metadata":null,"text":"Build Your Own Llama 3 Architecture from Scratch Using PyTorch","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_1":{"__typename":"Paragraph","id":"7956b3655d6f_1","name":"b491","type":"P","href":null,"layout":null,"metadata":null,"text":"A step-by-step guide to building the complete architecture of the Llama 3 model from scratch and performing training and inferencing on a custom dataset.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_xNP7aBpcmcMk4tXJ-Z8Mw.png":{"__typename":"ImageMetadata","id":"1*_xNP7aBpcmcMk4tXJ-Z8Mw.png","originalHeight":1572,"originalWidth":934,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_2":{"__typename":"Paragraph","id":"7956b3655d6f_2","name":"5b84","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*_xNP7aBpcmcMk4tXJ-Z8Mw.png"},"text":"[Image by writer]: Llama 3 architecture shows training and inferencing flow. I imagined this diagram as the official Llama 3 paper doesn’t have one. By the end of this article, I believe you should be able to draw a better architecture than this one.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":250,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_3":{"__typename":"Paragraph","id":"7956b3655d6f_3","name":"93d5","type":"H4","href":null,"layout":null,"metadata":null,"text":"What will you achieve by the end of this article?","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":49,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_4":{"__typename":"Paragraph","id":"7956b3655d6f_4","name":"1ab0","type":"OLI","href":null,"layout":null,"metadata":null,"text":"You’ll get an in-depth intuition of how each component of the Llama 3 model works under the hood.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_5":{"__typename":"Paragraph","id":"7956b3655d6f_5","name":"4e0a","type":"OLI","href":null,"layout":null,"metadata":null,"text":"You’ll write codes to build each component of Llama 3 and then assemble them all together to build a fully functional Llama 3 model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_6":{"__typename":"Paragraph","id":"7956b3655d6f_6","name":"61e5","type":"OLI","href":null,"layout":null,"metadata":null,"text":"You’ll also write codes to train your model with new custom datasets.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_7":{"__typename":"Paragraph","id":"7956b3655d6f_7","name":"aa26","type":"OLI","href":null,"layout":null,"metadata":null,"text":"You’ll also write code to perform inferencing so that your Llama 3 model can generate new texts based on input prompts.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_8":{"__typename":"Paragraph","id":"7956b3655d6f_8","name":"c0f7","type":"H4","href":null,"layout":null,"metadata":null,"text":"Prerequisites","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_9":{"__typename":"Paragraph","id":"7956b3655d6f_9","name":"4561","type":"ULI","href":null,"layout":null,"metadata":null,"text":"A basic knowledge of Python and Pytorch is required.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_10":{"__typename":"Paragraph","id":"7956b3655d6f_10","name":"973c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"A basic understanding of transformer concepts such as Self- attention and also knowledge of deep neural networks would certainly help though not compulsory.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_11":{"__typename":"Paragraph","id":"7956b3655d6f_11","name":"4abe","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we know what we want to achieve, let’s start building everything step by step.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":87,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_12":{"__typename":"Paragraph","id":"7956b3655d6f_12","name":"2996","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 1: The Input Block","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_13":{"__typename":"Paragraph","id":"7956b3655d6f_13","name":"c900","type":"P","href":null,"layout":null,"metadata":null,"text":"As shown in the Llama 3 architecture diagram above, the input block has 3 components:- Texts\u002F Prompts, Tokenizer and Embeddings.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":87,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_14":{"__typename":"Paragraph","id":"7956b3655d6f_14","name":"5e56","type":"P","href":null,"layout":null,"metadata":null,"text":"How do the components inside the Input Block work? There is a popular saying “A picture is worth a thousand words”, let’s check the flow diagram below to understand the workflow inside the Input block.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*7iKiOUpXAWUJ_vlVkWuC_w.png":{"__typename":"ImageMetadata","id":"1*7iKiOUpXAWUJ_vlVkWuC_w.png","originalHeight":1324,"originalWidth":1672,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_15":{"__typename":"Paragraph","id":"7956b3655d6f_15","name":"b0b8","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*7iKiOUpXAWUJ_vlVkWuC_w.png"},"text":"[Image by writer]: Input Block flow diagram displaying prompts, tokenizer, and embedding flow.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":94,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_16":{"__typename":"Paragraph","id":"7956b3655d6f_16","name":"ce54","type":"ULI","href":null,"layout":null,"metadata":null,"text":"First of all, a single or batch of texts\u002Fprompts will be passed into the model. For example: “Hello World” in the above flow diagram.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_17":{"__typename":"Paragraph","id":"7956b3655d6f_17","name":"96b1","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The input to the model should always be in number format as it is unable to process text. Tokenizer helps to convert these texts\u002Fprompts into token-ids (which is an index number representation of tokens in vocabulary). We’ll use the popular Tiny Shakespeare dataset to build the vocabulary and also train our model.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_18":{"__typename":"Paragraph","id":"7956b3655d6f_18","name":"5f32","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The tokenizer used in the Llama 3 model is TikToken, a type of subword tokenizer. However, we’ll be using a character-level tokenizer for our model building. The main reason is that we should know how to build a vocabulary and tokenizer including encode and decode functions all by ourselves. This way we’ll be able to learn how everything works under the hood and we’ll have full control over the code.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_19":{"__typename":"Paragraph","id":"7956b3655d6f_19","name":"a262","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Finally, each token-id will be transformed into an embedding vector of dimensions 128(in original Llama 3 8B, it is 4096). The embeddings will then be passed into the next block called the Decoder Block.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_20":{"__typename":"Paragraph","id":"7956b3655d6f_20","name":"f564","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s code the Input block:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_21":{"__typename":"Paragraph","id":"7956b3655d6f_21","name":"0c7f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Import necessary libraries\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nimport math\nimport numpy as np\nimport time\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, List\nimport pandas as pd\nfrom matplotlib import pyplot as plt","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_22":{"__typename":"Paragraph","id":"7956b3655d6f_22","name":"97e8","type":"PRE","href":null,"layout":null,"metadata":null,"text":"### Step 1: Input Block ###\n\n# Using Tiny Shakespeare dataset for character-level tokenizer. Some part of the following character-level tokenizer is referenced from Andrej karpathy's GitHub (https:\u002F\u002Fgithub.com\u002Fkarpathy\u002FnanoGPT\u002Fblob\u002Fmaster\u002Fdata\u002Fshakespeare_char\u002Fprepare.py) which I found is explained very well.\n# Load tiny_shakespeare data file (https:\u002F\u002Fgithub.com\u002Ftamangmilan\u002Fllama3\u002Fblob\u002Fmain\u002Ftiny_shakespeare.txt)\n\ndevice: str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availability\n\n# Load tiny_shakespeare data file.\nwith open('tiny_shakespeare.txt', 'r') as f:\n  data = f.read()\n\n# Prepare vocabulary by taking all the unique characters from the tiny_shakespeare data\nvocab = sorted(list(set(data)))\n\n# Training Llama 3 model requires addtional tokens such as \u003C|begin_of_text|\u003E, \u003C|end_of_text|\u003E and \u003C|pad_id|\u003E, we'll add them into vocabulary\nvocab.extend(['\u003C|begin_of_text|\u003E','\u003C|end_of_text|\u003E','\u003C|pad_id|\u003E'])\nvocab_size = len(vocab)\n\n# Create a mapping between characters with corresponding integer indexes in vocabulary.\n# This is important to build tokenizers encode and decode functions.\nitos = {i:ch for i, ch in enumerate(vocab)}\nstoi = {ch:i for i, ch in enumerate(vocab)}\n\n# Tokenizers encode function: take a string, output a list of integers\ndef encode(s):\n  return [stoi[ch] for ch in s]\n\n# Tokenizers decode function: take a list of integers, output a string\ndef decode(l):\n  return ''.join(itos[i] for i in l)\n\n# Define tensor token variable to be used later during model training\ntoken_bos = torch.tensor([stoi['\u003C|begin_of_text|\u003E']], dtype=torch.int, device=device)\ntoken_eos = torch.tensor([stoi['\u003C|end_of_text|\u003E']], dtype=torch.int, device=device)\ntoken_pad = torch.tensor([stoi['\u003C|pad_id|\u003E']], dtype=torch.int, device=device)\n\nprompts = \"Hello World\"\nencoded_tokens = encode(prompts)\ndecoded_text = decode(encoded_tokens)\n\n### Test: Input Block Code ###\n# You need take out the triple quotes below to perform testing\n\"\"\"\nprint(f\"Lenth of shakespeare in character: {len(data)}\")\nprint(f\"The vocabulary looks like this: {''.join(vocab)}\\n\")\nprint(f\"Vocab size: {vocab_size}\")\nprint(f\"encoded_tokens: {encoded_tokens}\")\nprint(f\"decoded_text: {decoded_text}\")\n\"\"\"\n### Test Results: ###\n\"\"\"\nLenth of shakespeare in character: 1115394\nThe vocabulary looks like this: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\u003C|begin_of_text|\u003E\u003C|end_of_text|\u003E\u003C|pad_id|\u003E\n\nVocab size: 68\nencoded_tokens: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\ndecoded_text: Hello World\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_23":{"__typename":"Paragraph","id":"7956b3655d6f_23","name":"a101","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 2: The Decoder Block","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_24":{"__typename":"Paragraph","id":"7956b3655d6f_24","name":"c689","type":"P","href":null,"layout":null,"metadata":null,"text":"If you look at the architecture diagram above, the decoder block consists of the following sub-components.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_25":{"__typename":"Paragraph","id":"7956b3655d6f_25","name":"b6ec","type":"ULI","href":null,"layout":null,"metadata":null,"text":"RMS Norm","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_26":{"__typename":"Paragraph","id":"7956b3655d6f_26","name":"bb9b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Rotary Positional Encoding","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_27":{"__typename":"Paragraph","id":"7956b3655d6f_27","name":"52ec","type":"ULI","href":null,"layout":null,"metadata":null,"text":"KV Cache","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_28":{"__typename":"Paragraph","id":"7956b3655d6f_28","name":"3c19","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Group Query Attention","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_29":{"__typename":"Paragraph","id":"7956b3655d6f_29","name":"3e2d","type":"ULI","href":null,"layout":null,"metadata":null,"text":"FeedForward Network","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_30":{"__typename":"Paragraph","id":"7956b3655d6f_30","name":"2c6b","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Decoder Block","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_31":{"__typename":"Paragraph","id":"7956b3655d6f_31","name":"be63","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s deep dive into each of these sub-components one by one.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_32":{"__typename":"Paragraph","id":"7956b3655d6f_32","name":"9ddd","type":"H4","href":null,"layout":null,"metadata":null,"text":"2a. RMS Norm (Root Mean Square Normalization):","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_33":{"__typename":"Paragraph","id":"7956b3655d6f_33","name":"68c5","type":"P","href":null,"layout":null,"metadata":null,"text":"Why do you need RMSNorm? In the architecture diagram above, you must have noticed that the output of the input block i.e. embedding vector passes through the RMSNorm block. This is because the embedding vector has many dimensions (4096 dim in Llama3-8b) and there is always a chance of having values in different ranges. This can cause model gradients to explode or vanish hence resulting in slow convergence or even divergence. RMSNorm brings these values into a certain range which helps to stabilize and accelerate the training process. This makes gradients have more consistent magnitudes and that results in making models converge more quickly.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":122,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":158,"end":171,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_34":{"__typename":"Paragraph","id":"7956b3655d6f_34","name":"e222","type":"P","href":null,"layout":null,"metadata":null,"text":"How does RMSNorm work? Let’s look at the following diagram first.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*l3namyydml7LFi67PqUu1w.png":{"__typename":"ImageMetadata","id":"1*l3namyydml7LFi67PqUu1w.png","originalHeight":894,"originalWidth":2540,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_35":{"__typename":"Paragraph","id":"7956b3655d6f_35","name":"4fc1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*l3namyydml7LFi67PqUu1w.png"},"text":"[Image by writer]: RMSNorm implementation on the input embedding of shape [3,3]","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":79,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_36":{"__typename":"Paragraph","id":"7956b3655d6f_36","name":"58db","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Just like layer normalization, RMSNorm is applied along the embedding features or dimension. The diagram above has embeddings of shape [3,3] meaning each token has 3 dimensions.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_37":{"__typename":"Paragraph","id":"7956b3655d6f_37","name":"03b4","type":"P","href":null,"layout":null,"metadata":null,"text":"Example: Let’s apply RMSNorm to the embedding of the first token X1:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":68,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_38":{"__typename":"Paragraph","id":"7956b3655d6f_38","name":"3662","type":"ULI","href":null,"layout":null,"metadata":null,"text":"The value of token X1 at each dimension i.e. x11, x12, and x13 will be individually divided by the Root Mean Square of all these values. The formula is shown in the diagram above.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":99,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_39":{"__typename":"Paragraph","id":"7956b3655d6f_39","name":"7108","type":"ULI","href":null,"layout":null,"metadata":null,"text":"E (Epsilon) which is a small constant is added to the Root Mean Square to avoid division by Zero for numerical stability.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_40":{"__typename":"Paragraph","id":"7956b3655d6f_40","name":"5c42","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Finally, a scaling parameter Gamma (Y) is multiplied by it. Each feature has one unique Gamma parameter (just like Y1 for dim d1, Y2 for dim d2 and Y3 for dim d3 in the diagram above) is a learning parameter that is scaled up or down to bring further stability to the normalization. The gamma parameter is initialized with value 1 (as shown in the calculation above).","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":29,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_41":{"__typename":"Paragraph","id":"7956b3655d6f_41","name":"f01c","type":"ULI","href":null,"layout":null,"metadata":null,"text":"As you noticed in the example above, the embedding values are large and spread in a wide range. After applying RMSNorm, the values are much smaller and in a small range. The calculation has been done with actual RMSNorm function.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_42":{"__typename":"Paragraph","id":"7956b3655d6f_42","name":"7f8f","type":"P","href":null,"layout":null,"metadata":null,"text":"Why choose RMSNorm over layer normalization? As you noticed above in the example, we didn’t calculate any mean or variance which is done in the case of layer normalization. Thus, we can say that RMSNorm reduces the computational overhead by avoiding the calculation of mean and variance. Also, according to the paper by the Author, RMSNorm gives performance advantages while not compromising on accuracy.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_43":{"__typename":"Paragraph","id":"7956b3655d6f_43","name":"fa69","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s code the RMSNorm:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_44":{"__typename":"Paragraph","id":"7956b3655d6f_44","name":"236f","type":"PRE","href":null,"layout":null,"metadata":null,"text":"# Step2: The Decoder Block\n# Note: Since the Llama 3 model is developed by Meta, so to be in sync with their codebase and for future compatibility,\n# I will use most of the code from Meta GitHub with some necessary changes required to achieve our goal.\n\n# Define parameters dataclass: we'll use these parameters during model building, training and inference.\n# Note: Since we want to see the results of training and inferencing faster rather than focusing on high accuracy, we're taking lower values for most of the parameters which are set higher in the Llama 3 model.\n\n@dataclass\nclass ModelArgs:\n    dim: int = 512              # embedding dimension\n    n_layers: int = 8           # number of model decoder blocks\n    n_heads: int = 8            # number of heads for queries embedding\n    n_kv_heads: int = 4         # number of heads for keys and values embedding\n    vocab_size: int = len(vocab) # Length of vocabulary\n    multiple_of: int = 256        # Require to calculate dim of feedfoward network\n    ffn_dim_multiplier: Optional[float] = None  # Require to calculate dim of feedfoward network\n    norm_eps: float = 1e-5                       # Default Epsilon value set for the RMSNorm calculation\n    rope_theta: float = 10000.0   # Default theta value for the RePE calculation\n\n    max_batch_size: int = 10     # Max batch size\n    max_seq_len: int = 256         # Max sequence length\n\n    epochs: int = 2500             # Total number of training iteration\n    log_interval: int = 10        # Number of interval to print the logs and loss values   \n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'   # Assign device to cuda or cpu based on availability ","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_45":{"__typename":"Paragraph","id":"7956b3655d6f_45","name":"ba87","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step2a: The RMSNorm\n\nclass RMSNorm(nn.Module):\n  def __init__(self, dim: int, eps: float = 1e-6):\n    super().__init__()\n    device = ModelArgs.device\n    self.eps = eps\n    # Scaling parameter gamma, initialized with one and the no of parameters is equal to the size of dim\n    self.weight = nn.Parameter(torch.ones(dim).to(device))\n\n  def _norm(self, x):\n    return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps).to(device)\n\n  def forward(self, x):\n    #Shape: x[bs,seq,dim]\n    output = self._norm(x.float()).type_as(x)\n\n    #Shape: x[bs,seq,dim] -\u003E x_norm[bs,seq,dim]\n    return output * self.weight\n\n### Test: RMSNorm Code ###\n# You need take out the triple quotes below to perform testing\n\"\"\"\nx = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)\nrms_norm = RMSNorm(dim=ModelArgs.dim)\nx_norm = rms_norm(x)\n\nprint(f\"Shape of x: {x.shape}\")\nprint(f\"Shape of x_norm: {x_norm.shape}\")\n\"\"\"\n### Test Results: ###\n\"\"\"\nShape of x: torch.Size([10, 256, 512])\nShape of x_norm: torch.Size([10, 256, 512])\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_46":{"__typename":"Paragraph","id":"7956b3655d6f_46","name":"a3e5","type":"H4","href":null,"layout":null,"metadata":null,"text":"2b. Rotary Positional Encoding (RoPE):","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_47":{"__typename":"Paragraph","id":"7956b3655d6f_47","name":"6e81","type":"P","href":null,"layout":null,"metadata":null,"text":"Why we do need Rotary Positional Encoding (RoPE)? Before we get into the why part, let’s review what we’ve done so far. First, we’ve converted input texts into embeddings. Next, we’ve applied RMSNorm to the embeddings. At this point, you must have noticed something is off. Let’s say the input text is “I love apple” or “apple love I”, the model will still treat both sentences as the same and learn it as the same. Because there is no order defined in the embeddings for the model to learn. Hence, the order is very important for any language model. In Llama 3 model architecture, RePE is used to define the position of each token in the sentences that maintain not only the order but also maintains the relative position of tokens in the sentences.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_48":{"__typename":"Paragraph","id":"7956b3655d6f_48","name":"d03d","type":"P","href":null,"layout":null,"metadata":null,"text":"So, what is Rotary Positional Encoding and how does it work? As mentioned in the why section above, RoPE is a type of position encoding that encodes the embeddings which maintains the order of tokens in the sentences by adding absolute positional information as well as incorporates the relative position information among the tokens. It performs the encoding action by rotating a given embedding by a special matrix called the rotation matrix. This simple yet very powerful mathematical derivation using rotation matrix is the heart of RoPE.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":227,"end":258,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":287,"end":316,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":445,"end":542,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*fpgtE7L7Br3Azn1KNcTOHw.png":{"__typename":"ImageMetadata","id":"1*fpgtE7L7Br3Azn1KNcTOHw.png","originalHeight":1152,"originalWidth":1612,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_49":{"__typename":"Paragraph","id":"7956b3655d6f_49","name":"906e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*fpgtE7L7Br3Azn1KNcTOHw.png"},"text":"[Image by writer]: Rotation matrix applied to 2-d vector","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_50":{"__typename":"Paragraph","id":"7956b3655d6f_50","name":"427a","type":"P","href":null,"layout":null,"metadata":null,"text":"The rotation matrix in the diagram above rotates a vector of 2-dimension. However, the number of dimensions in the Llama 3 model is 4096 which is a lot more. Let’s take a look at how to apply rotation on higher-dimension embeddings.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*s1VfCNxSdWCpvkAR7ELLpA.png":{"__typename":"ImageMetadata","id":"1*s1VfCNxSdWCpvkAR7ELLpA.png","originalHeight":1406,"originalWidth":1726,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_51":{"__typename":"Paragraph","id":"7956b3655d6f_51","name":"2e97","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*s1VfCNxSdWCpvkAR7ELLpA.png"},"text":"[Image by writer]: Example of RoPE implementation to Embedding","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_52":{"__typename":"Paragraph","id":"7956b3655d6f_52","name":"0bff","type":"P","href":null,"layout":null,"metadata":null,"text":"We now know that the rotation of embeddings involves the multiplication of each embedding position (m) value and theta (θ) for each pair of embedding dimensions. This is how RoPE can capture absolute position as well as relative position information by the implementation of the rotation matrix.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_53":{"__typename":"Paragraph","id":"7956b3655d6f_53","name":"d518","type":"P","href":null,"layout":null,"metadata":null,"text":"Note: the rotation matrix needs to be converted to polar form and the embedding vector needs to converted to complex before performing rotation. After rotation is completed, the rotated embeddings need to be converted back to real for attention operation. Also, RoPE is applied to Query and Key embedding only. It doesn’t apply to Value embedding.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":4,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_54":{"__typename":"Paragraph","id":"7956b3655d6f_54","name":"9a43","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s dive into RoPE coding:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_55":{"__typename":"Paragraph","id":"7956b3655d6f_55","name":"6781","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step2b: The RoPE\ndef precompute_freqs_cis(dim:int, seq_len: int, theta: float=10000.0):\n  # Computing Theta value for each dim pair which is dim\u002F2\n  device = ModelArgs.device\n  freqs = 1.0 \u002F (theta ** (torch.arange(0, dim, 2,device=device)[:(dim\u002F\u002F2)].float()\u002Fdim))\n\n  # Computing range of positions(m) in the sequence\n  t = torch.arange(seq_len, dtype=torch.float32, device=device)\n\n  # freqs gives all the Theta value range for all the position of tokens in the sequence\n  freqs = torch.outer(t, freqs).to(device)\n\n  # This is the rotation matrix which needs to be converted to Polar form in order to perform rotation to the embedding\n  freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)\n  return freqs_cis\n\ndef reshape_for_broadcast(freqs_cis, x):\n  ndim = x.ndim\n  assert 0\u003C=1\u003Cndim\n  assert freqs_cis.shape == (x.shape[1],x.shape[-1]), \"the last two dimension of freqs_cis, x must match\"\n  shape = [d if i==1 or i==ndim-1 else 1 for i,d in enumerate(x.shape)]\n  return freqs_cis.view(*shape)\n\ndef apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)-\u003ETuple[torch.Tensor, torch.Tensor]:\n  device = ModelArgs.device\n  # Applying rotary positional encoding to both query and key embedding together\n  # First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.\n  # Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number\n  xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)).to(device)    #xq_:[bsz, seq_len, n_heads, head_dim\u002F2]\n  xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)).to(device)    #xk_:[bsz, seq_len, n_heads, head_dim\u002F2]\n\n  # The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding\n  # Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis:[seq_len,head_dim] -\u003E freqs_cis:[1,seq_len,1,head_dim]\n  freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n\n  #Finally, perform rotation operation by multiplying with freqs_cis.\n  #After the rotation is completed, convert both xq_out and xk_out back to real number and return\n  xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).to(device) #xq_out:[bsz, seq_len, n_heads, head_dim]\n  xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).to(device) #xk_out:[bsz, seq_len, n_heads, head_dim]\n  return xq_out.type_as(xq), xk_out.type_as(xk)\n\n### Test: RoPE Code ###\n# Note: x_norm is calculated during RMSNorm and is being used for testing here.\n# You need take out the triple quotes below to perform testing\n\"\"\"\nhead_dim = ModelArgs.dim\u002F\u002FModelArgs.n_heads\nwq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=device)\nwk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=device)\nxq = wq(x_norm)\nxk = wk(x_norm)\nprint(f\"xq.shape: {xq.shape}\")\nprint(f\"xk.shape: {xk.shape}\")\n\nxq = xq.view(xq.shape[0],xq.shape[1],ModelArgs.n_heads, head_dim)\nxk = xk.view(xk.shape[0],xk.shape[1],ModelArgs.n_kv_heads, head_dim)\nprint(f\"xq.re-shape: {xq.shape}\")\nprint(f\"xk.re-shape: {xk.shape}\")\n\nfreqs_cis = precompute_freqs_cis(dim=head_dim, seq_len=ModelArgs.max_seq_len)\nprint(f\"freqs_cis.shape: {freqs_cis.shape}\")\n\nxq_rotate, xk_rotate = apply_rotary_emb(xq, xk, freqs_cis)\nprint(f\"xq_rotate.shape: {xq_rotate.shape}\")\nprint(f\"xk_rotate.shape: {xk_rotate.shape}\")\n\"\"\"\n### Test Results: ###\n\"\"\"\nxq.shape: torch.Size([10, 256, 512])\nxk.shape: torch.Size([10, 256, 256])\nxq.re-shape: torch.Size([10, 256, 8, 64])\nxk.re-shape: torch.Size([10, 256, 4, 64])\nfreqs_cis.shape: torch.Size([256, 32])\nxq_rotate.shape: torch.Size([10, 256, 8, 64])\nxk_rotate.shape: torch.Size([10, 256, 4, 64])\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_56":{"__typename":"Paragraph","id":"7956b3655d6f_56","name":"b216","type":"H4","href":null,"layout":null,"metadata":null,"text":"2c. KV Cache (Only required at Inferencing):","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_57":{"__typename":"Paragraph","id":"7956b3655d6f_57","name":"c64f","type":"P","href":null,"layout":null,"metadata":null,"text":"What is KV-Cache? In Llama 3 architecture, at the time of inferencing, the concept of KV-Cache is introduced to store previously generated tokens in the form of Key and Value cache. These caches will be used to calculate self-attention to generate the next token. Only key and value tokens are cached whereas query tokens are not cached, hence the term KV Cache.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_58":{"__typename":"Paragraph","id":"7956b3655d6f_58","name":"e659","type":"P","href":null,"layout":null,"metadata":null,"text":"Why do we need KV Cache? Let’s look at the diagram below to clarify our curiosity.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*AC-ns-4_Qdwiol3ZIBolHQ.png":{"__typename":"ImageMetadata","id":"1*AC-ns-4_Qdwiol3ZIBolHQ.png","originalHeight":1010,"originalWidth":1788,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_59":{"__typename":"Paragraph","id":"7956b3655d6f_59","name":"87c2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*AC-ns-4_Qdwiol3ZIBolHQ.png"},"text":"[Image by writer]: KV Cache implementation","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":42,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_60":{"__typename":"Paragraph","id":"7956b3655d6f_60","name":"dd62","type":"ULI","href":null,"layout":null,"metadata":null,"text":"In the A block of the diagram, when the output3 token is being generated, the previous output tokens (output1, output2) are still being calculated which is not necessary at all. This has caused an additional matrix multiplication during attention calculation hence computation resources are increased a lot.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_61":{"__typename":"Paragraph","id":"7956b3655d6f_61","name":"57bd","type":"ULI","href":null,"layout":null,"metadata":null,"text":"In block B of the diagram, the output tokens replace the input token in Query embedding. KV Cache stores the previously generated tokens. During attention score calculation, we will just have to use 1 token from the query and use previous tokens from the Key and Value cache. It reduces the matrix multiplication from 3x3 to 1x3 from block A to block B, which is almost 66% reduction. In the real world, with huge sequence lengths and batch size, this will help to reduce significant computation power. Finally, there will always be only one latest output token generated. This is the main reason KV-Cache has been introduced.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":89,"end":97,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":573,"end":626,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_62":{"__typename":"Paragraph","id":"7956b3655d6f_62","name":"8307","type":"H4","href":null,"layout":null,"metadata":null,"text":"2d. Group Query Attention:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_63":{"__typename":"Paragraph","id":"7956b3655d6f_63","name":"b051","type":"P","href":null,"layout":null,"metadata":null,"text":"Group query attention is the same as Muilt-Head attention which was used in previous models such as Llama 1 with the only difference being in the use of separate heads for queries and separate heads for keys\u002Fvalues. Usually, the number of heads assigned to queries is n-times to that of keys, and values heads. Let’s take a look at the diagram to build our understanding further.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*NGDw7teWrXccU5Xf6JIOMw.png":{"__typename":"ImageMetadata","id":"1*NGDw7teWrXccU5Xf6JIOMw.png","originalHeight":1082,"originalWidth":1728,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_64":{"__typename":"Paragraph","id":"7956b3655d6f_64","name":"7900","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*NGDw7teWrXccU5Xf6JIOMw.png"},"text":"[Image by writer]: Group query attention and Multi-Head Attention","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_65":{"__typename":"Paragraph","id":"7956b3655d6f_65","name":"49b2","type":"P","href":null,"layout":null,"metadata":null,"text":"In the given diagram, the multi-head attention has an equal number of heads across all queries, keys and values which is n_heads = 8.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_66":{"__typename":"Paragraph","id":"7956b3655d6f_66","name":"f601","type":"P","href":null,"layout":null,"metadata":null,"text":"The Group query attention block has 8 heads for queries (n_heads) and 4- heads (n_kv_heads) for keys and values, which is 2 times less than query heads.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_67":{"__typename":"Paragraph","id":"7956b3655d6f_67","name":"d744","type":"P","href":null,"layout":null,"metadata":null,"text":"Since MultiHead Attention is already so good, why do we need Group query attention? To answer this, we need to go back to KV Cache for a while. The KV cache helps reduce computation resources greatly. However, as KV Cache stores more and more previous tokens, the memory resources will increase significantly. This is not a good thing for the model performance point of view as well as the financial point of view. Hence, Group query attention is introduced. Reducing the number of heads for K and V decreases the number of parameters to be stored, and hence, less memory is being used. Various test results have proven that the model accuracy remains in the same ranges with this approach.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":415,"end":458,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_68":{"__typename":"Paragraph","id":"7956b3655d6f_68","name":"5eae","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s implement this in code:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_69":{"__typename":"Paragraph","id":"7956b3655d6f_69","name":"7c1a","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## The Attention Block [Step2c: The KV Cache; Step2d: Group Query Attention]\n## As mentioned before, the naming convention follows original the meta's LLama3 GitHub\n\nclass Attention(nn.Module):\n  def __init__(self, args: ModelArgs):\n    super().__init__()\n    self.args = args\n    # Embedding dimension\n    self.dim = args.dim\n    # Number of heads assigned to Query\n    self.n_heads = args.n_heads\n    # Number of heads assigned to Key and values. If \"None\", the number will be same as Query.\n    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n    # Dimension of each head relative to model dimension\n    self.head_dim = args.dim \u002F\u002F args.n_heads\n    # Number of repetition in order to make time Key, Value heads to match Query heads number\n    self.n_rep = args.n_heads \u002F\u002F args.n_kv_heads\n\n    # Weight initialize for Keys, Querys, Values and Oupt. Notice that the out_feature value of weight for q and kv are based on it's heads\n    self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=False, device=device)\n    self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False, device=device)\n    self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=False, device=device)\n    self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=False, device=device)\n\n    # Initialize caches to store Key, Values at start. (KV Cache Implementation)\n    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)\n\n  def forward(self, x: torch.Tensor, start_pos, inference):\n    # Shape of the input embedding: [bsz,seq_len,dim]\n    bsz, seq_len, _ = x.shape\n    # Mask will be used during 'Training' and is not required for 'inference' due to the use of KV cache.\n    mask = None\n\n    xq = self.wq(x)  #x[bsz,seq_len,dim]*wq[dim,n_heads * head_dim] -\u003E q[bsz,seq_len,n_heads * head_dim]\n    xk = self.wk(x)  #x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -\u003E k[bsz,seq_len,n_kv_heads * head_dim]\n    xv = self.wv(x)  #x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -\u003E v[bsz,seq_len,n_kv_heads * head_dim]\n\n    # Reshaping Querys, Keys and Values by their number of heads. (Group Query Attention Implementation)\n    xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)      #xq[bsz,seq_len,n_heads, head_dim]\n    xk = xk.view(bsz, seq_len, self.n_kv_heads, self.head_dim)   #xk[bsz,seq_len,n_kv_heads, head_dim]\n    xv = xv.view(bsz, seq_len, self.n_kv_heads, self.head_dim)   #xv[bsz,seq_len,n_kv_heads, head_dim]\n\n    # Model - Inference Mode: kv-cache is enabled at inference mode only.\n    if inference:\n      # Compute rotation matrix for each position in the sequence\n      freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len * 2)\n      # During inferencing, we should only take the rotation matrix range from the current position of the tokens.\n      freqs_cis = freqs_cis[start_pos : start_pos + seq_len]\n      # Apply RoPE to Queries and Keys embeddings\n      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n\n      self.cache_k = self.cache_k.to(xq)\n      self.cache_v = self.cache_v.to(xq)\n      # Store Keys and Values token embedding into their respective cache [KV Cache Implementation]\n      self.cache_k[:bsz, start_pos:start_pos + seq_len] = xk\n      self.cache_v[:bsz, start_pos:start_pos + seq_len] = xv\n\n      # Assign all the previous tokens embeddings upto current tokens position to Keys and Values variable for Attention Calculation\n      keys = self.cache_k[:bsz, :start_pos + seq_len]\n      values = self.cache_v[:bsz, :start_pos + seq_len]\n\n      # At this point, they Keys and Values shape aren't same with Queries Embedding which has to be in order to computer attention score\n      # Use repeat_kv function to make Keys,Values shape same as queries shape\n      keys = repeat_kv(keys, self.n_rep)      #keys[bsz,seq_len,n_heads,head_dim]\n      values = repeat_kv(values, self.n_rep)  #values[bsz,seq_len,n_heads,head_dim]\n\n    # Mode - Training mode: KV-Cache not implemented\n    else:\n      # Compute rotation matrix and apply RoPE to queries and keys for for training.\n      freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len)\n\n      #xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]\n      xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n\n      # Use repeat_kv function to make Keys,Values shape same as the queries shape\n      #keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]\n      keys = repeat_kv(xk, self.n_rep)\n      values = repeat_kv(xv, self.n_rep)\n\n      # For training mode, we'll compute mask and apply to the attention score later\n      mask = torch.full((seq_len, seq_len),float(\"-inf\"),device=self.args.device)\n      mask = torch.triu(mask, diagonal=1).to(self.args.device)\n\n    # To compute attention, we'll need to perform a transpose operation to reshape all queries, keys and values bring heads at dim 1 and seq at dim 2\n    xq = xq.transpose(1,2)                  #xq[bsz,n_heads,seq_len,head_dim]\n    keys = keys.transpose(1,2)              #keys[bsz,n_heads,seq_len,head_dim]\n    values = values.transpose(1,2)          #values[bsz,n_heads,seq_len,head_dim]\n\n    # Computing attention score\n    scores = torch.matmul(xq, keys.transpose(2,3)).to(self.args.device)\u002Fmath.sqrt(self.head_dim)\n    if mask is not None:\n      scores = scores + mask\n\n    # Apply softmax to the attention score\n    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n    # Matrix multiplication of attention score with the values\n    output = torch.matmul(scores, values).to(self.args.device)\n\n    # We get the contextual embedding for each head\n    # All heads need to be reshaped back and combined to give a single single contextual attention output\n    # Shape change: output[bsz,n_heads,seq_len,head_dim] -\u003E output[bsz,seq_len, n_heads,head_dim] -\u003E output[bsz,seq_len, n_heads * head_dim]\n    output = output.transpose(1,2).contiguous().view(bsz, seq_len, -1)\n\n    # shape: output [bsz,seq_len,dim]\n    return self.wo(output)\n\n# If the number of keys\u002Fvalues heads is less than query heads, this function expands the key\u002Fvalues embeddings with the required number of repetition\ndef repeat_kv(x:torch.Tensor, n_rep: int)-\u003E torch.Tensor:\n  bsz, seq_len, n_kv_heads, head_dim = x.shape\n  if n_rep == 1:\n    return x\n  return (\n      x[:,:,:,None,:]\n      .expand(bsz,seq_len,n_kv_heads,n_rep, head_dim)\n      .reshape(bsz,seq_len,n_kv_heads * n_rep, head_dim)\n  )\n\n\n### Test: Repeat_kv function ###\n# note: xk, x_norm is already calculated during RoPE, RMSNorm testing and is being used for testing here.\n# You need take out the triple quotes below to perform testing\n\"\"\"\nn_rep = ModelArgs.n_heads \u002F\u002F ModelArgs.n_kv_heads\nkeys = repeat_kv(xk, n_rep)\nprint(f\"xk.shape: {xk.shape}\")\nprint(f\"keys.shape: {keys.shape}\")\n\n## Test: Attention function\n# You need take out the triple quotes below to perform testing\n\nattention = Attention(ModelArgs)\nx_out = attention(x_norm,start_pos=0, inference=False)\nprint(f\"x_out.shape: {x_out.shape}\")\n\"\"\"\n### Test Results: ###\n\"\"\"\nxk.shape: torch.Size([10, 256, 4, 64])\nkeys.shape: torch.Size([10, 256, 8, 64])\nx_out.shape: torch.Size([10, 256, 512])\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"EXPLICIT","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_70":{"__typename":"Paragraph","id":"7956b3655d6f_70","name":"43bf","type":"H4","href":null,"layout":null,"metadata":null,"text":"2e. FeedForward Network (SwiGLU Activation):","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_71":{"__typename":"Paragraph","id":"7956b3655d6f_71","name":"f5fe","type":"P","href":null,"layout":null,"metadata":null,"text":"What does FeedForward Network do in the decoder block? As shown in the architecture diagram above, the attention output is first normalized during RMSNorm and then fed into the FeedForward network. Inside the feedforward network, the attention output embeddings will be expanded to the higher dimension throughout its hidden layers and learn more complex features of the tokens.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_72":{"__typename":"Paragraph","id":"7956b3655d6f_72","name":"d210","type":"P","href":null,"layout":null,"metadata":null,"text":"Why use SwiGLU instead of ReLU? Let’s take a look at the diagram to get the answer.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*liwKWW5zzQCmDgJ2yOGnFw.png":{"__typename":"ImageMetadata","id":"1*liwKWW5zzQCmDgJ2yOGnFw.png","originalHeight":940,"originalWidth":972,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_73":{"__typename":"Paragraph","id":"7956b3655d6f_73","name":"37b9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*liwKWW5zzQCmDgJ2yOGnFw.png"},"text":"[Image by writer]: FeedFoward Network with SwiGLU function","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":58,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_74":{"__typename":"Paragraph","id":"7956b3655d6f_74","name":"c174","type":"P","href":null,"layout":null,"metadata":null,"text":"As shown in the diagram above, the SwiGLU function behaves almost like ReLU in the positive axis. However, in the negative axis, SwiGLU outputs some negative values, which might be useful in learning smaller rather than flat 0 in the case of ReLU. Overall, as per the author, the performance with SwiGLU has been better than that with ReLU; hence, it was chosen.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_75":{"__typename":"Paragraph","id":"7956b3655d6f_75","name":"a2ec","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s dive into FeedForward code:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_76":{"__typename":"Paragraph","id":"7956b3655d6f_76","name":"b308","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step2e: The Feedfoward Network (SwiGLU activation)\nclass FeedForward(nn.Module):\n  def __init__(self, dim:int, hidden_dim:int, multiple_of:int, ffn_dim_multiplier: Optional[float]):\n    super().__init__()\n    # Models embedding dimension\n    self.dim = dim\n\n    # We must use the hidden dimensions calculation shared by Meta which is the ideal one for this model\n    # Hidden dimension are calculated such that it is a multiple of 256.\n    hidden_dim = int(2 * hidden_dim\u002F3)\n    if ffn_dim_multiplier is not None:\n      hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) \u002F\u002F multiple_of)\n\n    # define hiddne layers weights\n    self.w1 = nn.Linear(self.dim, hidden_dim, bias=False, device=device)\n    self.w2 = nn.Linear(hidden_dim, self.dim, bias=False, device=device)\n    self.w3 = nn.Linear(self.dim, hidden_dim, bias=False, device=device)\n\n  def forward(self, x):\n    # Shape: [bsz,seq_len,dim]\n    return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\n\n### Test: FeedForward module ###\n# note: x_out is already computed at Attention testing and is being used for testing here.\n# You need take out the triple quotes below to perform testing\n\"\"\"\nfeed_forward = FeedForward(ModelArgs.dim, 4 * ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)\nx_out = rms_norm(x_out)\nx_out = feed_forward(x_out)\nprint(f\"feed forward output: x_out.shape: {x_out.shape}\")\n\"\"\"\n\n### Test Results: ###\n\"\"\"\nfeed forward output: x_out.shape: torch.Size([10, 256, 512])\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_77":{"__typename":"Paragraph","id":"7956b3655d6f_77","name":"5036","type":"H4","href":null,"layout":null,"metadata":null,"text":"2f. Decoder Block:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_78":{"__typename":"Paragraph","id":"7956b3655d6f_78","name":"fbc1","type":"P","href":null,"layout":null,"metadata":null,"text":"As shown in the architecture diagram above (the very first diagram). The decoder block consists of multiple sub-components, which we’ve learned and coded in earlier sections (2a — 2f). Below is a pointwise operation that is being carried out inside the decoder block.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_79":{"__typename":"Paragraph","id":"7956b3655d6f_79","name":"fb66","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The embedding from the input block is fed into the Attention-RMSNorm block. This will be further fed into the Group Query Attention block.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_80":{"__typename":"Paragraph","id":"7956b3655d6f_80","name":"2d4c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The same embedding from the input block will then be added to the attention output.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_81":{"__typename":"Paragraph","id":"7956b3655d6f_81","name":"ebeb","type":"OLI","href":null,"layout":null,"metadata":null,"text":"After that, the attention output is fed into FeedFoward-RMSNorm and further fed into the FeedFoward network block.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_82":{"__typename":"Paragraph","id":"7956b3655d6f_82","name":"9ad1","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The output of the FeedFoward network is then added again with the attention output.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_83":{"__typename":"Paragraph","id":"7956b3655d6f_83","name":"c8ca","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The resulting output is called Decoder Output. This decoder output is then fed into another decoder block as input. This same operation will be repeated for the next 31 decoder blocks. The final decoder output of the 32nd decoder block is then passed to the Output block.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":31,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_84":{"__typename":"Paragraph","id":"7956b3655d6f_84","name":"28e3","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s see this action in the code below:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_85":{"__typename":"Paragraph","id":"7956b3655d6f_85","name":"3a19","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step2f: The Decoder Block. The class name is assigned as TransformerBlock to match the name of Meta llama 3 code base.\n\nclass TransformerBlock(nn.Module):\n  def __init__(self, args: ModelArgs):\n    super().__init__()\n    self.args = args\n    # Initilizate RMSNorm for attention\n    self.attention_norm = RMSNorm(dim=args.dim, eps = args.norm_eps)\n    # Initilizate Attention class\n    self.attention = Attention(args)\n    # Initilizate RMSNorm for feedfoward class\n    self.ff_norm = RMSNorm(dim=args.dim, eps = args.norm_eps)\n    # Initilizate feedfoward class\n    self.feedforward = FeedForward(args.dim, 4 * args.dim, args.multiple_of, args.ffn_dim_multiplier)\n\n  def forward(self, x, start_pos, inference):\n    # start_pos = token position for inference mode, inference = True for inference and False for training mode\n    # i) pass input embedding to attention_norm and then pass to attention block.\n    # ii) the output of attention is then added to embedding(before norm)\n    h = x + self.attention(self.attention_norm(x), start_pos, inference)\n\n    # i) pass attention output to ff_norm and then pass to the feedforward network.\n    # ii) the output of feedforward network is then added to the attention output(before ff_norm)\n    out = h + self.feedforward(self.ff_norm(h))\n    # Shape: [bsz,seq_len,dim]\n    return out\n\n\n### Test: TransformerBlock ###\n# You need take out the triple quotes below to perform testing\n\"\"\"\nx = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)\ntransformer_block = TransformerBlock(ModelArgs)\ntransformer_block_out = transformer_block(x,start_pos=0, inference=False)\nprint(f\"transformer_block_out.shape: {transformer_block_out.shape}\")\n\"\"\"\n\n### Test Results: ###\n\"\"\"\ntransformer_block_out.shape: torch.Size([10, 64, 128])\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_86":{"__typename":"Paragraph","id":"7956b3655d6f_86","name":"5d1f","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 3: The Output Block","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_87":{"__typename":"Paragraph","id":"7956b3655d6f_87","name":"bc28","type":"P","href":null,"layout":null,"metadata":null,"text":"The decoder output of the final decoder block will feed into the output block. It is first fed into the RMSNorm. Then, it will feed into the Linear Layer which generates logits. Next, one of the following two operations happens.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_88":{"__typename":"Paragraph","id":"7956b3655d6f_88","name":"cf23","type":"ULI","href":null,"layout":null,"metadata":null,"text":"If the mode is inference, top_p probability is calculated and the next token is generated. The next tokens generated will stop if the max generation length is reached or the end of sentence token is generated as the next token.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":15,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_89":{"__typename":"Paragraph","id":"7956b3655d6f_89","name":"6b7a","type":"ULI","href":null,"layout":null,"metadata":null,"text":"If the mode is Training, loss is computed with the target labels and training is repeated till the max epochs length is reached.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":15,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_90":{"__typename":"Paragraph","id":"7956b3655d6f_90","name":"4d36","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s take a look at the output block flow diagram for more clarity.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*TSiiz1znMsW0EMUDq2Cx7w.png":{"__typename":"ImageMetadata","id":"1*TSiiz1znMsW0EMUDq2Cx7w.png","originalHeight":1478,"originalWidth":1280,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_91":{"__typename":"Paragraph","id":"7956b3655d6f_91","name":"8744","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*TSiiz1znMsW0EMUDq2Cx7w.png"},"text":"[Image by writer]: LLama 3 output flow diagram for training and inference mode","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":78,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_92":{"__typename":"Paragraph","id":"7956b3655d6f_92","name":"7c43","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, let’s combine all components of 3 blocks (input block, decoder block and output blocks. This gives our final Llama 3 model.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_93":{"__typename":"Paragraph","id":"7956b3655d6f_93","name":"187a","type":"P","href":null,"layout":null,"metadata":null,"text":"let’s code the final Llama 3 model:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_94":{"__typename":"Paragraph","id":"7956b3655d6f_94","name":"2569","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step3: The Output Block\n# This is the Llama 3 model. Again, the class name is maintained as Transformer to match with Meta Llama 3 model.\n\nclass Transformer(nn.Module):\n  def __init__(self, params: ModelArgs):\n    super().__init__()\n    # set all the ModelArgs in params variable\n    self.params = params\n    # Initilizate embedding class from the input block\n    self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n\n    # Initialize the decoder block and store it inside the ModuleList. \n    # This is because we've 4 decoder blocks in our Llama 3 model. (Official Llama 3 has 32 blocks)\n    self.layers = nn.ModuleList()\n    for layer_id in range(params.n_layers):\n      self.layers.append(TransformerBlock(args=params))\n\n    # Initilizate RMSNorm for the output block\n    self.norm = RMSNorm(params.dim, eps = params.norm_eps)\n    \n    # Initilizate linear layer at the output block.\n    self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n\n  def forward(self, x, start_pos=0, targets=None):\n    \n    # start_pos = token position for inference mode, inference = True for inference and False for training mode\n    # x is the batch of token_ids generated from the texts or prompts using tokenizers.\n    # x[bsz, seq_len] -\u003E h[bsz, seq_len, dim]\n    h = self.tok_embeddings(x)\n\n    # If the target is none, Inference mode is activated and set to \"True\" and \"False\" if Training mode is activated.\n    if targets is None:\n      inference = True\n    else:\n      inference = False\n\n    # The embeddings (h) will then pass though all the decoder blocks.\n    for layer in self.layers:\n      h = layer(h, start_pos, inference)\n\n    # The output from the final decoder block will feed into the RMSNorm\n    h = self.norm(h)\n\n    # After normalized, the embedding h will then feed into the Linear layer. \n    # The main task of the Linear layer is to generate logits that maps the embeddings with the vocabulary size.\n    # h[bsz, seq_len, dim] -\u003E logits[bsz, seq_len, vocab_size]\n    logits = self.output(h).float()\n    loss = None\n\n    # Inference mode is activated if the targets is not available\n    if targets is None:\n      loss = None\n    # Training mode is activated if the targets are available. And Loss will be calculated for further model training. \n    else:\n      loss = F.cross_entropy(logits.view(-1, self.params.vocab_size), targets.view(-1))\n\n    return logits, loss\n\n\n### Test: Transformer (Llama Model) ###\n# You need take out the triple quotes below to perform testing\n\"\"\"\nmodel = Transformer(ModelArgs).to(ModelArgs.device)\nprint(model)\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WG1DxQDPocze_Y0nurNdFQ.png":{"__typename":"ImageMetadata","id":"1*WG1DxQDPocze_Y0nurNdFQ.png","originalHeight":802,"originalWidth":1614,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_95":{"__typename":"Paragraph","id":"7956b3655d6f_95","name":"70e7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*WG1DxQDPocze_Y0nurNdFQ.png"},"text":"[Image by Write]: LLama 3 layered architecture","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_96":{"__typename":"Paragraph","id":"7956b3655d6f_96","name":"c46f","type":"P","href":null,"layout":null,"metadata":null,"text":"The Llama 3 model we’ve just built looks perfect. We’re now ready to start our training process.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_97":{"__typename":"Paragraph","id":"7956b3655d6f_97","name":"c441","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 4: Train our Llama 3 Model:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_98":{"__typename":"Paragraph","id":"7956b3655d6f_98","name":"4db3","type":"P","href":null,"layout":null,"metadata":null,"text":"The training flow is provided in the output block flow diagram(step 3). Please refer to that flow again if you would like to have more clarity before starting training. Let’s begin writing the training code. I’ll also provide the necessary explanation within the code block as well.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_99":{"__typename":"Paragraph","id":"7956b3655d6f_99","name":"0f96","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step 4: Train Llama 3 Model:\n\n# Create a dataset by encoding the entire tiny_shakespeare data token_ids list using the tokenizer's encode function that we've built at the input block section\ndataset = torch.tensor(encode(data), dtype=torch.int).to(ModelArgs.device)\nprint(f\"dataset-shape: {dataset.shape}\")\n\n# Define function to generate batches from the given dataset\ndef get_dataset_batch(data, split, args:ModelArgs):\n  seq_len = args.max_seq_len\n  batch_size = args.max_batch_size\n  device = args.device\n\n  train = data[:int(0.8 * len(data))]\n  val = data[int(0.8 * len(data)): int(0.9 * len(data))]\n  test = data[int(0.9 * len(data)):]\n\n  batch_data = train\n  if split == \"val\":\n    batch_data = val\n\n  if split == \"test\":\n    batch_data = test\n  \n  # Picking random starting points from the dataset to give random samples for training, validation and testing.\n  \n  ix = torch.randint(0, len(batch_data) - seq_len - 3, (batch_size,)).to(device)\n  x = torch.stack([torch.cat([token_bos, batch_data[i:i+seq_len-1]]) for i in ix]).long().to(device)\n  y = torch.stack([torch.cat([batch_data[i+1:i+seq_len], token_eos]) for i in ix]).long().to(device)\n  \n  return x,y\n\n### Test: get_dataset function ###\n\"\"\"\nxs, ys = get_dataset_batch(dataset, split=\"train\", args=ModelArgs)\nprint([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])\n\"\"\"\n\n# Define a evaluate loss function to calculate and store training and validation loss for logging and plotting\n@torch.no_grad()\ndef evaluate_loss(model, args:ModelArgs):\n  out = {}\n  model.eval()\n\n  for split in [\"train\", \"val\"]:\n    losses = []\n    for _ in range(10):      \n      xb, yb = get_dataset_batch(dataset, split, args)\n      _, loss = model(x=xb, targets=yb)\n      losses.append(loss.item())\n    out[split] = np.mean(losses)\n\n  model.train()\n  return out\n\n# Define a training function to perform model training\ndef train(model, optimizer, args:ModelArgs):\n    epochs = args.epochs\n    log_interval = args.log_interval\n    device = args.device\n    losses = []   \n    start_time = time.time()\n\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        \n        xs, ys = get_dataset_batch(dataset, 'train', args)\n        xs = xs.to(device)\n        ys = ys.to(device)\n        logits, loss = model(x=xs, targets=ys)\n        loss.backward()\n        optimizer.step()\n\n        if epoch % log_interval == 0:\n            batch_time = time.time() - start_time\n            x = evaluate_loss(model, args)\n            losses += [x]            \n            print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f}\")\n            start_time = time.time()\n    \n    # Print the final validation loss\n    print(\"validation loss: \", losses[-1]['val'])\n    # Display the interval losses in plot \n    return pd.DataFrame(losses).plot()","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_100":{"__typename":"Paragraph","id":"7956b3655d6f_100","name":"3390","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, that we’ve defined the training function. Let’s start training with the following code block and observe the training results in the plot once the training is completed.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_101":{"__typename":"Paragraph","id":"7956b3655d6f_101","name":"35b7","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Start training our Llama 3 model\nmodel = Transformer(ModelArgs).to(ModelArgs.device)\noptimizer = torch.optim.Adam(model.parameters())\n\ntrain(model, optimizer, ModelArgs)","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"makefile"},"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*JS08OxC3GYrUiisOiks1TQ.png":{"__typename":"ImageMetadata","id":"1*JS08OxC3GYrUiisOiks1TQ.png","originalHeight":1106,"originalWidth":1350,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:7956b3655d6f_102":{"__typename":"Paragraph","id":"7956b3655d6f_102","name":"eea2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*JS08OxC3GYrUiisOiks1TQ.png"},"text":"[image by writer]: Training vs Validation loss graph","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_103":{"__typename":"Paragraph","id":"7956b3655d6f_103","name":"7eca","type":"P","href":null,"layout":null,"metadata":null,"text":"The above image displays the training and validation loss graph. The training has been conducted over 2500 epochs. It took around 10 min to complete the training process using Google Colab with default GPU and RAM settings which is very fast. The validation loss at the final epoch is 2.19 which is considered okay given the amount of training data we’re using and the number of epochs. To reduce the losses significantly, we will have to increase the size of the training data, higher number of epochs and higher GPU or processing power.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_104":{"__typename":"Paragraph","id":"7956b3655d6f_104","name":"b3cf","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we’ve completed our training. Let’s head into our final step — Inference and see how well the model generates the output texts given new input prompts.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_105":{"__typename":"Paragraph","id":"7956b3655d6f_105","name":"9f04","type":"H4","href":null,"layout":null,"metadata":null,"text":"Step 5: Inference Llama 3 Model:","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_106":{"__typename":"Paragraph","id":"7956b3655d6f_106","name":"7063","type":"P","href":null,"layout":null,"metadata":null,"text":"The inference flow is provided in the output block flow diagram(step 3). Let’s begin writing the inference code.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_107":{"__typename":"Paragraph","id":"7956b3655d6f_107","name":"5037","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Step 5: Inference Llama 3 Model:\n# This function generates text sequences based on provided prompts using the LLama 3 model we've built and trained.\n\ndef generate(model, prompts: str, params: ModelArgs, max_gen_len: int=500, temperature: float = 0.6, top_p: float = 0.9):\n\n    # prompt_tokens: List of user input texts or prompts\n    # max_gen_len: Maximum length of the generated text sequence.\n    # temperature: Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    # top_p: Top-p probability threshold for sampling prob output from the logits. Defaults to 0.9.\n    # prompt_tokens = [0]\n    bsz = 1  #For inferencing, in general user just input one prompt which we'll take it as 1-batch\n    prompt_tokens = token_bos.tolist() + encode(prompts)\n    assert len(prompt_tokens) \u003C= params.max_seq_len, \"prompt token length should be small than max_seq_len\"\n    total_len = min(len(prompt_tokens)+max_gen_len, params.max_seq_len)   \n\n    # this tokens matrix is to store the input prompts and all the output that is generated by model.\n    # later we'll use the tokenizers decode function to decode this token to view results in text format\n    tokens = torch.full((bsz,total_len), fill_value=token_pad.item(), dtype=torch.long, device=params.device)\n\n    # fill in the prompt tokens into the token matrix\n    tokens[:,:len(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=params.device)\n\n    #create a prompt_mask_token for later use to identify if the token is a prompt token or a padding token\n    # True if it is a prompt token, False if it is a padding token\n    input_text_mask = tokens != token_pad.item()\n\n    #now we can start inferencing using one token at a time from the prompt_tokens list starting with the first position.\n    prev_pos = 0\n    for cur_pos in range(1, total_len):\n      with torch.no_grad():\n        logits, _ = model(x=tokens[:,prev_pos:cur_pos], start_pos=prev_pos)\n      if temperature \u003E 0:      \n        probs = torch.softmax(logits[:, -1]\u002Ftemperature, dim=-1)\n        next_token = sample_top_p(probs, top_p)        \n      else:\n        next_token = torch.argmax(logits[:, -1], dim=-1)        \n\n      next_token = next_token.reshape(-1)\n\n      # only replace the token if it's a padding token\n      next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n      tokens[:, cur_pos] = next_token\n\n      prev_pos = cur_pos\n      if tokens[:,cur_pos]==token_pad.item() and next_token == token_eos.item():\n        break\n\n    output_tokens, output_texts = [], []    \n\n    for i, toks in enumerate(tokens.tolist()):\n      # eos_idx = toks.index(token_eos.item())\n      if token_eos.item() in toks:\n        eos_idx = toks.index(token_eos.item())\n        toks = toks[:eos_idx]\n\n      output_tokens.append(toks)\n      output_texts.append(decode(toks))\n    return output_tokens, output_texts\n\n# Perform top-p (nucleus) sampling on a probability distribution.\n# probs (torch.Tensor): Probability distribution tensor derived from the logits.\n# p: Probability threshold for top-p sampling.\n# According to the paper, Top-p sampling selects the smallest set of tokens whose cumulative probability mass exceeds the threshold p. \n# The distribution is renormalized based on the selected tokens.\ndef sample_top_p(probs, p):\n    probs_sort, prob_idx = torch.sort(probs, dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = probs_sum - probs_sort \u003E p\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    next_token = torch.gather(prob_idx, -1, next_token)    \n    # Sampled token indices from the vocabular is returned \n    return next_token","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_108":{"__typename":"Paragraph","id":"7956b3655d6f_108","name":"e384","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s perform inferencing on new Prompts and check the generated output","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_109":{"__typename":"Paragraph","id":"7956b3655d6f_109","name":"0e5c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"## Perform the inferencing on user input prompts\nprompts = \"Consider you what services he has done\"\noutput_tokens, output_texts = generate(model, prompts, ModelArgs)\noutput_texts = output_texts[0].replace(\"\u003C|begin_of_text|\u003E\", \"\")\nprint(output_texts)\n\n## Output ##\n\"\"\"\nConsider you what services he has done o eretrane\nadetranytnn i eey i ade hs rcuh i eey,ad hsatsTns rpae,T\neon o i hseflns o i eee ee hs ote i ocal ersl,Bnnlnface\no i hmr a il nwye ademto nt i a ere\nh i ees.\nFrm oe o etrane o oregae,alh,t orede i oeral\n\"\"\"","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":{"__typename":"CodeBlockMetadata","mode":"AUTO","lang":"python"},"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_110":{"__typename":"Paragraph","id":"7956b3655d6f_110","name":"260a","type":"P","href":null,"layout":null,"metadata":null,"text":"And yes, we can see that our Llama 3 model is able to perform inference and generate texts on new prompts, though the output does not seem great given the amount of training data and epochs we’ve used for training. I am sure with much larger training data, we’ll achieve much better accuracy.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_111":{"__typename":"Paragraph","id":"7956b3655d6f_111","name":"0033","type":"P","href":null,"layout":null,"metadata":null,"text":"And this is it! we have successfully built our own Llama 3 model from scratch. We’ve also successfully trained the model and managed to perform inferencing to generate new texts within a very short amount of time using Google Colab Notebook with given free GPU and RAM. If you have followed along so far, I would personally congratulate you for the great effort you’ve put in.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_112":{"__typename":"Paragraph","id":"7956b3655d6f_112","name":"c75f","type":"P","href":null,"layout":null,"metadata":null,"text":"My final thoughts","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_113":{"__typename":"Paragraph","id":"7956b3655d6f_113","name":"e91c","type":"P","href":null,"layout":null,"metadata":null,"text":"Llama 3 and its other variances are the most popular open-source LLM currently available in the LLM space. I believe the ability to build Llama 3 from scratch provides all the necessary foundation to build a lot of new exciting LLM-based applications. I truly believe that knowledge should be free to all. Feel free to use the source code and update it to build your personal or professional projects. Good luck to you all.","hasDropCap":null,"dropCapImage":null,"markups":[],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_114":{"__typename":"Paragraph","id":"7956b3655d6f_114","name":"c41f","type":"P","href":null,"layout":null,"metadata":null,"text":"Thanks a lot for reading!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_115":{"__typename":"Paragraph","id":"7956b3655d6f_115","name":"9d8d","type":"P","href":null,"layout":null,"metadata":null,"text":"Link to Google Colab notebook","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":29,"href":"https:\u002F\u002Fgithub.com\u002Ftamangmilan\u002Fllama3\u002Fblob\u002Fmain\u002Fbuild_llama3_from_scratch.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_116":{"__typename":"Paragraph","id":"7956b3655d6f_116","name":"dea4","type":"P","href":null,"layout":null,"metadata":null,"text":"References","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"Paragraph:7956b3655d6f_117":{"__typename":"Paragraph","id":"7956b3655d6f_117","name":"0d54","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Meta Llama3 Github: https:\u002F\u002Fgithub.com\u002Fmeta-llama\u002Fllama3","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":20,"end":56,"href":"https:\u002F\u002Fgithub.com\u002Fmeta-llama\u002Fllama3","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":19,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"codeBlockMetadata":null,"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:98111c9905da-viewerId:a67723e842b6":{"__typename":"CollectionViewerEdge","id":"collectionId:98111c9905da-viewerId:a67723e842b6","isEditor":false,"isMuting":false,"canEditOwnPosts":false,"canEditPosts":false,"isFollowing":false,"isSubscribedToLetters":false,"isSubscribedToMediumNewsletter":false,"isSubscribedToEmails":false,"isWriter":false},"UserViewerEdge:userId:141fa70b60b6-viewerId:a67723e842b6":{"__typename":"UserViewerEdge","id":"userId:141fa70b60b6-viewerId:a67723e842b6","isMuting":false},"PostViewerEdge:postId:2ce1ecaa901c-viewerId:a67723e842b6":{"__typename":"PostViewerEdge","shouldIndexPostForExternalSearch":true,"id":"postId:2ce1ecaa901c-viewerId:a67723e842b6"},"Tag:artificial-intelligence":{"__typename":"Tag","id":"artificial-intelligence","displayTitle":"Artificial Intelligence","normalizedTagSlug":"artificial-intelligence"},"Tag:machine-learning":{"__typename":"Tag","id":"machine-learning","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:large-language-models":{"__typename":"Tag","id":"large-language-models","displayTitle":"Large Language Models","normalizedTagSlug":"large-language-models"},"Tag:deep-learning":{"__typename":"Tag","id":"deep-learning","displayTitle":"Deep Learning","normalizedTagSlug":"deep-learning"},"Tag:data-science":{"__typename":"Tag","id":"data-science","displayTitle":"Data Science","normalizedTagSlug":"data-science"},"Post:2ce1ecaa901c":{"__typename":"Post","id":"2ce1ecaa901c","collection":{"__ref":"Collection:98111c9905da"},"content({\"postMeteringOptions\":{\"referrer\":\"\"}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"8c1a","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:7956b3655d6f_0"},{"__ref":"Paragraph:7956b3655d6f_1"},{"__ref":"Paragraph:7956b3655d6f_2"},{"__ref":"Paragraph:7956b3655d6f_3"},{"__ref":"Paragraph:7956b3655d6f_4"},{"__ref":"Paragraph:7956b3655d6f_5"},{"__ref":"Paragraph:7956b3655d6f_6"},{"__ref":"Paragraph:7956b3655d6f_7"},{"__ref":"Paragraph:7956b3655d6f_8"},{"__ref":"Paragraph:7956b3655d6f_9"},{"__ref":"Paragraph:7956b3655d6f_10"},{"__ref":"Paragraph:7956b3655d6f_11"},{"__ref":"Paragraph:7956b3655d6f_12"},{"__ref":"Paragraph:7956b3655d6f_13"},{"__ref":"Paragraph:7956b3655d6f_14"},{"__ref":"Paragraph:7956b3655d6f_15"},{"__ref":"Paragraph:7956b3655d6f_16"},{"__ref":"Paragraph:7956b3655d6f_17"},{"__ref":"Paragraph:7956b3655d6f_18"},{"__ref":"Paragraph:7956b3655d6f_19"},{"__ref":"Paragraph:7956b3655d6f_20"},{"__ref":"Paragraph:7956b3655d6f_21"},{"__ref":"Paragraph:7956b3655d6f_22"},{"__ref":"Paragraph:7956b3655d6f_23"},{"__ref":"Paragraph:7956b3655d6f_24"},{"__ref":"Paragraph:7956b3655d6f_25"},{"__ref":"Paragraph:7956b3655d6f_26"},{"__ref":"Paragraph:7956b3655d6f_27"},{"__ref":"Paragraph:7956b3655d6f_28"},{"__ref":"Paragraph:7956b3655d6f_29"},{"__ref":"Paragraph:7956b3655d6f_30"},{"__ref":"Paragraph:7956b3655d6f_31"},{"__ref":"Paragraph:7956b3655d6f_32"},{"__ref":"Paragraph:7956b3655d6f_33"},{"__ref":"Paragraph:7956b3655d6f_34"},{"__ref":"Paragraph:7956b3655d6f_35"},{"__ref":"Paragraph:7956b3655d6f_36"},{"__ref":"Paragraph:7956b3655d6f_37"},{"__ref":"Paragraph:7956b3655d6f_38"},{"__ref":"Paragraph:7956b3655d6f_39"},{"__ref":"Paragraph:7956b3655d6f_40"},{"__ref":"Paragraph:7956b3655d6f_41"},{"__ref":"Paragraph:7956b3655d6f_42"},{"__ref":"Paragraph:7956b3655d6f_43"},{"__ref":"Paragraph:7956b3655d6f_44"},{"__ref":"Paragraph:7956b3655d6f_45"},{"__ref":"Paragraph:7956b3655d6f_46"},{"__ref":"Paragraph:7956b3655d6f_47"},{"__ref":"Paragraph:7956b3655d6f_48"},{"__ref":"Paragraph:7956b3655d6f_49"},{"__ref":"Paragraph:7956b3655d6f_50"},{"__ref":"Paragraph:7956b3655d6f_51"},{"__ref":"Paragraph:7956b3655d6f_52"},{"__ref":"Paragraph:7956b3655d6f_53"},{"__ref":"Paragraph:7956b3655d6f_54"},{"__ref":"Paragraph:7956b3655d6f_55"},{"__ref":"Paragraph:7956b3655d6f_56"},{"__ref":"Paragraph:7956b3655d6f_57"},{"__ref":"Paragraph:7956b3655d6f_58"},{"__ref":"Paragraph:7956b3655d6f_59"},{"__ref":"Paragraph:7956b3655d6f_60"},{"__ref":"Paragraph:7956b3655d6f_61"},{"__ref":"Paragraph:7956b3655d6f_62"},{"__ref":"Paragraph:7956b3655d6f_63"},{"__ref":"Paragraph:7956b3655d6f_64"},{"__ref":"Paragraph:7956b3655d6f_65"},{"__ref":"Paragraph:7956b3655d6f_66"},{"__ref":"Paragraph:7956b3655d6f_67"},{"__ref":"Paragraph:7956b3655d6f_68"},{"__ref":"Paragraph:7956b3655d6f_69"},{"__ref":"Paragraph:7956b3655d6f_70"},{"__ref":"Paragraph:7956b3655d6f_71"},{"__ref":"Paragraph:7956b3655d6f_72"},{"__ref":"Paragraph:7956b3655d6f_73"},{"__ref":"Paragraph:7956b3655d6f_74"},{"__ref":"Paragraph:7956b3655d6f_75"},{"__ref":"Paragraph:7956b3655d6f_76"},{"__ref":"Paragraph:7956b3655d6f_77"},{"__ref":"Paragraph:7956b3655d6f_78"},{"__ref":"Paragraph:7956b3655d6f_79"},{"__ref":"Paragraph:7956b3655d6f_80"},{"__ref":"Paragraph:7956b3655d6f_81"},{"__ref":"Paragraph:7956b3655d6f_82"},{"__ref":"Paragraph:7956b3655d6f_83"},{"__ref":"Paragraph:7956b3655d6f_84"},{"__ref":"Paragraph:7956b3655d6f_85"},{"__ref":"Paragraph:7956b3655d6f_86"},{"__ref":"Paragraph:7956b3655d6f_87"},{"__ref":"Paragraph:7956b3655d6f_88"},{"__ref":"Paragraph:7956b3655d6f_89"},{"__ref":"Paragraph:7956b3655d6f_90"},{"__ref":"Paragraph:7956b3655d6f_91"},{"__ref":"Paragraph:7956b3655d6f_92"},{"__ref":"Paragraph:7956b3655d6f_93"},{"__ref":"Paragraph:7956b3655d6f_94"},{"__ref":"Paragraph:7956b3655d6f_95"},{"__ref":"Paragraph:7956b3655d6f_96"},{"__ref":"Paragraph:7956b3655d6f_97"},{"__ref":"Paragraph:7956b3655d6f_98"},{"__ref":"Paragraph:7956b3655d6f_99"},{"__ref":"Paragraph:7956b3655d6f_100"},{"__ref":"Paragraph:7956b3655d6f_101"},{"__ref":"Paragraph:7956b3655d6f_102"},{"__ref":"Paragraph:7956b3655d6f_103"},{"__ref":"Paragraph:7956b3655d6f_104"},{"__ref":"Paragraph:7956b3655d6f_105"},{"__ref":"Paragraph:7956b3655d6f_106"},{"__ref":"Paragraph:7956b3655d6f_107"},{"__ref":"Paragraph:7956b3655d6f_108"},{"__ref":"Paragraph:7956b3655d6f_109"},{"__ref":"Paragraph:7956b3655d6f_110"},{"__ref":"Paragraph:7956b3655d6f_111"},{"__ref":"Paragraph:7956b3655d6f_112"},{"__ref":"Paragraph:7956b3655d6f_113"},{"__ref":"Paragraph:7956b3655d6f_114"},{"__ref":"Paragraph:7956b3655d6f_115"},{"__ref":"Paragraph:7956b3655d6f_116"},{"__ref":"Paragraph:7956b3655d6f_117"}]},"validatedShareKey":"","shareKeyCreator":null},"creator":{"__ref":"User:141fa70b60b6"},"inResponseToEntityType":null,"isLocked":false,"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fpub.towardsai.net\u002Fbuild-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c","primaryTopic":null,"topics":[],"isLimitedState":false,"isPublished":true,"allowResponses":true,"responsesLocked":false,"visibility":"PUBLIC","latestPublishedVersion":"7956b3655d6f","postResponses":{"__typename":"PostResponses","count":4},"responseDistribution":"NOT_DISTRIBUTED","clapCount":1041,"title":"Build Your Own Llama 3 Architecture from Scratch Using PyTorch","isSeries":false,"sequence":null,"uniqueSlug":"build-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c","socialTitle":"","socialDek":"","canonicalUrl":"https:\u002F\u002Fpub.towardsai.net\u002Fbuild-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c","metaDescription":"","latestPublishedAt":1725809642488,"readingTime":25.156603773584905,"previewContent":{"__typename":"PreviewContent","subtitle":"A step-by-step guide to building the complete architecture of the Llama 3 model from scratch and performing training and inferencing on a…"},"previewImage":{"__ref":"ImageMetadata:1*_xNP7aBpcmcMk4tXJ-Z8Mw.png"},"isShortform":false,"seoMetaTags":{"__typename":"SEOMetaTags","jsonLd":"{\"@context\":\"https:\u002F\u002Fschema.org\",\"@type\":\"SocialMediaPosting\",\"image\":[\"https:\u002F\u002Fmiro.medium.com\u002F\"],\"url\":\"https:\u002F\u002Fpub.towardsai.net\u002Fbuild-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c\",\"dateCreated\":\"2024-09-01T09:13:59Z\",\"datePublished\":\"2024-09-01T09:13:59Z\",\"dateModified\":\"2024-09-08T15:34:02Z\",\"headline\":\"Build Your Own Llama 3 Architecture from Scratch Using PyTorch\",\"name\":\"Build Your Own Llama 3 Architecture from Scratch Using PyTorch\",\"description\":\"“” is published by Milan Tamang in Towards AI.\",\"identifier\":\"2ce1ecaa901c\",\"author\":{\"@type\":\"Person\",\"name\":\"Milan Tamang\",\"url\":\"https:\u002F\u002Fmedium.com\u002F@tamangmilan\"},\"creator\":[\"Milan Tamang\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Towards AI\",\"url\":\"https:\u002F\u002Fpub.towardsai.net\",\"logo\":{\"@type\":\"ImageObject\",\"width\":777,\"height\":777,\"url\":\"https:\u002F\u002Fmiro.medium.com\u002Fv2\u002Fresize:fit:777\u002F1*JyIThO-cLjlChQLb6kSlVQ.png\"}},\"mainEntityOfPage\":\"https:\u002F\u002Fpub.towardsai.net\u002Fbuild-your-own-llama-3-architecture-from-scratch-using-pytorch-2ce1ecaa901c\",\"isAccessibleForFree\":true}"},"seoDescription":"","shortformType":"SHORTFORM_TYPE_LINK","firstPublishedAt":1725182039896,"viewerEdge":{"__ref":"PostViewerEdge:postId:2ce1ecaa901c-viewerId:a67723e842b6"},"seoTitle":"","isSuspended":false,"license":"ALL_RIGHTS_RESERVED","tags":[{"__ref":"Tag:artificial-intelligence"},{"__ref":"Tag:machine-learning"},{"__ref":"Tag:large-language-models"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:data-science"}],"isFeaturedInPublishedPublication":false,"isNewsletter":false,"statusForCollection":"APPROVED","pendingCollection":null,"detectedLanguage":"en","wordCount":6322,"layerCake":6}}</script><script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/manifest.8100777f.js"></script><script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/723.093de8f1.js"></script><script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/main.1774c3fe.js"></script><script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/instrumentation.47ae8b31.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/reporting.851fdaca.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/5052.eb638269.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/628.add32a62.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/4237.091bc151.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/6840.4f082ee3.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3326.8a80e879.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/7566.fa51707d.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/7908.908acb8a.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3927.2f9f3eed.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/8640.0d3bced2.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/9967.f31ca2af.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/6372.c6e20496.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/5429.931b7269.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/7381.c53435ab.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/5522.e9951c29.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/5660.6336e947.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/4929.d5148d55.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/6834.6c66e3cc.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1887.e955bdb2.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/7979.35c5b2af.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/7975.3f8d607c.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3877.96683729.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/9256.629cdc7e.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/8768.71979c5d.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/144.76c298be.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3666.6579eeda.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/1069.6236ad3b.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3523.7017067b.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3960.16e44094.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/PostPage.MainContent.a239456b.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/2698.9eecb474.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/3974.ee0dd7bf.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/2527.5b775609.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/PostResponsesContent.e940a3b4.chunk.js"></script>
<script src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/responses.editor.d61aa7c4.chunk.js"></script>
<script id="__LOADABLE_REQUIRED_CHUNKS__" type="application/json">[]</script>
<script id="__LOADABLE_REQUIRED_CHUNKS___ext" type="application/json">{"namedChunks":[]}</script><script>window.main();</script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9710a5f79b627f51',t:'MTc1NTUxMjEwOS4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script><iframe height="1" width="1" style="position: absolute; top: 0px; left: 0px; border: none; visibility: hidden;" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/saved_resource(1).html"></iframe><script defer="" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon="{&quot;rayId&quot;:&quot;9710a5f79b627f51&quot;,&quot;serverTiming&quot;:{&quot;name&quot;:{&quot;cfExtPri&quot;:true,&quot;cfEdge&quot;:true,&quot;cfOrigin&quot;:true,&quot;cfL4&quot;:true,&quot;cfSpeedBrain&quot;:true,&quot;cfCacheStatus&quot;:true}},&quot;version&quot;:&quot;2025.8.0&quot;,&quot;token&quot;:&quot;0b5f665943484354a59c39c6833f7078&quot;}" crossorigin="anonymous"></script>
<div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe title="reCAPTCHA" width="256" height="60" role="presentation" name="a-x3izt8pyuip" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/anchor.html"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;" src="./Build Your Own Llama 3 Architecture from Scratch Using PyTorch _ by Milan Tamang _ Towards AI_files/saved_resource(2).html"></iframe></div></body></html>
<!DOCTYPE html>
<!-- saved from url=(0125)https://readmedium.com/en/https:/medium.com/@yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe -->
<html lang="en" data-theme="dark" class="dark"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover"><link rel="stylesheet" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/68907a8c5fb73615.css" data-precedence="next"><link rel="stylesheet" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/b89dd6028ec10fe0.css" data-precedence="next"><link rel="preload" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/webpack-201b96c0da5002a8.js" as="script" fetchpriority="low"><script async="" src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/clarity.js"></script><script async="" src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/k14ha83exi"></script><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/fd9d1056-b59a6f32ac74cecf.js" async=""></script><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/864-fc79a795ba0d747c.js" async=""></script><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/main-app-d37764a0d4c9aeed.js" async=""></script><link rel="preload" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/js" as="script"><link rel="icon" href="https://readmedium.com/favicon.ico" sizes="32x32"><link rel="icon" href="https://readmedium.com/favicon.svg" type="image/svg+xml"><title>Decoder-Only Transformers Explained: The Engine Behind LLMs</title><meta name="description" content="Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of…"><link rel="canonical" href="https://readmedium.com/en/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe"><meta property="og:title" content="Decoder-Only Transformers Explained: The Engine Behind LLMs"><meta property="og:description" content="Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of…"><meta property="og:url" content="https://readmedium.com/en/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:creator" content="@readmedium"><meta name="twitter:title" content="Decoder-Only Transformers Explained: The Engine Behind LLMs"><meta name="twitter:description" content="Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of…"><script>(self.__next_s=self.__next_s||[]).push([0,{"children":"\n  (function () {\n    function getImplicitPreference() {\n      var mediaQuery = '(prefers-color-scheme: dark)'\n      var mql = window.matchMedia(mediaQuery)\n      var hasImplicitPreference = typeof mql.matches === 'boolean'\n\n      if (hasImplicitPreference) {\n        return mql.matches ? 'dark' : 'light'\n      }\n\n      return null\n    }\n\n    function themeIsValid(theme) {\n      return theme === 'light' || theme === 'dark'\n    }\n\n    var themeToSet = 'light'\n    var preference = window.localStorage.getItem('payload-theme')\n\n    if (themeIsValid(preference)) {\n      themeToSet = preference\n    } else {\n      var implicitPreference = getImplicitPreference()\n\n      if (implicitPreference) {\n        themeToSet = implicitPreference\n      }\n    }\n\n    // payload 主题控制\n    document.documentElement.setAttribute('data-theme', themeToSet)\n    // tailwind主题控制\n    if(themeToSet === 'dark'){\n      document.documentElement.classList.add('dark')\n    }\n  })();\n  "}])</script><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/polyfills-c67a75d1b6f99dc8.js" nomodule=""></script><link type="text/css" rel="stylesheet" id="dark-mode-custom-link"><link type="text/css" rel="stylesheet" id="dark-mode-general-link"><style lang="en" type="text/css" id="dark-mode-custom-style"></style><style lang="en" type="text/css" id="dark-mode-native-style"></style><style lang="en" type="text/css" id="dark-mode-native-sheet"></style><script>
  (function () {
    function getImplicitPreference() {
      var mediaQuery = '(prefers-color-scheme: dark)'
      var mql = window.matchMedia(mediaQuery)
      var hasImplicitPreference = typeof mql.matches === 'boolean'

      if (hasImplicitPreference) {
        return mql.matches ? 'dark' : 'light'
      }

      return null
    }

    function themeIsValid(theme) {
      return theme === 'light' || theme === 'dark'
    }

    var themeToSet = 'light'
    var preference = window.localStorage.getItem('payload-theme')

    if (themeIsValid(preference)) {
      themeToSet = preference
    } else {
      var implicitPreference = getImplicitPreference()

      if (implicitPreference) {
        themeToSet = implicitPreference
      }
    }

    // payload 主题控制
    document.documentElement.setAttribute('data-theme', themeToSet)
    // tailwind主题控制
    if(themeToSet === 'dark'){
      document.documentElement.classList.add('dark')
    }
  })();
  </script><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/core.pt5ow5lr.js" type="module"></script><style type="text/css">.a2a_hide{display:none}.a2a_logo_color{background-color:#0166ff}.a2a_menu,.a2a_menu *{-moz-box-sizing:content-box;-webkit-box-sizing:content-box;box-sizing:content-box;float:none;margin:0;padding:0;position:static;height:auto;width:auto}.a2a_menu{border-radius:6px;display:none;direction:ltr;background:#FFF;font:16px sans-serif-light,HelveticaNeue-Light,"Helvetica Neue Light","Helvetica Neue",Arial,Helvetica,"Liberation Sans",sans-serif;color:#000;line-height:12px;border:1px solid #CCC;vertical-align:baseline;overflow:hidden}.a2a_mini{min-width:200px;position:absolute;width:300px;z-index:9999997}.a2a_overlay{display:none;background:#616c7deb;backdrop-filter:blur(4px);-webkit-backdrop-filter:blur(4px);position:fixed;top:0;right:0;left:0;bottom:0;z-index:9999998;-webkit-tap-highlight-color:transparent;transition:opacity .14s,backdrop-filter .14s}.a2a_full{background:#FFF;border:1px solid #FFF;box-shadow:#2a2a2a1a 0 0 20px 10px;height:auto;height:calc(320px);top:15%;left:50%;margin-left:-320px;position:fixed;text-align:center;width:640px;z-index:9999999;transition:transform .14s,opacity .14s}.a2a_full_footer,.a2a_full_header,.a2a_full_services{border:0;margin:0;padding:12px;box-sizing:border-box}.a2a_full_header{padding-bottom:8px}.a2a_full_services{height:280px;overflow-y:scroll;padding:0 12px;-webkit-overflow-scrolling:touch}.a2a_full_services .a2a_i{display:inline-block;float:none;width:181px;width:calc(33.334% - 18px)}div.a2a_full_footer{font-size:12px;text-align:center;padding:8px 14px}div.a2a_full_footer a,div.a2a_full_footer a:visited{display:inline;font-size:12px;line-height:14px;padding:8px 14px}div.a2a_full_footer a:focus,div.a2a_full_footer a:hover{background:0 0;border:0;color:#0166FF}div.a2a_full_footer a span.a2a_s_a2a,div.a2a_full_footer a span.a2a_w_a2a{background-size:14px;border-radius:3px;display:inline-block;height:14px;line-height:14px;margin:0 3px 0 0;vertical-align:top;width:14px}.a2a_modal{height:0;left:50%;margin-left:-320px;position:fixed;text-align:center;top:15%;width:640px;z-index:9999999;transition:transform .14s,opacity .14s;-webkit-tap-highlight-color:transparent}.a2a_modal_body{background:0 0;border:0;font:24px sans-serif-light,HelveticaNeue-Light,"Helvetica Neue Light","Helvetica Neue",Arial,Helvetica,"Liberation Sans",sans-serif;position:relative;height:auto;width:auto}.a2a_thanks{color:#fff;height:auto;margin-top:20px;width:auto}.a2a_thanks>div:first-child{margin:0 0 40px 0}.a2a_thanks div *{height:inherit}#a2a_copy_link{background:#FFF;border:1px solid #FFF;cursor:pointer;margin-top:15%}label.a2a_s_link#a2a_copy_link_icon,label.a2a_w_link#a2a_copy_link_icon{background-size:48px;border-radius:0;display:inline-block;height:48px;left:0;line-height:48px;margin:0 3px 0 0;position:absolute;vertical-align:top;width:48px}#a2a_modal input#a2a_copy_link_text{background-color:transparent;border:0;color:#2A2A2A;cursor:pointer;font:inherit;height:48px;left:62px;max-width:initial;min-height:auto;padding:0;position:relative;width:564px;width:calc(100% - 76px)}#a2a_copy_link_copied{background-color:#0166ff;color:#fff;display:none;font:inherit;font-size:16px;margin-top:1px;padding:3px 8px}@media (forced-colors:active){.a2a_color_buttons a,.a2a_svg{forced-color-adjust:none}}@media (prefers-color-scheme:dark){.a2a_menu a,.a2a_menu a.a2a_i,.a2a_menu a.a2a_i:visited,.a2a_menu a.a2a_more,i.a2a_i{border-color:#2a2a2a!important;color:#fff!important}.a2a_menu a.a2a_i:active,.a2a_menu a.a2a_i:focus,.a2a_menu a.a2a_i:hover,.a2a_menu a.a2a_more:active,.a2a_menu a.a2a_more:focus,.a2a_menu a.a2a_more:hover,.a2a_menu_find_container{border-color:#444!important;background-color:#444!important}.a2a_menu:not(.a2a_thanks){background-color:#2a2a2a;border-color:#2a2a2a}.a2a_menu_find{color:#fff!important}.a2a_menu label.a2a_s_find svg{background-color:transparent!important}.a2a_menu label.a2a_s_find svg path{fill:#fff!important}.a2a_full{box-shadow:#00000066 0 0 20px 10px}.a2a_overlay{background-color:#373737eb}}@media print{.a2a_floating_style,.a2a_menu,.a2a_overlay{visibility:hidden}}@keyframes a2aFadeIn{from{opacity:0}to{opacity:1}}.a2a_starting{opacity:0}.a2a_starting.a2a_full,.a2a_starting.a2a_modal{transform:scale(.8)}@media (max-width:639px){.a2a_full{border-radius:0;top:15%;left:0;margin-left:auto;width:100%}.a2a_modal{left:0;margin-left:10px;width:calc(100% - 20px)}}@media (min-width:318px) and (max-width:437px){.a2a_full .a2a_full_services .a2a_i{width:calc(50% - 18px)}}@media (max-width:317px){.a2a_full .a2a_full_services .a2a_i{width:calc(100% - 18px)}}@media (max-height:436px){.a2a_full{bottom:40px;height:auto;top:40px}}@media (max-height:550px){.a2a_modal{top:30px}}@media (max-height:360px){.a2a_modal{top:20px}.a2a_thanks>div:first-child{margin-bottom:20px}}.a2a_menu a{color:#0166FF;text-decoration:none;font:16px sans-serif-light,HelveticaNeue-Light,"Helvetica Neue Light","Helvetica Neue",Arial,Helvetica,"Liberation Sans",sans-serif;line-height:14px;height:auto;width:auto;outline:0}.a2a_menu a.a2a_i:visited,.a2a_menu a.a2a_more{color:#0166FF}.a2a_menu a.a2a_i:active,.a2a_menu a.a2a_i:focus,.a2a_menu a.a2a_i:hover,.a2a_menu a.a2a_more:active,.a2a_menu a.a2a_more:focus,.a2a_menu a.a2a_more:hover{color:#2A2A2A;border-color:#EEE;border-style:solid;background-color:#EEE;text-decoration:none}.a2a_menu label.a2a_s_find{background-size:24px;height:24px;left:8px;pointer-events:auto;position:absolute;top:7px;width:24px}.a2a_menu label.a2a_s_find svg{background-color:#FFF}.a2a_menu label.a2a_s_find svg path{fill:#CCC}#a2a_menu_container{display:inline-block}.a2a_menu_find_container{border:1px solid #CCC;border-radius:6px;padding:2px 24px 2px 0;position:relative;text-align:left}.a2a_cols_container .a2a_col1{overflow-x:hidden;overflow-y:auto;-webkit-overflow-scrolling:touch}#a2a_modal input,#a2a_modal input[type=text],.a2a_menu input,.a2a_menu input[type=text]{display:block;background-image:none;box-shadow:none;line-height:100%;margin:0;outline:0;overflow:hidden;padding:0;-moz-box-shadow:none;-webkit-box-shadow:none;-webkit-appearance:none}#a2afeed_find_container input,#a2afeed_find_container input[type=text],#a2apage_find_container input,#a2apage_find_container input[type=text]{background-color:transparent;border:0;box-sizing:content-box;color:#2A2A2A;float:none;font:inherit;font-size:16px;height:28px;line-height:20px;left:38px;outline:0;margin:0;max-width:initial;min-height:initial;padding:2px 0;position:relative;width:99%}.a2a_clear{clear:both}.a2a_svg{background-repeat:no-repeat;display:block;overflow:hidden;height:32px;line-height:32px;padding:0;pointer-events:none;width:32px}.a2a_svg svg{background-repeat:no-repeat;background-position:50% 50%;border:none;display:block;left:0;margin:0 auto;overflow:hidden;padding:0;position:relative;top:0;width:auto;height:auto}a.a2a_i,i.a2a_i{display:block;float:left;border:1px solid #FFF;line-height:24px;padding:6px 8px;text-align:left;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;width:132px}a.a2a_i span,a.a2a_more span{display:inline-block;overflow:hidden;vertical-align:top}a.a2a_i .a2a_svg{margin:0 6px 0 0}a.a2a_i .a2a_svg,a.a2a_more .a2a_svg{background-size:24px;height:24px;line-height:24px;width:24px}a.a2a_sss:hover{border-left:1px solid #CCC}a.a2a_more{border-bottom:1px solid #FFF;border-left:0;border-right:0;line-height:24px;margin:6px 0 0;padding:6px;-webkit-touch-callout:none}a.a2a_more span{height:24px;margin:0 6px 0 0}.a2a_kit .a2a_svg{background-repeat:repeat}.a2a_default_style a:empty,.a2a_flex_style a:empty,.a2a_floating_style a:empty,.a2a_overlay_style a:empty{display:none}.a2a_color_buttons a,.a2a_floating_style a{text-decoration:none}.a2a_default_style:not(.a2a_flex_style) a{float:left;line-height:16px;padding:0 2px}.a2a_default_style a:hover .a2a_svg,.a2a_floating_style a:hover .a2a_svg,.a2a_overlay_style a:hover .a2a_svg svg{opacity:.7}.a2a_overlay_style.a2a_default_style a:hover .a2a_svg{opacity:1}.a2a_default_style .a2a_count,.a2a_default_style .a2a_svg,.a2a_floating_style .a2a_svg,.a2a_menu .a2a_svg,.a2a_vertical_style .a2a_count,.a2a_vertical_style .a2a_svg{border-radius:4px}.a2a_default_style .a2a_counter img,.a2a_default_style .a2a_dd,.a2a_default_style .a2a_svg{float:left}.a2a_default_style .a2a_img_text{margin-right:4px}.a2a_default_style .a2a_divider{border-left:1px solid #000;display:inline;float:left;height:16px;line-height:16px;margin:0 5px}.a2a_kit a{cursor:pointer;transition:none}.a2a_floating_style{background-color:#fff;border-radius:6px;position:fixed;z-index:9999995}.a2a_overlay_style{z-index:2147483647}.a2a_floating_style,.a2a_overlay_style{animation:a2aFadeIn .2s ease-in;padding:4px}.a2a_vertical_style:not(.a2a_flex_style) a{clear:left;display:block;overflow:hidden;padding:4px}.a2a_floating_style.a2a_default_style{bottom:0}.a2a_floating_style.a2a_default_style a,.a2a_overlay_style.a2a_default_style a{padding:4px}.a2a_count{background-color:#fff;border:1px solid #ccc;box-sizing:border-box;color:#2a2a2a;display:block;float:left;font:12px Arial,Helvetica,sans-serif;height:16px;margin-left:4px;position:relative;text-align:center;width:50px}.a2a_count:after,.a2a_count:before{border:solid transparent;border-width:4px 4px 4px 0;content:"";height:0;left:0;line-height:0;margin:-4px 0 0 -4px;position:absolute;top:50%;width:0}.a2a_count:before{border-right-color:#ccc}.a2a_count:after{border-right-color:#fff;margin-left:-3px}.a2a_count span{animation:a2aFadeIn .14s ease-in}.a2a_vertical_style .a2a_counter img{display:block}.a2a_vertical_style .a2a_count{float:none;margin-left:0;margin-top:6px}.a2a_vertical_style .a2a_count:after,.a2a_vertical_style .a2a_count:before{border:solid transparent;border-width:0 4px 4px 4px;content:"";height:0;left:50%;line-height:0;margin:-4px 0 0 -4px;position:absolute;top:0;width:0}.a2a_vertical_style .a2a_count:before{border-bottom-color:#ccc}.a2a_vertical_style .a2a_count:after{border-bottom-color:#fff;margin-top:-3px}.a2a_color_buttons .a2a_count,.a2a_color_buttons .a2a_count:after,.a2a_color_buttons .a2a_count:before,.a2a_color_buttons.a2a_vertical_style .a2a_count:after,.a2a_color_buttons.a2a_vertical_style .a2a_count:before{background-color:transparent;border:none;color:#fff;float:none;width:auto}.a2a_color_buttons.a2a_vertical_style .a2a_count{margin-top:0}.a2a_flex_style{display:flex;align-items:flex-start;gap:0}.a2a_default_style.a2a_flex_style{left:0;right:0;width:100%}.a2a_vertical_style.a2a_flex_style{flex-direction:column;top:0;bottom:0}.a2a_flex_style a{display:flex;justify-content:center;flex:1;padding:4px}.a2a_flex_style.a2a_vertical_style a{flex-direction:column}.a2a_floating_style.a2a_color_buttons,.a2a_floating_style.a2a_flex_style{border-radius:0;padding:0}.a2a_floating_style.a2a_default_style.a2a_flex_style{bottom:0}.a2a_kit.a2a_flex_style .a2a_counter img,.a2a_kit.a2a_flex_style .a2a_dd,.a2a_kit.a2a_flex_style .a2a_svg{float:none}.a2a_nowrap{white-space:nowrap}.a2a_note{margin:0 auto;padding:9px;font-size:12px;text-align:center}.a2a_note .a2a_note_note{margin:0;color:#2A2A2A}.a2a_wide a{display:block;margin-top:3px;border-top:1px solid #EEE;text-align:center}.a2a_label{position:absolute!important;clip-path:polygon(0px 0px,0px 0px,0px 0px);-webkit-clip-path:polygon(0px 0px,0px 0px,0px 0px);overflow:hidden;height:1px;width:1px}.a2a_kit,.a2a_menu,.a2a_modal,.a2a_overlay{-ms-touch-action:manipulation;touch-action:manipulation;outline:0}.a2a_dd{-webkit-user-drag:none}.a2a_dd img{border:0}.a2a_button_facebook_like iframe{max-width:none}</style></head><body><!--$--><nav class="fixed z-10 h-[54px] w-screen flex flex-grow-0 items-center justify-between bg-white dark:bg-black px-4 shadow-md dark:shadow-neutral-900"><a class="scale-90 sm:scale-100 cursor-pointer flex items-center flex-shrink-0 text-white hover:scale-105 transform transition" href="https://readmedium.com/"><img alt="Read Medium logo" loading="lazy" width="141" height="37" decoding="async" data-nimg="1" class="mt-2 mb-2 h-6 w-auto sm:px-3" style="color:transparent" srcset="/_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=256&amp;q=75 1x, /_next/image?url=%2Fimages%2Flogo%2Flogo-black.webp&amp;w=384&amp;q=75 2x" src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/logo-black.jpeg"></a><div class="px-3 flex justify-center items-center"><div class="h-8 text-black sm:h-9 relative flex items-center justify-center w-full sm:w-60 bg-gray-100 dark:bg-gray-900 rounded-[16px] border-[none]"><div class="px-1 sm:px-3"><svg class="text-gray-300" width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.1 11.06a6.95 6.95 0 1 1 13.9 0 6.95 6.95 0 0 1-13.9 0zm6.94-8.05a8.05 8.05 0 1 0 5.13 14.26l3.75 3.75a.56.56 0 1 0 .8-.79l-3.74-3.73A8.05 8.05 0 0 0 11.04 3v.01z" fill="currentColor"></path></svg></div><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" data-testid="headerSearchInput" class="w-full leading-5 [font-family:sohne,&quot;Helvetica_Neue&quot;,Helvetica,Arial,sans-serif] [outline:none] text-sm bg-transparent text-[rgb(36,36,36)] dark:text-white m-0 pl-0 pr-5 py-2.5 border-[none]" placeholder="Search" value=""><div class="invisible
            self-center fixed left-3 right-3 w-[calc(100vw - 1.5rem)] top-[54px]
            sm:absolute sm:left-[auto] sm:right-[auto] sm:w-[680px] sm:max-w-[80vw]  overflow-y-auto
            color:text-black dark:text-white bg-white dark:bg-black border-gray-300 dark:border-gray-400 border-[1px] rounded-lg
            p-3  max-w-[680px] min-h-[32px] max-h-[80vh] sm:max-h-[90vh] flex flex-col justify-start items-center"><div class="divide-y divide-gray-200 dark:divide-gray-500"></div><span>No Results</span></div></div></div><div class="flex h-full items-center shrink-0"><div class="sm:mr-4 scale-90 sm:scale-100 cursor-pointer border-2 border-gray-200 dark:border-gray-700 rounded-lg w-9 h-9 flex justify-center items-center"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6"><path stroke-linecap="round" stroke-linejoin="round" d="M21.752 15.002A9.718 9.718 0 0118 15.75c-5.385 0-9.75-4.365-9.75-9.75 0-1.33.266-2.597.748-3.752A9.753 9.753 0 003 11.25C3 16.635 7.365 21 12.75 21a9.753 9.753 0 009.002-5.998z"></path></svg></div><div class="scale-90 sm:scale-100 h-9 flex justify-center items-center text-black dark:text-white"><span class="mr-1">Translate to</span><div class="w-9 sm:w-[100px] relative h-full"><button class="sm:px-2 cursor-pointer h-9 flex justify-center sm:justify-between items-center border-2 border-gray-200 dark:border-gray-700 relative w-full rounded-lg bg-white dark:bg-black py-2 text-left focus:outline-none focus-visible:border-indigo-500 focus-visible:ring-2 focus-visible:ring-white/75 focus-visible:ring-offset-2 focus-visible:ring-offset-orange-300 sm:text-sm" id="headlessui-listbox-button-:Rdbddb6lla:" type="button" aria-haspopup="listbox" aria-expanded="false" data-headlessui-state=""><span class="truncate text-black dark:text-white hidden sm:block">English</span><span class="block truncate text-black dark:text-white sm:hidden ">EN</span><svg class="w-4 h-4 hidden sm:block text-gray-950 dark:text-gray-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 20 20" stroke-width="1.5" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 8.25l-7.5 7.5-7.5-7.5"></path></svg></button></div></div></div></nav><div class="Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx pt-[72px] flex flex-col justify-start items-center bg-white dark:bg-black text-black dark:text-white min-h-full"><div class="read-medium-post max-w-[680px] max-w-full md:max-w-[680px] pb-16"><div class="flex justify-between items-center "><div class="flex justify-start items-center space-x-2"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_e3okk9dKJTwm-hFiWlhX4g.jpg" alt="avatar" class="w-11 h-11 rounded-full border border-solid border-[rgba(0,0,0,0.05)]"><span class="text-gray-950 dark:text-white text-xl max-w-[240px] md:max-w-[400px]">Yash Bhaskar</span></div><div class="cursor-pointer relative"><div class="relative z-10" data-headlessui-state=""><button class="
                text-white/90
                group inline-flex items-center rounded-md bg-transparent px-3 py-2 text-base font-medium hover:text-white focus:outline-none" type="button" aria-expanded="false" data-headlessui-state="" aria-controls="headlessui-popover-panel-:R4q5ldb6lla:" id="headlessui-popover-button-:R2q5ldb6lla:"><svg class="text-black dark:text-white" width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="currentColor"></path></svg></button><div class="invisible bg-gray-200 dark:bg-gray-400 w-14 rounded h-auto block absolute left-1/2 z-1 max-w-sm -translate-x-1/2 transform sm:px-0 lg:max-w-3xl" id="headlessui-popover-panel-:R4q5ldb6lla:" tabindex="-1" data-headlessui-state=""><div class="absolute z-[-1] top-0 left-1/2 transform -translate-x-1/2 -mt-[4px]"><div class="w-3 h-3 bg-gray-200 dark:bg-gray-400 transform rotate-45"></div></div><div class="py-3 pb-0 flex flex-col justify-around items-center overflow-hidden rounded-lg"><div class="w-full space-y-3 flex flex-col justify-around items-center a2a_kit a2a_kit_size_32 a2a_default_style" style="line-height: 32px;"><a class="a2a_button_twitter" target="_blank" rel="nofollow noopener" href="https://readmedium.com/#twitter"><span class="a2a_svg a2a_s__default a2a_s_twitter" style="background-color: rgb(29, 155, 240);"><svg focusable="false" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="#FFF" d="M28 8.557a10 10 0 0 1-2.828.775 4.93 4.93 0 0 0 2.166-2.725 9.7 9.7 0 0 1-3.13 1.194 4.92 4.92 0 0 0-3.593-1.55 4.924 4.924 0 0 0-4.794 6.049c-4.09-.21-7.72-2.17-10.15-5.15a4.94 4.94 0 0 0-.665 2.477c0 1.71.87 3.214 2.19 4.1a5 5 0 0 1-2.23-.616v.06c0 2.39 1.7 4.38 3.952 4.83-.414.115-.85.174-1.297.174q-.476-.001-.928-.086a4.935 4.935 0 0 0 4.6 3.42 9.9 9.9 0 0 1-6.114 2.107q-.597 0-1.175-.068a13.95 13.95 0 0 0 7.55 2.213c9.056 0 14.01-7.507 14.01-14.013q0-.32-.015-.637c.96-.695 1.795-1.56 2.455-2.55z"></path></svg></span><span class="a2a_label">Twitter</span></a><a class="a2a_button_facebook" target="_blank" rel="nofollow noopener" href="https://readmedium.com/#facebook"><span class="a2a_svg a2a_s__default a2a_s_facebook" style="background-color: rgb(8, 102, 255);"><svg focusable="false" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="#fff" d="M28 16c0-6.627-5.373-12-12-12S4 9.373 4 16c0 5.628 3.875 10.35 9.101 11.647v-7.98h-2.474V16H13.1v-1.58c0-4.085 1.849-5.978 5.859-5.978.76 0 2.072.15 2.608.298v3.325c-.283-.03-.775-.045-1.386-.045-1.967 0-2.728.745-2.728 2.683V16h3.92l-.673 3.667h-3.247v8.245C23.395 27.195 28 22.135 28 16"></path></svg></span><span class="a2a_label">Facebook</span></a><a class="a2a_button_linkedin" target="_blank" rel="nofollow noopener" href="https://readmedium.com/#linkedin"><span class="a2a_svg a2a_s__default a2a_s_linkedin" style="background-color: rgb(0, 123, 181);"><svg focusable="false" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="#FFF" d="M6.227 12.61h4.19v13.48h-4.19zm2.095-6.7a2.43 2.43 0 0 1 0 4.86c-1.344 0-2.428-1.09-2.428-2.43s1.084-2.43 2.428-2.43m4.72 6.7h4.02v1.84h.058c.56-1.058 1.927-2.176 3.965-2.176 4.238 0 5.02 2.792 5.02 6.42v7.395h-4.183v-6.56c0-1.564-.03-3.574-2.178-3.574-2.18 0-2.514 1.7-2.514 3.46v6.668h-4.187z"></path></svg></span><span class="a2a_label">LinkedIn</span></a><a class="a2a_button_wechat" target="_blank" rel="nofollow noopener" href="https://readmedium.com/#wechat"><span class="a2a_svg a2a_s__default a2a_s_wechat" style="background-color: rgb(123, 179, 46);"><svg focusable="false" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><g fill="#FFF"><path d="M20.674 12.458c-2.228.116-4.165.792-5.738 2.318-1.59 1.542-2.315 3.43-2.116 5.772-.87-.108-1.664-.227-2.462-.294-.276-.023-.602.01-.836.14-.774.438-1.517.932-2.397 1.482.16-.73.266-1.37.45-1.985.137-.45.074-.7-.342-.994-2.673-1.89-3.803-4.714-2.958-7.624.78-2.69 2.697-4.323 5.302-5.173 3.555-1.16 7.55.022 9.712 2.845a6.63 6.63 0 0 1 1.38 3.516zm-10.253-.906c.025-.532-.44-1.01-.984-1.027a.997.997 0 0 0-1.038.964.984.984 0 0 0 .977 1.02 1.017 1.017 0 0 0 1.05-.96zm5.35-1.028c-.55.01-1.01.478-1 1.012.01.554.466.987 1.03.98a.98.98 0 0 0 .99-1.01.99.99 0 0 0-1.02-.982"></path><path d="M25.68 26.347c-.705-.314-1.352-.785-2.042-.857-.686-.072-1.408.324-2.126.398-2.187.224-4.147-.386-5.762-1.88-3.073-2.842-2.634-7.2.92-9.53 3.16-2.07 7.795-1.38 10.022 1.493 1.944 2.51 1.716 5.837-.658 7.94-.687.61-.934 1.11-.493 1.917.086.148.095.336.14.523zm-8.03-7.775c.448 0 .818-.35.835-.795a.835.835 0 0 0-.83-.865.845.845 0 0 0-.84.86c.016.442.388.8.834.8zm5.176-1.658a.83.83 0 0 0-.824.794c-.02.47.347.858.813.86.45 0 .807-.34.823-.79a.825.825 0 0 0-.812-.864"></path></g></svg></span><span class="a2a_label">WeChat</span></a><a class="a2a_button_qzone" target="_blank" rel="nofollow noopener" href="https://readmedium.com/#qzone"><span class="a2a_svg a2a_s__default a2a_s_qzone" style="background-color: rgb(43, 130, 217);"><svg focusable="false" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path fill="#FFF" d="m27.996 12.83-7.423-.737c-.566-.053-.694-.142-.87-.604l-3.175-7.045c-.29-.598-.765-.598-1.055 0l-3.384 7.04c-.23.393-.34.48-.898.534l-7.187.807c-.66.064-.808.493-.327.952l5.64 5.184c.266.25.27.355.195.697l-1.447 7.61c-.124.65.247.915.82.58l6.44-3.715c.45-.284.87-.293 1.31-.018l6.47 3.734c.575.334.948.07.826-.58L22.83 21.2c.66-.226 1.305-.5 1.69-.81l-.156.03c-2.29.547-5.438.872-8.355.872-1.08 0-2.128-.038-3.13-.11l-.006.005a39 39 0 0 1-2.53-.26c-.3-.05.026-.242.026-.242l7.76-5.513s.202-.126.002-.153c-3.188-.5-6.723-.627-10.042-.627h-.23c2.246-.51 5.07-.815 8.14-.815 1.81 0 3.54.105 5.11.296-.002.003.888.124 1.31.193.33.05.024.24.024.24l-7.77 5.384s-.18.107.015.136c2.39.337 5.332.457 7.98.49l-.118-.65c-.06-.38 0-.51.284-.78l5.478-5.12c.485-.455.34-.88-.318-.945z"></path></svg></span><span class="a2a_label">Qzone</span></a><div style="clear: both;"></div></div></div></div></div><div style="position:fixed;top:1px;left:1px;width:1px;height:0;padding:0;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;display:none"></div></div></div><div class="relative mt-4"><div class="h-fit overflow-auto pb-8 bg-white dark:bg-black w-full px-4 pt-4 rounded-lg shadow-md"><div class="prose break-words dark:prose-invert prose-p:leading-relaxed prose-pre:p-0 text-[var(--theme-text)]"><p class="!font-bold !my-2">Summary</p>
<p class="!my-2 last:mb-0">Decoder-only transformers are a neural network architecture that enables large language models to generate context-aware text by utilizing mechanisms such as self-attention, multi-head attention, and positional encoding.</p>
<p class="!font-bold !my-2">Abstract</p>
<p class="!my-2 last:mb-0">The article delves into the functionality of decoder-only transformers, the driving force behind large language models (LLMs) like GPT-3, LLaMA, and Gemini. It explains how these models surpass traditional word embeddings by using self-attention to create contextualized embeddings that adapt the representation of words based on their surrounding context. The self-attention mechanism involves query, key, and value matrices to determine the relevance of words within a sentence, with masking applied to ensure that only preceding words influence the interpretation of a given word. Multi-head attention expands on this by capturing multiple contextual aspects simultaneously. The transformer architecture stacks these self-attention layers with multilayer perceptrons (MLPs) to refine embeddings and incorporates positional encoding to account for word order. These components collectively empower LLMs to understand and produce human-like text with high accuracy.</p>
<p class="!font-bold !my-2">Opinions</p>
<ul>
<li class="!my-2 last:mb-0">Traditional word embeddings like Word2Vec or GloVe are insufficient for capturing the nuanced meanings of words in different contexts.</li>
<li class="!my-2 last:mb-0">Self-attention is a superior method for generating contextualized embeddings, which are crucial for accurate text generation.</li>
<li class="!my-2 last:mb-0">Masking in self-attention is essential to maintain the causal flow of information in text generation, preventing future words from influencing past interpretations.</li>
<li class="!my-2 last:mb-0">Multi-head attention is praised for its ability to capture diverse aspects of context, enriching the model's understanding of language.</li>
<li class="!my-2 last:mb-0">Positional encoding is a necessary addition to self-attention mechanisms to ensure the significance of word order is not lost.</li>
<li class="!my-2 last:mb-0">The combination of self-attention, MLPs, and positional encoding in decoder-only transformers is seen as a breakthrough in the field of natural language processing, leading to the advanced capabilities of LLMs.</li>
</ul></div></div><div class="h-[50px] absolute flex items-center justify-center w-full bottom-0 left-0 text-center rounded-lg leading-[100%] cursor-pointer bg-gradient-to-b from-[rgba(255,255,255,0.16)] dark:from-[rgba(0,0,0,0.16)] to-white dark:to-black"><svg class="rotate-180 mt-[20px] ml-1 w-4 h-4 text-gray-950 dark:text-gray-500" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 20 20" stroke-width="1.5" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 8.25l-7.5 7.5-7.5-7.5"></path></svg></div></div><div class="mt-8 flex justify-center"><a target="_blank" title="PS2 Filter AI" rel="noopener noreferrer" href="https://openai01.net/"><button class="text-center align-middle px-2 w-[320px] md:w-[600px] dark:bg-[#121212] dark:text-white text-[16px] min-h-[42px] rounded-lg border border-gray-400">Use the OpenAI o1 models for free at OpenAI01.net (10 times a day for free)!<svg width="20px" height="20px" class="inline-flex ml-1" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke="#fff"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"><g id="Interface / External_Link"><path id="Vector" d="M10.0002 5H8.2002C7.08009 5 6.51962 5 6.0918 5.21799C5.71547 5.40973 5.40973 5.71547 5.21799 6.0918C5 6.51962 5 7.08009 5 8.2002V15.8002C5 16.9203 5 17.4801 5.21799 17.9079C5.40973 18.2842 5.71547 18.5905 6.0918 18.7822C6.5192 19 7.07899 19 8.19691 19H15.8031C16.921 19 17.48 19 17.9074 18.7822C18.2837 18.5905 18.5905 18.2839 18.7822 17.9076C19 17.4802 19 16.921 19 15.8031V14M20 9V4M20 4H15M20 4L13 11" stroke="#fff" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></g></g></svg></button></a></div><div><article><h1 id="f35c">Decoder-Only Transformers Explained: The Engine Behind LLMs</h1><figure id="68b8"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_qTjjAvXmrSaRdN1LODLVGA.png"><figcaption><a href="https://arxiv.org/abs/2305.07716">https://arxiv.org/abs/2305.07716</a></figcaption></figure><p id="df67">Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of these powerful models lies a specific type of neural network architecture called the <b>decoder-only transformer</b>. This article will discuss the inner workings of decoder-only transformers, providing a clear and comprehensive explanation of their key components and how they enable LLMs to understand and generate human-like text.</p><h1 id="f3a9">1. The Problem with Word-to-Vector</h1><p id="294e">Traditional natural language processing techniques often relied on simple word embeddings, where each word is represented by a fixed vector, similar to how Word2Vec or GloVe operate. This means that regardless of the context in which a word appears, its vector representation remains the same. For example, using a simple word embedding approach, the vector for “bank” in “river bank” would be identical to the vector for “bank” in “financial bank.” These embeddings, however, fail to capture the nuanced meanings a word can take on depending on its surrounding words. A robust language model needs to understand these contextual differences to generate accurate and meaningful text.</p><h1 id="5144">2. How Self-Attention Provides Contextualized Embeddings</h1><p id="c658">Decoder-only transformers tackle this challenge through a mechanism called <b>self-attention</b>. This innovative technique allows the model to consider the relationships between different words in a sentence, effectively creating contextualized embeddings that reflect the meaning of a word within its specific context.</p><figure id="bc35"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/0_FA3tTGE4MrVbNk_w.png"><figcaption></figcaption></figure><p id="bacf"><b>2.1. The Mechanics of Self-Attention: Query, Key, and Value</b></p><p id="9469">Self-attention operates by transforming each word’s initial embedding based on its interactions with all other words in the sequence. This transformation involves three key components:</p><ul><li><b>Query Matrix (WQ):</b> Each word’s embedding is multiplied by the query matrix to generate a query vector. Imagine this vector as the word asking a question, “What other words in this sentence are relevant to me?”</li><li><b>Key Matrix (WK):</b> Simultaneously, every word’s embedding is multiplied by the key matrix to produce a key vector. These vectors are potential answers to the queries.</li><li><b>Value Matrix (WV):</b> Finally, the value matrix projects each word’s embedding into a value vector, representing the information that will be passed to other words based on their relevance.</li></ul><p id="23b5"><b>2.2. Capturing Correlations and the Need for Masking</b></p><p id="1d4e">To measure the relevance of each word to another, the model computes the dot product of each query vector with every key vector. This generates a grid of scores, representing the correlation between each word pair. Higher scores indicate stronger relevance.</p><p id="12a5">In decoder-only transformers, however, we want to prevent future words from influencing the interpretation of past words. This is crucial for maintaining the flow of information in text generation, where the model should only predict the next word based on what has come before. To achieve this, we apply <b>masking</b>: the scores for future words are set to negative infinity, effectively zeroing them out after a softmax normalization.</p><p id="d138"><b>2.3. The Role of the Value Matrix</b></p><p id="37b5">Once the attention scores are computed and masked, the value matrix comes into play. For each word, we take a weighted sum of the value vectors of all other words, where the weights are the normalized attention scores. In essence, this allows relevant words to “pass” their information to the word being processed.</p><p id="fda0"><b>2.4. From Word Embeddings to Contextualized Embeddings</b></p><p id="427e">The final step in self-attention involves adding the weighted sum of value vectors to the original word embedding. This updated embedding now incorporates contextual information, reflecting the word’s meaning within the sentence. The formula for this transformation can be summarized as:</p><blockquote id="f8aa"><p>Contextualized Embedding = Original Embedding + Weighted Sum of Value Vectors (weighted by attention scores)</p></blockquote><h1 id="5df0">3. Multi-Head Attention: Expanding the Scope of Context</h1><p id="6088">While self-attention is powerful, it can be limited by its reliance on a single set of query, key, and value matrices. <b>Multi-head attention</b> addresses this limitation by running multiple self-attention operations in parallel, each with its own unique set of matrices. This allows the model to capture different aspects of context simultaneously. For instance, one head might focus on grammatical relationships, while another might focus on semantic connections. The outputs of all heads are then combined to produce a richer, more nuanced contextualized embedding.</p><figure id="8db9"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/0_TaZXsiwcPwng0Ffr.png"><figcaption><a href="https://arxiv.org/abs/1706.03762v7">https://arxiv.org/abs/1706.03762v7</a></figcaption></figure><h1 id="e919">4. Decoder-Only Transformers: A Series of Masked Self-Attention and MLPs</h1><p id="2731">A decoder-only transformer is constructed by stacking multiple layers, each comprising two main components:</p><ul><li><b>Masked Self-Attention:</b> As described above, this layer allows words to incorporate contextual information from preceding words.</li><li><b>Multilayer Perceptrons (MLPs):</b> MLPs are essentially feed-forward neural networks that process each word’s embedding independently. They can be thought of as the “knowledge banks” of the transformer, incorporating general knowledge learned during training to further refine the embeddings.</li></ul><p id="c769"><b>4.1. MLPs: Incorporating Knowledge into Embeddings</b></p><p id="c693">MLPs operate by projecting each word’s embedding into a higher-dimensional space, applying a non-linear activation function (like ReLU), and then projecting it back down to the original embedding dimension. This process allows the MLP to extract complex features from the embeddings and incorporate information that’s not explicitly captured by the attention mechanism.</p><h1 id="9920">5. Positional Encoding: Capturing Word Order</h1><p id="addb">While self-attention considers relationships between words, it doesn’t inherently capture their order within the sequence. To address this, decoder-only transformers utilize <b>positional encoding</b>. This involves adding a vector representing the word’s position in the sentence to its embedding. These positional embeddings can be learned during training or generated using predefined functions, such as sine and cosine waves. By incorporating positional information, the model can understand the significance of word order in sentences.</p><h1 id="31ed">Conclusion</h1><p id="03e3">Decoder-only transformers are remarkable architectures. By leveraging self-attention, MLPs, and positional encoding, they provide LLMs with the ability to understand and generate text with a level of sophistication previously unattainable. This architecture is constantly evolving, with researchers exploring novel modifications to further enhance its capabilities. As we continue to push the boundaries of language models, understanding the principles behind decoder-only transformers remains crucial for unlocking their full potential.</p><blockquote id="7cdf"><p><b><i>Connect with me </i></b><i>: <a href="https://www.linkedin.com/in/yash-bhaskar/">https://www.linkedin.com/in/yash-bhaskar/</a>
<b>More Articles like this</b>: <a href="https://medium.com/@yash9439">https://medium.com/@yash9439</a></i></p></blockquote></article><script defer="" src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/vcd15cbe7772f49c399c6a5babf22c1241717689176015" integrity="sha512-ZpsOmlRQV6y907TI0dKBHq9Md29nnaEIPlkf84rnaERnq6zvWvPUqr2ft8M1aS28oN72PdrCzSjY4U6VaAw1EQ==" data-cf-beacon="{&quot;rayId&quot;:&quot;971763669842e9c3&quot;,&quot;serverTiming&quot;:{&quot;name&quot;:{&quot;cfExtPri&quot;:true,&quot;cfEdge&quot;:true,&quot;cfOrigin&quot;:true,&quot;cfL4&quot;:true,&quot;cfSpeedBrain&quot;:true,&quot;cfCacheStatus&quot;:true}},&quot;version&quot;:&quot;2025.8.0&quot;,&quot;token&quot;:&quot;4af5d1557add4585a1ecb017352c34ac&quot;}" crossorigin="anonymous"></script>
</div><div class=" text-sm font-bold mt-12 space-y-[4px]"><div class="undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]">Decoder Only Transformers</div><div class="undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]">Self Attention</div></div></div><span class="flex justify-center items-center w-full py-12 text-2xl divide-y border-t border-gray-300 dark:border-gray-700">Recommended from ReadMedium</span><div class="max-w-[680px]"><div class="w-full py-8"><div class="flex justify-start items-center mb-4"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_8biNIOdTZO6v4MDdtPmm2Q.png" alt="avatar" class="w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]"><span class="ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white">LM Po</span></div><a href="https://readmedium.com/understanding-self-attention-and-transformer-network-architecture-0734f73b8fa3"><span class="text-[20px] font-bold leading-[24px]">Self-Attention and Transformer Network Architecture</span><p class="text-[16px] font-[400] my-1">The introduction of Transformer models in 2017 marked a significant turning point in the fields of Natural Language Processing (NLP) and…</p></a><div class="mt-6"><span>14 min read</span></div></div><div class="w-full py-8"><div class="flex justify-start items-center mb-4"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_yTk-vcLIuC_3A8TGrvLWUg.png" alt="avatar" class="w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]"><span class="ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white">Charles Chi</span></div><a href="https://readmedium.com/understanding-query-key-value-in-transformers-c579b93054cc"><span class="text-[20px] font-bold leading-[24px]">Understanding Query, Key, Value in Transformers and LLMs</span><p class="text-[16px] font-[400] my-1">Building intuition through accessible, simplified examples to explore inner workings of the Attention Mechanism.</p></a><div class="mt-6"><span>9 min read</span></div></div><div class="w-full py-8"><div class="flex justify-start items-center mb-4"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_PRRJs0q0QzXuHtHYKUh-Mg.jpg" alt="avatar" class="w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]"><span class="ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white">Manyi</span></div><a href="https://readmedium.com/architecture-of-meta-llama-3-2-1b-model-e6216cdad960"><span class="text-[20px] font-bold leading-[24px]">Architecture of Meta Llama 3.2 1B Model</span><p class="text-[16px] font-[400] my-1">Meta released Llama 3.2 models in September 2024. In this article, we take an in-depth look of Llama 3.2 1B model’s architecture.</p></a><div class="mt-6"><span>4 min read</span></div></div><div class="w-full py-8"><div class="flex justify-start items-center mb-4"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_XZapwwtSpdDz7JSo_kFUJA.jpg" alt="avatar" class="w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]"><span class="ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white">Himanshu Bamoria</span></div><a href="https://readmedium.com/research-paper-summary-training-language-models-to-self-correct-via-reinforcement-learning-d14b52f7bbd2"><span class="text-[20px] font-bold leading-[24px]">[Research Paper Summary]Training Language Models to Self-Correct via Reinforcement Learning</span><p class="text-[16px] font-[400] my-1"></p></a><div class="mt-6"><span>4 min read</span></div></div><div class="w-full py-8"><div class="flex justify-start items-center mb-4"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_LDjQS3c-G1gsojOf24ijGg@2x.jpg" alt="avatar" class="w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]"><span class="ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white">Vipra Singh</span></div><a href="https://readmedium.com/llm-architectures-explained-encoder-decoder-architecture-part-4-b96ace71394c"><span class="text-[20px] font-bold leading-[24px]">LLM Architectures Explained: Encoder-Decoder Architecture (Part 4)</span><p class="text-[16px] font-[400] my-1">Deep Dive into the architecture &amp; building real-world applications leveraging NLP Models starting from RNN to Transformer.</p></a><div class="mt-6"><span>34 min read</span></div></div><div class="w-full py-8"><div class="flex justify-start items-center mb-4"><img src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/0_vc7cpjnWjyj0c_WR.jpg" alt="avatar" class="w-8 h-8 shadow-[rgba(0,0,0,0.05)_0px_0px_0px_1px_inset] rounded-[50%] border-[none]"><span class="ml-2 font-normal leading-5 text-sm text-[rgb(36,36,36)] overflow-hidden text-ellipsis [display:-webkit-box] [-webkit-line-clamp:1] [-webkit-box-orient:vertical] break-all max-h-none m-0 dark:text-white">Satyabrata Dash</span></div><a href="https://readmedium.com/understanding-graph-based-rag-systems-a-deep-dive-into-graphrag-and-lightrag-daf4f982d7d9"><span class="text-[20px] font-bold leading-[24px]">Understanding Graph-based RAG Systems: A Deep Dive into GraphRAG and LightRAG</span><p class="text-[16px] font-[400] my-1">The Need for Graph-based RAG Systems</p></a><div class="mt-6"><span>12 min read</span></div></div></div></div><!--/$--><footer class="Footer_footer__3uw59"><div class="Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx Footer_wrap__EQ_jB"><a href="https://readmedium.com/"><picture><img class="Footer_logo__o5wYk" alt="Read Medium Logo" src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/logo-white.webp"></picture></a><nav class="Footer_nav__juVyT"><a target="_blank" rel="noopener noreferrer" href="https://openai01.net/">Free OpenAI o1 chat</a><a target="_blank" rel="noopener noreferrer" href="https://openaio1api.com/">Try OpenAI o1 API</a></nav></div></footer><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/webpack-201b96c0da5002a8.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"0:\"$L1\"\n"])</script><script>self.__next_f.push([1,"2:HL[\"/_next/static/css/68907a8c5fb73615.css\",\"style\"]\n3:HL[\"/_next/static/css/b89dd6028ec10fe0.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I{\"id\":6054,\"chunks\":[\"272:static/chunks/webpack-201b96c0da5002a8.js\",\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\",\"864:static/chunks/864-fc79a795ba0d747c.js\"],\"name\":\"\",\"async\":false}\n6:I{\"id\":1729,\"chunks\":[\"272:static/chunks/webpack-201b96c0da5002a8.js\",\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\",\"864:static/chunks/864-fc79a795ba0d747c.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"1:[[],[\"$\",\"$L4\",null,{\"buildId\":\"c0Nu8GPLU6DtrlNljKuHT\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/en/https:/medium.com/@yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\",\"initialTree\":[\"\",{\"children\":[[\"lang\",\"en\",\"d\"],{\"children\":[[\"slug\",\"https%3A/medium.com/%40yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\",\"c\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L5\"],\"globalErrorComponent\":\"$6\",\"children\":[null,\"$L7\",null]}]]\n"])</script><script>self.__next_f.push([1,"8:I{\"id\":1443,\"chunks\":[\"272:static/chunks/webpack-201b96c0da5002a8.js\",\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\",\"864:static/chunks/864-fc79a795ba0d747c.js\"],\"name\":\"\",\"async\":false}\n9:I{\"id\":8639,\"chunks\":[\"272:static/chunks/webpack-201b96c0da5002a8.js\",\"971:static/chunks/fd9d1056-b59a6f32ac74cecf.js\",\"864:static/chunks/864-fc79a795ba0d747c.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[null,\"$La\",null],\"segment\":[\"lang\",\"en\",\"d\"]},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/68907a8c5fb73615.css\",\"precedence\":\"next\"}]]}]\n"])</script><script>self.__next_f.push([1,"b:I{\"id\":4244,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"845:static/chunks/845-a25742f4af4e91ff.js\",\"84:static/chunks/app/[lang]/layout-af08458983e85ed1.js\"],\"name\":\"\",\"async\":false}\nc:I{\"id\":3050,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"845:static/chunks/845-a25742f4af4e91ff.js\",\"84:static/chunks/app/[lang]/layout-af08458983e85ed1.js\"],\"name\":\"Providers\",\"async\":false}\ne:I{\"id\":4724,\"chunks\":[\""])</script><script>self.__next_f.push([1,"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"91:static/chunks/app/[lang]/[...slug]/layout-edff2c76e26276a0.js\"],\"name\":\"\",\"async\":false}\nf:I{\"id\":9891,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"845:static/chunks/845-a25742f4af4e91ff.js\",\"84:static/chunks/app/[lang]/layout-af08458983e85ed1.js\"],\"name\":\"\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"$Lb\",null,{\"id\":\"theme-script\",\"strategy\":\"beforeInteractive\",\"dangerouslySetInnerHTML\":{\"__html\":\"\\n  (function () {\\n    function getImplicitPreference() {\\n      var mediaQuery = '(prefers-color-scheme: dark)'\\n      var mql = window.matchMedia(mediaQuery)\\n      var hasImplicitPreference = typeof mql.matches === 'boolean'\\n\\n      if (hasImplicitPreference) {\\n        return mql.matches ? 'dark' : 'light'\\n      }\\n\\n      return null\\n    }\\n\\n    function themeIsValid(theme) {\\n      return theme === 'light' || theme === 'dark'\\n    }\\n\\n    var themeToSet = 'light'\\n    var preference = window.localStorage.getItem('payload-theme')\\n\\n    if (themeIsValid(preference)) {\\n      themeToSet = preference\\n    } else {\\n      var implicitPreference = getImplicitPreference()\\n\\n      if (implicitPreference) {\\n        themeToSet = implicitPreference\\n      }\\n    }\\n\\n    // payload 主题控制\\n    document.documentElement.setAttribute('data-theme', themeToSet)\\n    // tailwind主题控制\\n    if(themeToSet === 'dark'){\\n      document.documentElement.classList.add('dark')\\n    }\\n  })();\\n  \"}}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"sizes\":\"32x32\"}],[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}]]}],[\"$\",\"body\",null,{\"children\":[\"$\",\"$Lc\",null,{\"children\":[[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",[\"lang\",\"en\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[null,\"$Ld\",null],\"segment\":[\"slug\",\"https%3A/medium.com/%40yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\",\"c\"]},\"styles\":[]}],[\"$\",\"footer\",null,{\"className\":\"Footer_footer__3uw59\",\"children\":[\"$\",\"div\",null,{\"className\":\"Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx Footer_wrap__EQ_jB\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/\",\"children\":[\"$\",\"picture\",null,{\"children\":[\"$\",\"img\",null,{\"className\":\"Footer_logo__o5wYk\",\"alt\":\"Read Medium Logo\",\"src\":\"/images/logo/logo-white.webp\"}]}]}],[\"$\",\"nav\",null,{\"className\":\"Footer_nav__juVyT\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"https://openai01.net/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Free OpenAI o1 chat\"}],[\"$\",\"$Le\",null,{\"href\":\"https://openaio1api.com/\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"Try OpenAI o1 API\"}]]}]]}]}]]}]}],[\"$\",\"$Lf\",null,{}]]}]\n"])</script><script>self.__next_f.push([1,"10:\"$Sreact.suspense\"\n11:I{\"id\":6964,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"91:static/chunks/app/[lang]/[...slug]/layout-edff2c76e26276a0.js\"],\"name\":\"Image\",\"async\":false}\n12:I{\"id\":210,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"317:static/chunks/317-6e0166f62ae1"])</script><script>self.__next_f.push([1,"807b.js\",\"91:static/chunks/app/[lang]/[...slug]/layout-edff2c76e26276a0.js\"],\"name\":\"\",\"async\":false}\n13:I{\"id\":2681,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"91:static/chunks/app/[lang]/[...slug]/layout-edff2c76e26276a0.js\"],\"name\":\"ThemeSelector\",\"async\":false}\n14:I{\"id\":9044,\"chunks\":[\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf6"])</script><script>self.__next_f.push([1,"3de2d.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"91:static/chunks/app/[lang]/[...slug]/layout-edff2c76e26276a0.js\"],\"name\":\"LanguageSelector\",\"async\":false}\n"])</script><script>self.__next_f.push([1,"d:[\"$\",\"$10\",null,{\"fallback\":[[\"$\",\"nav\",null,{\"className\":\"fixed z-10 h-[54px] w-screen flex flex-grow-0 items-center justify-between bg-white dark:bg-black px-4 shadow-md dark:shadow-neutral-900\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/\",\"className\":\"scale-90 sm:scale-100 cursor-pointer flex items-center flex-shrink-0 text-white hover:scale-105 transform transition\",\"children\":[\"$\",\"$L11\",null,{\"className\":\"mt-2 mb-2 h-6 w-auto sm:px-3\",\"width\":141,\"height\":37,\"src\":\"/images/logo/logo-black.webp\",\"alt\":\"Read Medium logo\"}]}],[\"$\",\"$L12\",null,{\"className\":\"px-3 flex justify-center items-center\"}],[\"$\",\"div\",null,{\"className\":\"flex h-full items-center shrink-0\",\"children\":[[\"$\",\"$L13\",null,{\"className\":\"sm:mr-4\"}],[\"$\",\"$L14\",null,{\"originLang\":\"$undefined\",\"lang\":\"en\",\"pathname\":\"$undefined\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx pt-[72px] flex flex-col justify-start items-center bg-white dark:bg-black text-black dark:text-white min-h-full\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-[680px] w-full h-screen\",\"children\":[\"$\",\"div\",null,{\"role\":\"status\",\"className\":\"rounded animate-pulse\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center mb-4\",\"children\":[[\"$\",\"svg\",null,{\"className\":\"w-11 h-11 me-3 text-gray-200 dark:text-gray-700\",\"aria-hidden\":\"true\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 20 20\",\"children\":[\"$\",\"path\",null,{\"d\":\"M10 0a10 10 0 1 0 10 10A10.011 10.011 0 0 0 10 0Zm0 5a3 3 0 1 1 0 6 3 3 0 0 1 0-6Zm0 13a8.949 8.949 0 0 1-4.951-1.488A3.987 3.987 0 0 1 9 13h2a3.987 3.987 0 0 1 3.951 3.512A8.949 8.949 0 0 1 10 18Z\"}]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"h-2.5 bg-gray-200 rounded-full dark:bg-gray-700 w-32 mb-2\"}],[\"$\",\"div\",null,{\"className\":\"w-48 h-2 bg-gray-200 rounded-full dark:bg-gray-700\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"mt-12 h-2.5 bg-gray-200 rounded-full dark:bg-gray-700 w-48 mb-4\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700 mb-2.5\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700 mb-2.5\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700\"}],[\"$\",\"div\",null,{\"className\":\"mt-8  flex items-center justify-center h-48 mb-4 bg-gray-300 rounded dark:bg-gray-700\",\"children\":[\"$\",\"svg\",null,{\"className\":\"w-10 h-10 text-gray-200 dark:text-gray-600\",\"aria-hidden\":\"true\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"fill\":\"currentColor\",\"viewBox\":\"0 0 16 20\",\"children\":[[\"$\",\"path\",null,{\"d\":\"M14.066 0H7v5a2 2 0 0 1-2 2H0v11a1.97 1.97 0 0 0 1.934 2h12.132A1.97 1.97 0 0 0 16 18V2a1.97 1.97 0 0 0-1.934-2ZM10.5 6a1.5 1.5 0 1 1 0 2.999A1.5 1.5 0 0 1 10.5 6Zm2.221 10.515a1 1 0 0 1-.858.485h-8a1 1 0 0 1-.9-1.43L5.6 10.039a.978.978 0 0 1 .936-.57 1 1 0 0 1 .9.632l1.181 2.981.541-1a.945.945 0 0 1 .883-.522 1 1 0 0 1 .879.529l1.832 3.438a1 1 0 0 1-.031.988Z\"}],[\"$\",\"path\",null,{\"d\":\"M5 5V.13a2.96 2.96 0 0 0-1.293.749L.879 3.707A2.98 2.98 0 0 0 .13 5H5Z\"}]]}]}],[\"$\",\"div\",null,{\"className\":\"mt-8 h-2.5 bg-gray-200 rounded-full dark:bg-gray-700 w-48 mb-4\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700 mb-2.5\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700 mb-2.5\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700\"}],[\"$\",\"div\",null,{\"className\":\"mt-8 h-2.5 bg-gray-200 rounded-full dark:bg-gray-700 w-48 mb-4\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700 mb-2.5\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700 mb-2.5\"}],[\"$\",\"div\",null,{\"className\":\"h-2 bg-gray-200 rounded-full dark:bg-gray-700\"}]]}]}]}]],\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",[\"lang\",\"en\",\"d\"],\"children\",[\"slug\",\"https%3A/medium.com/%40yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\",\"c\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L15\",\"$L16\",null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/b89dd6028ec10fe0.css\",\"precedence\":\"next\"}]]}]}]\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Decoder-Only Transformers Explained: The Engine Behind LLMs\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of…\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover\"}],[\"$\",\"link\",\"4\",{\"rel\":\"canonical\",\"href\":\"https://readmedium.com/en/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Decoder-Only Transformers Explained: The Engine Behind LLMs\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:description\",\"content\":\"Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of…\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:url\",\"content\":\"https://readmedium.com/en/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@readmedium\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Decoder-Only Transformers Explained: The Engine Behind LLMs\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Large language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of…\"}]]\n"])</script><script>self.__next_f.push([1,"15:null\n"])</script><script>self.__next_f.push([1,"17:I{\"id\":5362,\"chunks\":[\"411:static/chunks/8318a8be-b8e7397c242123f5.js\",\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"62:static/chunks/62-bd0c3d6303ae814e.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"992:static/chunks/992-3e360c41c9bb376b.js\",\"90:static/chunks/app/[lang]/[...slug]/page-256eb16a6cb69640.js\"],\"name\":\"\",\"async\":false}\n18:I{\"id\":8577,\"chunks\":[\"411:static/chun"])</script><script>self.__next_f.push([1,"ks/8318a8be-b8e7397c242123f5.js\",\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"62:static/chunks/62-bd0c3d6303ae814e.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"992:static/chunks/992-3e360c41c9bb376b.js\",\"90:static/chunks/app/[lang]/[...slug]/page-256eb16a6cb69640.js\"],\"name\":\"\",\"async\":false}\n1b:I{\"id\":826,\"chunks\":[\"411:static/chunks/8318a8be-b8e7397c242123f5.js\",\"724:stat"])</script><script>self.__next_f.push([1,"ic/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"62:static/chunks/62-bd0c3d6303ae814e.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"992:static/chunks/992-3e360c41c9bb376b.js\",\"90:static/chunks/app/[lang]/[...slug]/page-256eb16a6cb69640.js\"],\"name\":\"\",\"async\":false}\n1d:I{\"id\":2911,\"chunks\":[\"411:static/chunks/8318a8be-b8e7397c242123f5.js\",\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:s"])</script><script>self.__next_f.push([1,"tatic/chunks/848-bede78ffaf63de2d.js\",\"475:static/chunks/475-36c9ef78992d7529.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"62:static/chunks/62-bd0c3d6303ae814e.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"992:static/chunks/992-3e360c41c9bb376b.js\",\"90:static/chunks/app/[lang]/[...slug]/page-256eb16a6cb69640.js\"],\"name\":\"\",\"async\":false}\n1e:I{\"id\":5077,\"chunks\":[\"411:static/chunks/8318a8be-b8e7397c242123f5.js\",\"724:static/chunks/724-1461eaeda7f61205.js\",\"848:static/chunks/848-bede78ffaf63de2d.js\",\"47"])</script><script>self.__next_f.push([1,"5:static/chunks/475-36c9ef78992d7529.js\",\"490:static/chunks/490-5d95994d1559162d.js\",\"62:static/chunks/62-bd0c3d6303ae814e.js\",\"317:static/chunks/317-6e0166f62ae1807b.js\",\"992:static/chunks/992-3e360c41c9bb376b.js\",\"90:static/chunks/app/[lang]/[...slug]/page-256eb16a6cb69640.js\"],\"name\":\"\",\"async\":false}\n19:T2069,"])</script><script>self.__next_f.push([1,"\u003cbody\u003e\u003carticle\u003e\u003ch1 id=\"f35c\"\u003eDecoder-Only Transformers Explained: The Engine Behind LLMs\u003c/h1\u003e\u003cfigure id=\"68b8\"\u003e\u003cimg src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qTjjAvXmrSaRdN1LODLVGA.png\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://arxiv.org/abs/2305.07716\"\u003ehttps://arxiv.org/abs/2305.07716\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"df67\"\u003eLarge language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of these powerful models lies a specific type of neural network architecture called the \u003cb\u003edecoder-only transformer\u003c/b\u003e. This article will discuss the inner workings of decoder-only transformers, providing a clear and comprehensive explanation of their key components and how they enable LLMs to understand and generate human-like text.\u003c/p\u003e\u003ch1 id=\"f3a9\"\u003e1. The Problem with Word-to-Vector\u003c/h1\u003e\u003cp id=\"294e\"\u003eTraditional natural language processing techniques often relied on simple word embeddings, where each word is represented by a fixed vector, similar to how Word2Vec or GloVe operate. This means that regardless of the context in which a word appears, its vector representation remains the same. For example, using a simple word embedding approach, the vector for “bank” in “river bank” would be identical to the vector for “bank” in “financial bank.” These embeddings, however, fail to capture the nuanced meanings a word can take on depending on its surrounding words. A robust language model needs to understand these contextual differences to generate accurate and meaningful text.\u003c/p\u003e\u003ch1 id=\"5144\"\u003e2. How Self-Attention Provides Contextualized Embeddings\u003c/h1\u003e\u003cp id=\"c658\"\u003eDecoder-only transformers tackle this challenge through a mechanism called \u003cb\u003eself-attention\u003c/b\u003e. This innovative technique allows the model to consider the relationships between different words in a sentence, effectively creating contextualized embeddings that reflect the meaning of a word within its specific context.\u003c/p\u003e\u003cfigure id=\"bc35\"\u003e\u003cimg src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FA3tTGE4MrVbNk_w.png\"\u003e\u003cfigcaption\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"bacf\"\u003e\u003cb\u003e2.1. The Mechanics of Self-Attention: Query, Key, and Value\u003c/b\u003e\u003c/p\u003e\u003cp id=\"9469\"\u003eSelf-attention operates by transforming each word’s initial embedding based on its interactions with all other words in the sequence. This transformation involves three key components:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eQuery Matrix (WQ):\u003c/b\u003e Each word’s embedding is multiplied by the query matrix to generate a query vector. Imagine this vector as the word asking a question, “What other words in this sentence are relevant to me?”\u003c/li\u003e\u003cli\u003e\u003cb\u003eKey Matrix (WK):\u003c/b\u003e Simultaneously, every word’s embedding is multiplied by the key matrix to produce a key vector. These vectors are potential answers to the queries.\u003c/li\u003e\u003cli\u003e\u003cb\u003eValue Matrix (WV):\u003c/b\u003e Finally, the value matrix projects each word’s embedding into a value vector, representing the information that will be passed to other words based on their relevance.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"23b5\"\u003e\u003cb\u003e2.2. Capturing Correlations and the Need for Masking\u003c/b\u003e\u003c/p\u003e\u003cp id=\"1d4e\"\u003eTo measure the relevance of each word to another, the model computes the dot product of each query vector with every key vector. This generates a grid of scores, representing the correlation between each word pair. Higher scores indicate stronger relevance.\u003c/p\u003e\u003cp id=\"12a5\"\u003eIn decoder-only transformers, however, we want to prevent future words from influencing the interpretation of past words. This is crucial for maintaining the flow of information in text generation, where the model should only predict the next word based on what has come before. To achieve this, we apply \u003cb\u003emasking\u003c/b\u003e: the scores for future words are set to negative infinity, effectively zeroing them out after a softmax normalization.\u003c/p\u003e\u003cp id=\"d138\"\u003e\u003cb\u003e2.3. The Role of the Value Matrix\u003c/b\u003e\u003c/p\u003e\u003cp id=\"37b5\"\u003eOnce the attention scores are computed and masked, the value matrix comes into play. For each word, we take a weighted sum of the value vectors of all other words, where the weights are the normalized attention scores. In essence, this allows relevant words to “pass” their information to the word being processed.\u003c/p\u003e\u003cp id=\"fda0\"\u003e\u003cb\u003e2.4. From Word Embeddings to Contextualized Embeddings\u003c/b\u003e\u003c/p\u003e\u003cp id=\"427e\"\u003eThe final step in self-attention involves adding the weighted sum of value vectors to the original word embedding. This updated embedding now incorporates contextual information, reflecting the word’s meaning within the sentence. The formula for this transformation can be summarized as:\u003c/p\u003e\u003cblockquote id=\"f8aa\"\u003e\u003cp\u003eContextualized Embedding = Original Embedding + Weighted Sum of Value Vectors (weighted by attention scores)\u003c/p\u003e\u003c/blockquote\u003e\u003ch1 id=\"5df0\"\u003e3. Multi-Head Attention: Expanding the Scope of Context\u003c/h1\u003e\u003cp id=\"6088\"\u003eWhile self-attention is powerful, it can be limited by its reliance on a single set of query, key, and value matrices. \u003cb\u003eMulti-head attention\u003c/b\u003e addresses this limitation by running multiple self-attention operations in parallel, each with its own unique set of matrices. This allows the model to capture different aspects of context simultaneously. For instance, one head might focus on grammatical relationships, while another might focus on semantic connections. The outputs of all heads are then combined to produce a richer, more nuanced contextualized embedding.\u003c/p\u003e\u003cfigure id=\"8db9\"\u003e\u003cimg src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TaZXsiwcPwng0Ffr.png\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762v7\"\u003ehttps://arxiv.org/abs/1706.03762v7\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"e919\"\u003e4. Decoder-Only Transformers: A Series of Masked Self-Attention and MLPs\u003c/h1\u003e\u003cp id=\"2731\"\u003eA decoder-only transformer is constructed by stacking multiple layers, each comprising two main components:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eMasked Self-Attention:\u003c/b\u003e As described above, this layer allows words to incorporate contextual information from preceding words.\u003c/li\u003e\u003cli\u003e\u003cb\u003eMultilayer Perceptrons (MLPs):\u003c/b\u003e MLPs are essentially feed-forward neural networks that process each word’s embedding independently. They can be thought of as the “knowledge banks” of the transformer, incorporating general knowledge learned during training to further refine the embeddings.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"c769\"\u003e\u003cb\u003e4.1. MLPs: Incorporating Knowledge into Embeddings\u003c/b\u003e\u003c/p\u003e\u003cp id=\"c693\"\u003eMLPs operate by projecting each word’s embedding into a higher-dimensional space, applying a non-linear activation function (like ReLU), and then projecting it back down to the original embedding dimension. This process allows the MLP to extract complex features from the embeddings and incorporate information that’s not explicitly captured by the attention mechanism.\u003c/p\u003e\u003ch1 id=\"9920\"\u003e5. Positional Encoding: Capturing Word Order\u003c/h1\u003e\u003cp id=\"addb\"\u003eWhile self-attention considers relationships between words, it doesn’t inherently capture their order within the sequence. To address this, decoder-only transformers utilize \u003cb\u003epositional encoding\u003c/b\u003e. This involves adding a vector representing the word’s position in the sentence to its embedding. These positional embeddings can be learned during training or generated using predefined functions, such as sine and cosine waves. By incorporating positional information, the model can understand the significance of word order in sentences.\u003c/p\u003e\u003ch1 id=\"31ed\"\u003eConclusion\u003c/h1\u003e\u003cp id=\"03e3\"\u003eDecoder-only transformers are remarkable architectures. By leveraging self-attention, MLPs, and positional encoding, they provide LLMs with the ability to understand and generate text with a level of sophistication previously unattainable. This architecture is constantly evolving, with researchers exploring novel modifications to further enhance its capabilities. As we continue to push the boundaries of language models, understanding the principles behind decoder-only transformers remains crucial for unlocking their full potential.\u003c/p\u003e\u003cblockquote id=\"7cdf\"\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eConnect with me \u003c/i\u003e\u003c/b\u003e\u003ci\u003e: \u003ca href=\"https://www.linkedin.com/in/yash-bhaskar/\"\u003ehttps://www.linkedin.com/in/yash-bhaskar/\u003c/a\u003e\n\u003cb\u003eMore Articles like this\u003c/b\u003e: \u003ca href=\"https://medium.com/@yash9439\"\u003ehttps://medium.com/@yash9439\u003c/a\u003e\u003c/i\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003c/article\u003e\u003c/body\u003e"])</script><script>self.__next_f.push([1,"1a:T858,"])</script><script>self.__next_f.push([1,"# Summary\n\nDecoder-only transformers are a neural network architecture that enables large language models to generate context-aware text by utilizing mechanisms such as self-attention, multi-head attention, and positional encoding.\n\n# Abstract\n\nThe article delves into the functionality of decoder-only transformers, the driving force behind large language models (LLMs) like GPT-3, LLaMA, and Gemini. It explains how these models surpass traditional word embeddings by using self-attention to create contextualized embeddings that adapt the representation of words based on their surrounding context. The self-attention mechanism involves query, key, and value matrices to determine the relevance of words within a sentence, with masking applied to ensure that only preceding words influence the interpretation of a given word. Multi-head attention expands on this by capturing multiple contextual aspects simultaneously. The transformer architecture stacks these self-attention layers with multilayer perceptrons (MLPs) to refine embeddings and incorporates positional encoding to account for word order. These components collectively empower LLMs to understand and produce human-like text with high accuracy.\n\n# Opinions\n\n- Traditional word embeddings like Word2Vec or GloVe are insufficient for capturing the nuanced meanings of words in different contexts.\n- Self-attention is a superior method for generating contextualized embeddings, which are crucial for accurate text generation.\n- Masking in self-attention is essential to maintain the causal flow of information in text generation, preventing future words from influencing past interpretations.\n- Multi-head attention is praised for its ability to capture diverse aspects of context, enriching the model's understanding of language.\n- Positional encoding is a necessary addition to self-attention mechanisms to ensure the significance of word order is not lost.\n- The combination of self-attention, MLPs, and positional encoding in decoder-only transformers is seen as a breakthrough in the field of natural language processing, leading to the advanced capabilities of LLMs."])</script><script>self.__next_f.push([1,"1c:T2069,"])</script><script>self.__next_f.push([1,"\u003cbody\u003e\u003carticle\u003e\u003ch1 id=\"f35c\"\u003eDecoder-Only Transformers Explained: The Engine Behind LLMs\u003c/h1\u003e\u003cfigure id=\"68b8\"\u003e\u003cimg src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/1*qTjjAvXmrSaRdN1LODLVGA.png\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://arxiv.org/abs/2305.07716\"\u003ehttps://arxiv.org/abs/2305.07716\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"df67\"\u003eLarge language models (LLMs) like GPT-3, LLaMA, and Gemini are revolutionizing how we interact with and generate text. At the heart of these powerful models lies a specific type of neural network architecture called the \u003cb\u003edecoder-only transformer\u003c/b\u003e. This article will discuss the inner workings of decoder-only transformers, providing a clear and comprehensive explanation of their key components and how they enable LLMs to understand and generate human-like text.\u003c/p\u003e\u003ch1 id=\"f3a9\"\u003e1. The Problem with Word-to-Vector\u003c/h1\u003e\u003cp id=\"294e\"\u003eTraditional natural language processing techniques often relied on simple word embeddings, where each word is represented by a fixed vector, similar to how Word2Vec or GloVe operate. This means that regardless of the context in which a word appears, its vector representation remains the same. For example, using a simple word embedding approach, the vector for “bank” in “river bank” would be identical to the vector for “bank” in “financial bank.” These embeddings, however, fail to capture the nuanced meanings a word can take on depending on its surrounding words. A robust language model needs to understand these contextual differences to generate accurate and meaningful text.\u003c/p\u003e\u003ch1 id=\"5144\"\u003e2. How Self-Attention Provides Contextualized Embeddings\u003c/h1\u003e\u003cp id=\"c658\"\u003eDecoder-only transformers tackle this challenge through a mechanism called \u003cb\u003eself-attention\u003c/b\u003e. This innovative technique allows the model to consider the relationships between different words in a sentence, effectively creating contextualized embeddings that reflect the meaning of a word within its specific context.\u003c/p\u003e\u003cfigure id=\"bc35\"\u003e\u003cimg src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*FA3tTGE4MrVbNk_w.png\"\u003e\u003cfigcaption\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp id=\"bacf\"\u003e\u003cb\u003e2.1. The Mechanics of Self-Attention: Query, Key, and Value\u003c/b\u003e\u003c/p\u003e\u003cp id=\"9469\"\u003eSelf-attention operates by transforming each word’s initial embedding based on its interactions with all other words in the sequence. This transformation involves three key components:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eQuery Matrix (WQ):\u003c/b\u003e Each word’s embedding is multiplied by the query matrix to generate a query vector. Imagine this vector as the word asking a question, “What other words in this sentence are relevant to me?”\u003c/li\u003e\u003cli\u003e\u003cb\u003eKey Matrix (WK):\u003c/b\u003e Simultaneously, every word’s embedding is multiplied by the key matrix to produce a key vector. These vectors are potential answers to the queries.\u003c/li\u003e\u003cli\u003e\u003cb\u003eValue Matrix (WV):\u003c/b\u003e Finally, the value matrix projects each word’s embedding into a value vector, representing the information that will be passed to other words based on their relevance.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"23b5\"\u003e\u003cb\u003e2.2. Capturing Correlations and the Need for Masking\u003c/b\u003e\u003c/p\u003e\u003cp id=\"1d4e\"\u003eTo measure the relevance of each word to another, the model computes the dot product of each query vector with every key vector. This generates a grid of scores, representing the correlation between each word pair. Higher scores indicate stronger relevance.\u003c/p\u003e\u003cp id=\"12a5\"\u003eIn decoder-only transformers, however, we want to prevent future words from influencing the interpretation of past words. This is crucial for maintaining the flow of information in text generation, where the model should only predict the next word based on what has come before. To achieve this, we apply \u003cb\u003emasking\u003c/b\u003e: the scores for future words are set to negative infinity, effectively zeroing them out after a softmax normalization.\u003c/p\u003e\u003cp id=\"d138\"\u003e\u003cb\u003e2.3. The Role of the Value Matrix\u003c/b\u003e\u003c/p\u003e\u003cp id=\"37b5\"\u003eOnce the attention scores are computed and masked, the value matrix comes into play. For each word, we take a weighted sum of the value vectors of all other words, where the weights are the normalized attention scores. In essence, this allows relevant words to “pass” their information to the word being processed.\u003c/p\u003e\u003cp id=\"fda0\"\u003e\u003cb\u003e2.4. From Word Embeddings to Contextualized Embeddings\u003c/b\u003e\u003c/p\u003e\u003cp id=\"427e\"\u003eThe final step in self-attention involves adding the weighted sum of value vectors to the original word embedding. This updated embedding now incorporates contextual information, reflecting the word’s meaning within the sentence. The formula for this transformation can be summarized as:\u003c/p\u003e\u003cblockquote id=\"f8aa\"\u003e\u003cp\u003eContextualized Embedding = Original Embedding + Weighted Sum of Value Vectors (weighted by attention scores)\u003c/p\u003e\u003c/blockquote\u003e\u003ch1 id=\"5df0\"\u003e3. Multi-Head Attention: Expanding the Scope of Context\u003c/h1\u003e\u003cp id=\"6088\"\u003eWhile self-attention is powerful, it can be limited by its reliance on a single set of query, key, and value matrices. \u003cb\u003eMulti-head attention\u003c/b\u003e addresses this limitation by running multiple self-attention operations in parallel, each with its own unique set of matrices. This allows the model to capture different aspects of context simultaneously. For instance, one head might focus on grammatical relationships, while another might focus on semantic connections. The outputs of all heads are then combined to produce a richer, more nuanced contextualized embedding.\u003c/p\u003e\u003cfigure id=\"8db9\"\u003e\u003cimg src=\"https://cdn-images-1.readmedium.com/v2/resize:fit:800/0*TaZXsiwcPwng0Ffr.png\"\u003e\u003cfigcaption\u003e\u003ca href=\"https://arxiv.org/abs/1706.03762v7\"\u003ehttps://arxiv.org/abs/1706.03762v7\u003c/a\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003ch1 id=\"e919\"\u003e4. Decoder-Only Transformers: A Series of Masked Self-Attention and MLPs\u003c/h1\u003e\u003cp id=\"2731\"\u003eA decoder-only transformer is constructed by stacking multiple layers, each comprising two main components:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eMasked Self-Attention:\u003c/b\u003e As described above, this layer allows words to incorporate contextual information from preceding words.\u003c/li\u003e\u003cli\u003e\u003cb\u003eMultilayer Perceptrons (MLPs):\u003c/b\u003e MLPs are essentially feed-forward neural networks that process each word’s embedding independently. They can be thought of as the “knowledge banks” of the transformer, incorporating general knowledge learned during training to further refine the embeddings.\u003c/li\u003e\u003c/ul\u003e\u003cp id=\"c769\"\u003e\u003cb\u003e4.1. MLPs: Incorporating Knowledge into Embeddings\u003c/b\u003e\u003c/p\u003e\u003cp id=\"c693\"\u003eMLPs operate by projecting each word’s embedding into a higher-dimensional space, applying a non-linear activation function (like ReLU), and then projecting it back down to the original embedding dimension. This process allows the MLP to extract complex features from the embeddings and incorporate information that’s not explicitly captured by the attention mechanism.\u003c/p\u003e\u003ch1 id=\"9920\"\u003e5. Positional Encoding: Capturing Word Order\u003c/h1\u003e\u003cp id=\"addb\"\u003eWhile self-attention considers relationships between words, it doesn’t inherently capture their order within the sequence. To address this, decoder-only transformers utilize \u003cb\u003epositional encoding\u003c/b\u003e. This involves adding a vector representing the word’s position in the sentence to its embedding. These positional embeddings can be learned during training or generated using predefined functions, such as sine and cosine waves. By incorporating positional information, the model can understand the significance of word order in sentences.\u003c/p\u003e\u003ch1 id=\"31ed\"\u003eConclusion\u003c/h1\u003e\u003cp id=\"03e3\"\u003eDecoder-only transformers are remarkable architectures. By leveraging self-attention, MLPs, and positional encoding, they provide LLMs with the ability to understand and generate text with a level of sophistication previously unattainable. This architecture is constantly evolving, with researchers exploring novel modifications to further enhance its capabilities. As we continue to push the boundaries of language models, understanding the principles behind decoder-only transformers remains crucial for unlocking their full potential.\u003c/p\u003e\u003cblockquote id=\"7cdf\"\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eConnect with me \u003c/i\u003e\u003c/b\u003e\u003ci\u003e: \u003ca href=\"https://www.linkedin.com/in/yash-bhaskar/\"\u003ehttps://www.linkedin.com/in/yash-bhaskar/\u003c/a\u003e\n\u003cb\u003eMore Articles like this\u003c/b\u003e: \u003ca href=\"https://medium.com/@yash9439\"\u003ehttps://medium.com/@yash9439\u003c/a\u003e\u003c/i\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003c/article\u003e\u003c/body\u003e"])</script><script>self.__next_f.push([1,"16:[[\"$\",\"nav\",null,{\"className\":\"fixed z-10 h-[54px] w-screen flex flex-grow-0 items-center justify-between bg-white dark:bg-black px-4 shadow-md dark:shadow-neutral-900\",\"children\":[[\"$\",\"$Le\",null,{\"href\":\"/\",\"className\":\"scale-90 sm:scale-100 cursor-pointer flex items-center flex-shrink-0 text-white hover:scale-105 transform transition\",\"children\":[\"$\",\"$L11\",null,{\"className\":\"mt-2 mb-2 h-6 w-auto sm:px-3\",\"width\":141,\"height\":37,\"src\":\"/images/logo/logo-black.webp\",\"alt\":\"Read Medium logo\"}]}],[\"$\",\"$L12\",null,{\"className\":\"px-3 flex justify-center items-center\"}],[\"$\",\"div\",null,{\"className\":\"flex h-full items-center shrink-0\",\"children\":[[\"$\",\"$L13\",null,{\"className\":\"sm:mr-4\"}],[\"$\",\"$L14\",null,{\"originLang\":\"en\",\"lang\":\"en\",\"pathname\":\"https%3A/medium.com/%40yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\"}]]}]]}],[\"$\",\"div\",null,{\"className\":\"Gutter_gutter__QB0_n Gutter_gutterLeft__9iSai Gutter_gutterRight__4jfEx pt-[72px] flex flex-col justify-start items-center bg-white dark:bg-black text-black dark:text-white min-h-full\",\"children\":[[\"$\",\"div\",null,{\"className\":\"read-medium-post max-w-[680px] max-w-full md:max-w-[680px] pb-16\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center \",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex justify-start items-center space-x-2\",\"children\":[[\"$\",\"img\",null,{\"src\":\"https://miro.readmedium.com/v2/resize:fill:88:88/1*e3okk9dKJTwm-hFiWlhX4g.jpeg\",\"alt\":\"avatar\",\"className\":\"w-11 h-11 rounded-full border border-solid border-[rgba(0,0,0,0.05)]\"}],[\"$\",\"span\",null,{\"className\":\"text-gray-950 dark:text-white text-xl max-w-[240px] md:max-w-[400px]\",\"children\":\"Yash Bhaskar\"}]]}],[\"$\",\"$L17\",null,{}]]}],[\"$\",\"$L18\",null,{\"html\":\"$19\",\"content\":\"$1a\",\"summarizelng\":\"en\",\"mediumPostId\":\"3a3224086afe\"}],[\"$\",\"div\",null,{\"className\":\"mt-8 flex justify-center\",\"children\":[\"$\",\"$L1b\",null,{}]}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$1c\"}}],[\"$\",\"div\",null,{\"className\":\" text-sm font-bold mt-12 space-y-[4px]\",\"children\":[[\"$\",\"div\",null,{\"className\":\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\",\"children\":\"Decoder Only Transformers\"}],[\"$\",\"div\",null,{\"className\":\"undefined inline-flex justify-center items-center mr-2 font-normal leading-5 text-sm text-[#242424] bg-[#F2F2F2] relative border transition-[background] duration-300 ease-[ease] whitespace-nowrap px-4 rounded-[100px] border-solid border-[#F2F2F2]\",\"children\":\"Self Attention\"}]]}],[\"$\",\"$L1d\",null,{}]]}],[\"$\",\"span\",null,{\"className\":\"flex justify-center items-center w-full py-12 text-2xl divide-y border-t border-gray-300 dark:border-gray-700\",\"children\":\"Recommended from ReadMedium\"}],[\"$\",\"$L1e\",null,{\"className\":\"max-w-[680px]\",\"postUrl\":\"https%3A/medium.com/%40yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe\",\"recommendPosts\":[{\"uniqueSlug\":\"understanding-self-attention-and-transformer-network-architecture-0734f73b8fa3\",\"title\":\"Self-Attention and Transformer Network Architecture\",\"subtitle\":\"The introduction of Transformer models in 2017 marked a significant turning point in the fields of Natural Language Processing (NLP) and…\",\"authorInfo\":{\"name\":\"LM Po\",\"avatarUrl\":\"https://miro.medium.com/v2/resize:fill:88:88/1*8biNIOdTZO6v4MDdtPmm2Q.png\"},\"postImg\":\"https://miro.medium.com/v2/resize:fit:224/1*D8HzNBM9Y1qZPveOFRE3sQ.png\",\"firstTag\":null,\"readingTime\":13.666037735849056,\"createdAt\":\"2024-12-13T01:11:11.392Z\",\"isEligibleForRevenue\":true},{\"uniqueSlug\":\"understanding-query-key-value-in-transformers-c579b93054cc\",\"title\":\"Understanding Query, Key, Value in Transformers and LLMs\",\"subtitle\":\"Building intuition through accessible, simplified examples to explore inner workings of the Attention Mechanism.\",\"authorInfo\":{\"name\":\"Charles Chi\",\"avatarUrl\":\"https://miro.medium.com/v2/resize:fill:88:88/1*yTk-vcLIuC_3A8TGrvLWUg.png\"},\"postImg\":\"https://miro.medium.com/v2/resize:fit:224/1*aCgbtUGUqcJ-gpOp9ji5Lw.png\",\"firstTag\":null,\"readingTime\":8.869811320754716,\"createdAt\":\"2024-10-27T18:11:07.504Z\",\"isEligibleForRevenue\":false},{\"uniqueSlug\":\"architecture-of-meta-llama-3-2-1b-model-e6216cdad960\",\"title\":\"Architecture of Meta Llama 3.2 1B Model\",\"subtitle\":\"Meta released Llama 3.2 models in September 2024. In this article, we take an in-depth look of Llama 3.2 1B model’s architecture.\",\"authorInfo\":{\"name\":\"Manyi\",\"avatarUrl\":\"https://miro.medium.com/v2/resize:fill:88:88/1*PRRJs0q0QzXuHtHYKUh-Mg.jpeg\"},\"postImg\":\"https://miro.medium.com/v2/resize:fit:224/\",\"firstTag\":null,\"readingTime\":3.3811320754716983,\"createdAt\":\"2024-12-03T21:10:49.007Z\",\"isEligibleForRevenue\":true},{\"uniqueSlug\":\"research-paper-summary-training-language-models-to-self-correct-via-reinforcement-learning-d14b52f7bbd2\",\"title\":\"[Research Paper Summary]Training Language Models to Self-Correct via Reinforcement Learning\",\"subtitle\":\"\",\"authorInfo\":{\"name\":\"Himanshu Bamoria\",\"avatarUrl\":\"https://miro.medium.com/v2/resize:fill:88:88/1*XZapwwtSpdDz7JSo_kFUJA.jpeg\"},\"postImg\":\"https://miro.medium.com/v2/resize:fit:224/1*CgWHmp3owWjP2q6S2RFbVg.png\",\"firstTag\":null,\"readingTime\":3.686792452830189,\"createdAt\":\"2024-10-14T10:26:14.821Z\",\"isEligibleForRevenue\":false},{\"uniqueSlug\":\"llm-architectures-explained-encoder-decoder-architecture-part-4-b96ace71394c\",\"title\":\"LLM Architectures Explained: Encoder-Decoder Architecture (Part 4)\",\"subtitle\":\"Deep Dive into the architecture \u0026 building real-world applications leveraging NLP Models starting from RNN to Transformer.\",\"authorInfo\":{\"name\":\"Vipra Singh\",\"avatarUrl\":\"https://miro.medium.com/v2/resize:fill:88:88/1*LDjQS3c-G1gsojOf24ijGg@2x.jpeg\"},\"postImg\":\"https://miro.medium.com/v2/resize:fit:224/0*XCF22nmg4Qn1i3iC\",\"firstTag\":null,\"readingTime\":33.218867924528304,\"createdAt\":\"2024-11-17T19:27:41.824Z\",\"isEligibleForRevenue\":true},{\"uniqueSlug\":\"understanding-graph-based-rag-systems-a-deep-dive-into-graphrag-and-lightrag-daf4f982d7d9\",\"title\":\"Understanding Graph-based RAG Systems: A Deep Dive into GraphRAG and LightRAG\",\"subtitle\":\"The Need for Graph-based RAG Systems\",\"authorInfo\":{\"name\":\"Satyabrata Dash\",\"avatarUrl\":\"https://miro.medium.com/v2/resize:fill:88:88/0*vc7cpjnWjyj0c_WR.jpg\"},\"postImg\":\"https://miro.medium.com/v2/resize:fit:224/1*C1m8cKyDE1pe48Xqg1Z1GA.png\",\"firstTag\":null,\"readingTime\":11.249056603773584,\"createdAt\":\"2024-12-10T02:31:36.742Z\",\"isEligibleForRevenue\":true}]}]]}]]\n"])</script><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_e3okk9dKJTwm-hFiWlhX4g.jpg"><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_8biNIOdTZO6v4MDdtPmm2Q.png"><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_yTk-vcLIuC_3A8TGrvLWUg.png"><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_PRRJs0q0QzXuHtHYKUh-Mg.jpg"><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_XZapwwtSpdDz7JSo_kFUJA.jpg"><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/1_LDjQS3c-G1gsojOf24ijGg@2x.jpg"><link rel="preload" as="image" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/0_vc7cpjnWjyj0c_WR.jpg"><link rel="preload" href="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/page.js" as="script"><script>$RS=function(a,b){a=document.getElementById(a);b=document.getElementById(b);for(a.parentNode.removeChild(a);a.firstChild;)b.parentNode.insertBefore(a.firstChild,b);b.parentNode.removeChild(b)};$RS("S:1","P:1")</script><script>$RC=function(b,c,e){c=document.getElementById(c);c.parentNode.removeChild(c);var a=document.getElementById(b);if(a){b=a.previousSibling;if(e)b.data="$!",a.setAttribute("data-dgst",e);else{e=b.parentNode;a=b.nextSibling;var f=0;do{if(a&&8===a.nodeType){var d=a.data;if("/$"===d)if(0===f)break;else f--;else"$"!==d&&"$?"!==d&&"$!"!==d||f++}d=a.nextSibling;e.removeChild(a);a=d}while(a);for(;c.firstChild;)e.insertBefore(c.firstChild,a);b.data="$"}b._reactRetry&&b._reactRetry()}};$RC("B:0","S:0")</script><script id="_next-ga-init" data-nscript="afterInteractive">
          window['dataLayer'] = window['dataLayer'] || [];
          function gtag(){window['dataLayer'].push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-YSBN5EJVBL');</script><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/js" id="_next-ga" data-nscript="afterInteractive"></script><next-route-announcer style="position: absolute;"><template shadowrootmode="open"><div aria-live="assertive" id="__next-route-announcer__" role="alert" style="position: absolute; border: 0px; height: 1px; margin: -1px; padding: 0px; width: 1px; clip: rect(0px, 0px, 0px, 0px); overflow: hidden; white-space: nowrap; overflow-wrap: normal;"></div></template></next-route-announcer><script src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/page.js" async="true" data-nscript="afterInteractive"></script><div id="addtoany" style="position: static;"><div class="a2a_overlay" id="a2a_overlay"></div><div id="a2a_modal" class="a2a_modal a2a_hide" role="dialog" tabindex="-1" aria-label=""><div class="a2a_modal_body a2a_menu a2a_hide" id="a2a_copy_link"><label for="a2a_copy_link_text" id="a2a_copy_link_icon" class="a2a_svg a2a_s_link a2a_logo_color"><svg focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><title>Copy link</title><path fill="#FFF" d="M7.591 21.177c0-.36.126-.665.377-.917l2.804-2.804a1.235 1.235 0 0 1 .913-.378c.377 0 .7.144.97.43-.026.028-.11.11-.255.25-.144.14-.24.236-.29.29a2.82 2.82 0 0 0-.2.256 1.056 1.056 0 0 0-.177.344 1.43 1.43 0 0 0-.046.37c0 .36.126.666.377.918a1.25 1.25 0 0 0 .918.377c.126.001.251-.015.373-.047.125-.037.242-.096.345-.175.09-.06.176-.127.256-.2.1-.094.196-.19.29-.29.14-.142.223-.23.25-.254.297.28.445.607.445.984 0 .36-.126.664-.377.916l-2.778 2.79a1.242 1.242 0 0 1-.917.364c-.36 0-.665-.118-.917-.35l-1.982-1.97a1.223 1.223 0 0 1-.378-.9l-.001-.004Zm9.477-9.504c0-.36.126-.665.377-.917l2.777-2.79a1.235 1.235 0 0 1 .913-.378c.35 0 .656.12.917.364l1.984 1.968c.254.252.38.553.38.903 0 .36-.126.665-.38.917l-2.802 2.804a1.238 1.238 0 0 1-.916.364c-.377 0-.7-.14-.97-.418.026-.027.11-.11.255-.25a7.5 7.5 0 0 0 .29-.29c.072-.08.139-.166.2-.255.08-.103.14-.22.176-.344.032-.12.048-.245.047-.37 0-.36-.126-.662-.377-.914a1.247 1.247 0 0 0-.917-.377c-.136 0-.26.015-.37.046-.114.03-.23.09-.346.175a3.868 3.868 0 0 0-.256.2c-.054.05-.15.148-.29.29-.14.146-.222.23-.25.258-.294-.278-.442-.606-.442-.983v-.003ZM5.003 21.177c0 1.078.382 1.99 1.146 2.736l1.982 1.968c.745.75 1.658 1.12 2.736 1.12 1.087 0 2.004-.38 2.75-1.143l2.777-2.79c.75-.747 1.12-1.66 1.12-2.737 0-1.106-.392-2.046-1.183-2.818l1.186-1.185c.774.79 1.708 1.186 2.805 1.186 1.078 0 1.995-.376 2.75-1.13l2.803-2.81c.751-.754 1.128-1.671 1.128-2.748 0-1.08-.382-1.993-1.146-2.738L23.875 6.12C23.13 5.372 22.218 5 21.139 5c-1.087 0-2.004.382-2.75 1.146l-2.777 2.79c-.75.747-1.12 1.66-1.12 2.737 0 1.105.392 2.045 1.183 2.817l-1.186 1.186c-.774-.79-1.708-1.186-2.805-1.186-1.078 0-1.995.377-2.75 1.132L6.13 18.426c-.754.755-1.13 1.672-1.13 2.75l.003.001Z"></path></svg></label><input id="a2a_copy_link_text" type="text" title="Copy link" readonly=""><div id="a2a_copy_link_copied">✓</div></div><div class="a2a_modal_body a2a_menu a2a_thanks a2a_hide" id="a2a_thanks"><div class="a2a_localize" data-a2a-localize="inner,ThanksForSharing">Thanks for sharing!</div></div></div><div class="a2a_menu a2a_full a2a_localize" id="a2apage_full" role="dialog" tabindex="-1" aria-label="Share" data-a2a-localize="title,Share"><div class="a2a_full_header"><div id="a2apage_find_container" class="a2a_menu_find_container"><label for="a2apage_find" id="a2apage_find_icon" class="a2a_svg a2a_s_find"><svg focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" aria-hidden="true"><title>Find any service</title><path fill="#CCC" d="M19.7 18.2l-4.5-4.5c.7-1.1 1.2-2.3 1.2-3.6 0-3.5-2.8-6.3-6.3-6.3s-6.3 2.8-6.3 6.3 2.8 6.3 6.3 6.3c1.4 0 2.6-.4 3.6-1.2l4.5 4.5c.6.6 1.3.7 1.7.2.5-.4.4-1.1-.2-1.7zm-9.6-3.6c-2.5 0-4.5-2.1-4.5-4.5 0-2.5 2.1-4.5 4.5-4.5 2.5 0 4.5 2.1 4.5 4.5s-2 4.5-4.5 4.5z"></path></svg></label><input id="a2apage_find" class="a2a_menu_find a2a_localize" type="text" autocomplete="off" title="Find any service" data-a2a-localize="title,FindAnyServiceToAddTo"></div></div><div class="a2a_full_services" id="a2apage_full_services" role="presentation"></div><div class="a2a_full_footer"><a href="https://www.addtoany.com/" title="Share Buttons" rel="noopener" target="_blank"><span class="a2a_svg a2a_s__default a2a_s_a2a a2a_logo_color"><svg focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><g fill="#FFF"><path d="M14 7h4v18h-4z"></path><path d="M7 14h18v4H7z"></path></g></svg></span>AddToAny</a></div></div><div id="a2apage_dropdown" class="a2a_menu a2a_mini a2a_localize a2a_hide" tabindex="-1" aria-label="Share" data-a2a-localize="label,Share"><div class="a2a_mini_services" id="a2apage_mini_services"></div><div id="a2apage_cols_container" class="a2a_cols_container"><div class="a2a_col1" id="a2apage_col1"></div><div id="a2apage_2_col1" class="a2a_hide"></div><div class="a2a_clear"></div></div><div class="a2apage_wide a2a_wide"><a href="https://readmedium.com/en/https:/medium.com/@yash9439/decoder-only-transformers-explained-the-engine-behind-llms-3a3224086afe#addtoany" id="a2apage_show_more_less" class="a2a_more a2a_localize" title="Show all" data-a2a-localize="title,ShowAll"><span class="a2a_svg a2a_s__default a2a_s_a2a a2a_logo_color"><svg focusable="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><g fill="#FFF"><path d="M14 7h4v18h-4z"></path><path d="M7 14h18v4H7z"></path></g></svg></span><span class="a2a_localize" data-a2a-localize="inner,More">More…</span></a></div></div><div style="height: 1px; width: 1px; position: absolute; z-index: 100000; top: 0px; visibility: hidden;"><iframe id="a2a_sm_ifr" title="AddToAny Utility Frame" aria-hidden="true" src="./Decoder-Only Transformers Explained_ The Engine Behind LLMs_files/sm.25.html" style="height: 1px; width: 1px; border: 0px; left: 0px; top: 0px; position: absolute; z-index: 100000; display: none;"></iframe></div></div><script id="tool_clarity" data-nscript="lazyOnload">
          (function(c,l,a,r,i,t,y){
              c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
              t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
              y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
          })(window, document, "clarity", "script", "k14ha83exi");
        </script></body></html>